[{"content":"在网站上有一个动态打字特效当然是一件很酷的事啊。无意之间找到了一个动态打字特效插件 Typed-js 。该插件还实现了各种自定义满足不同的需求。可以说是非常非常好用了，而且安装使用也是非常的简单。\n该插件的作者是 Matt Boldt，这是他的主页：https://mattboldt.com/ 。\nTyped-js 的 Github：https://github.com/mattboldt/typed.js/ 。\n该插件的自定义 Demo 如下：http://mattboldt.github.io/typed.js/ 。\n下面我们大概讲一下该插件的使用方法。\n 安装方法 官方提供了三种安装方法：\nnpm install typed.js yarn add typed.js bower install typed.js 或者使用 script CDN 导入\n\u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/typed.js@2.0.12\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;  使用 从官方给出的例子来看\nThis is really all you need to get going.\n// Can also be included with a regular script tag import Typed from \u0026#39;typed.js\u0026#39;; var options = { strings: [\u0026#39;\u0026lt;i\u0026gt;First\u0026lt;/i\u0026gt; sentence.\u0026#39;, \u0026#39;\u0026amp;amp; a second sentence.\u0026#39;], typeSpeed: 40 }; var typed = new Typed(\u0026#39;.element\u0026#39;, options); 上面这个例子展示了基本的使用方法。我们再来看一个更具体的例子\n var typed = new Typed('#typed', { stringsElement: '#typed-strings' });  \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/typed.js@2.0.12\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var typed = new Typed(\u0026#39;#typed\u0026#39;, { stringsElement: \u0026#39;#typed-strings\u0026#39; }); \u0026lt;/script\u0026gt; \u0026lt;div id=\u0026#34;typed-strings\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Typed.js is a \u0026lt;strong\u0026gt;JavaScript\u0026lt;/strong\u0026gt; library.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;It \u0026lt;em\u0026gt;types\u0026lt;/em\u0026gt; out sentences.\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;span id=\u0026#34;typed\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; stringElement 为定义的外部动态打字文字，这里也可以直接使用 strings:[] 来替代。 typed-strings 表示要插入动态打字特效的模块。一般的自定义则在 new Typed() 中设置。\n 样式设置 css 动画是基于 javascript 初始化构建的，所以可以在 css 中设置各种样式\n/* 光标 */ .typed-cursor { } /* 如果设置了淡出选项 */ .typed-fade-out { }  自定义 var typed = new Typed(\u0026#39;.element\u0026#39;, { /** * @property {array} strings 要输入的字符 * @property {string} stringsElement 包含字符串 children 的元素的 ID */ strings: [ \u0026#39;These are the default values...\u0026#39;, \u0026#39;You know what you should do?\u0026#39;, \u0026#39;Use your own!\u0026#39;, \u0026#39;Have a great day!\u0026#39; ], stringsElement: null, /** * @property {number} typeSpeed 类型速度，单位：毫秒 */ typeSpeed: 0, /** * @property {number} startDelay 输入开始前的时间，单位：毫秒 */ startDelay: 0, /** * @property {number} backSpeed 退格速度，单位：毫秒 */ backSpeed: 0, /** * @property {boolean} smartBackspace 只退格与前一个字符串不匹配的内容 */ smartBackspace: true, /** * @property {boolean} shuffle 随机播放字符串 */ shuffle: false, /** * @property {number} backDelay 退格前的延迟时间，单位：毫秒 */ backDelay: 700, /** * @property {boolean} fadeOut 用淡出替代退格 * @property {string} fadeOutClass 淡出动画 * @property {boolean} fadeOutDelay 淡出延迟，单位：毫秒 */ fadeOut: false, fadeOutClass: \u0026#39;typed-fade-out\u0026#39;, fadeOutDelay: 500, /** * @property {boolean} loop 进行循环播放 * @property {number} loopCount 循环播放次数 */ loop: false, loopCount: Infinity, /** * @property {boolean} showCursor 显示光标 * @property {string} cursorChar 光标字符串，一般为\u0026#39;|\u0026#39;，我个人比较喜欢设置为背景定宽 * @property {boolean} autoInsertCss 插入 CSS 光标并淡出到 HTML \u0026lt;head\u0026gt; */ showCursor: true, cursorChar: \u0026#39;|\u0026#39;, autoInsertCss: true, /** * @property {string} attr 输出的字符串属性 * Ex: input placeholder, value, or just HTML text */ attr: null, /** * @property {boolean} bindInputFocusEvents 如果 el 是文本输入的话则绑定到焦点和模糊 */ bindInputFocusEvents: false, /** * @property {string} contentType 内容类型为 html 或者 null */ contentType: \u0026#39;html\u0026#39;, /** * 开始输入之前执行 * @param {Typed} self */ onBegin: (self) =\u0026gt; {}, /** * 所有输入完成执行 * @param {Typed} self */ onComplete: (self) =\u0026gt; {}, /** * 每个字符串输入之前执行 * @param {number} arrayPos * @param {Typed} self */ preStringTyped: (arrayPos, self) =\u0026gt; {}, /** * 每个字符串输入之后执行 * @param {number} arrayPos * @param {Typed} self */ onStringTyped: (arrayPos, self) =\u0026gt; {}, /** * 循环过程中输入最后一个字符串后执行 * @param {Typed} self */ onLastStringBackspaced: (self) =\u0026gt; {}, /** * 输入停止执行 * @param {number} arrayPos * @param {Typed} self */ onTypingPaused: (arrayPos, self) =\u0026gt; {}, /** * Typing has been started after being stopped * @param {number} arrayPos * @param {Typed} self */ onTypingResumed: (arrayPos, self) =\u0026gt; {}, /** * 重置之后执行 * @param {Typed} self */ onReset: (self) =\u0026gt; {}, /** * 停止之后执行 * @param {number} arrayPos * @param {Typed} self */ onStop: (arrayPos, self) =\u0026gt; {}, /** * 开始之后执行 * @param {number} arrayPos * @param {Typed} self */ onStart: (arrayPos, self) =\u0026gt; {}, /** * 销毁之后 * @param {Typed} self */ onDestroy: (self) =\u0026gt; {} });  举例 http://mattboldt.github.io/typed.js/  ","permalink":"https://yibocat.github.io/posts/jzrj/jzrj_3/","summary":"在网站上有一个动态打字特效当然是一件很酷的事啊。无意之间找到了一个动态打字特效插件 Typed-js 。该插件还实现了各种自定义满足不同的需求。可以说是非常非","title":"[建站日记] 动态打字特效"},{"content":"什么是正向传播？正向传播是按照神经网络的输入到输出的顺序，依次计算从模型中间变量的过程。而反向传播则是计算神经网络的参数梯度的过程。在训练神经网络时，正向传播与反向传播是相互依赖的。正向传播依次计算模型中间变量，反向传播再根据变量对参数进行梯度求导。所以可以这么理解，在参数进行初始化之后对模型进行交替的正向传播与反向传播，并根据反向传播计算的梯度迭代模型参数。\n 正向传播 前几篇文章讲的基本上都是正向传播，这里我们简单地再推导一下。\n假设输入为 $x\\in \\mathbb{R}^d$ ，不考虑偏差，中间变量为 $z\\in\\mathbb{R}^h$，$\\pmb{W}^{(1)}\\in\\mathbb{R}^{h\\times d}$ 为隐藏层的权重参数。设 $\\phi()$ 为激活函数，则有 $$ z=\\pmb{W}^{(1)}x,\\newline \\pmb{h} = \\phi(z) $$ 这里 $\\pmb{h}$ 表示得到的隐藏层变量，也就是中间变量。\n假设输出层参数为 $\\pmb{W}^{(2)}\\in\\mathbb{R}^{q\\times h}$ ，则得到的向量长度为 $q$ 的输出层变量为 $$ \\pmb{o}=\\pmb{W}^{(2)}\\pmb{h} $$ 我们假设损失函数为 $\\ell$ ，样本标签为 $y$ ，则样本损失项为 $$ L=\\ell(\\pmb{o},y) $$ 引入正则化项 $s$ $$ s=\\frac{\\lambda}{2}(\\vert\\vert\\pmb{W}^{(1)}\\vert\\vert^2_F+\\vert\\vert\\pmb{W}^{(2)}\\vert\\vert^2_F) $$ 得到模型在给定样本上的损失为 $J = L+s$ ，我们称 $J$ 为给定样本的目标函数。\n 反向传播 我们说反向传播是计算神经网络参数梯度的方法，其基本原理是微积分中的链式法则。\n 链式法则\n假设 $y=g(x)，z=f(y)$，则 $z=h(x)=f(g(x))$ 。对这两个函数分别求导有 $$ \\frac{dy}{dx}=g'(x) \\newline \\frac{dz}{dy}=f'(y) $$ 根据链式法则，有 $$ h'(x)=\\frac{dz}{dx}=\\frac{dz}{dy}\\cdot\\frac{dy}{dx} $$ 即复合函数求导使用乘法法则，或称为链式法则。在反向传播中我们可以完美地使用这个法则。\n 反向传播\n我们将链式法则应用到反向传播中。在上文中，我们的模型参数为 $\\pmb{W}^{(1)}$ 和 $\\pmb{W}^{(2)}$ ，因此反向传播所要作的计算是 $\\frac{\\partial J}{\\partial\\pmb{W}^{(1)}}$ 和 $\\frac{\\partial J}{\\partial\\pmb{W}^{(2)}}$，反向传播的计算次序与正向传播计算次序恰恰相反。我们求目标函数关于 $L$ 和 $s$ 的梯度，有 $$ \\frac{\\partial J}{\\partial L}=1,\\quad \\frac{\\partial J}{\\partial s}=1 $$ 然后根据链式法则计算目标函数对于输出层变量的梯度 $$ \\frac{\\partial J}{\\partial o}=\\frac{\\partial J}{\\partial L}\\cdot\\frac{\\partial L}{\\partial o}=\\frac{\\partial L}{\\partial o} $$ 接着计算正则化项对于两个参数的梯度 $$ \\frac{\\partial s}{\\partial\\pmb{W}^{(1)}}=\\lambda\\pmb{W}^{(1)},\\quad \\frac{\\partial s}{\\partial\\pmb{W}^{(2)}}=\\lambda\\pmb{W}^{(2)} $$ 然后先计算靠近输出层参数的梯度 $\\frac{\\partial J}{\\partial\\pmb{W}^{(2)}}\\in\\mathbb{R}^{q\\times h}$ $$ \\begin{aligned} \\frac{\\partial J}{\\partial\\pmb{W}^{(2)}} \u0026amp; = \\frac{\\partial J}{\\partial o}\\centerdot\\frac{\\partial o}{\\partial\\pmb{W}^{(2)}}+\\frac{\\partial J}{\\partial s}\\centerdot\\frac{\\partial s}{\\partial \\pmb{W}^{(2)}} \\newline \u0026amp; = \\frac{\\partial J}{\\partial o}\\pmb{h}^{\\top}+\\lambda\\pmb{W}^{(2)} \\end{aligned} $$ 然后继续沿着输出层向隐藏层反向传播，隐藏层变量的梯度为 $$ \\frac{\\partial J}{\\partial \\pmb{h}} = \\frac{\\partial J}{\\partial o}\\centerdot\\frac{\\partial o}{\\partial\\pmb{h}}={\\pmb{W}^{(2)}}^{\\top}\\frac{\\partial J}{\\partial o} $$ 由于激活函数 $\\phi$ 按元素运算，中间变量 $z$ 的梯度 $\\frac{\\partial J}{\\partial z}$ 的计算需要使用按元素乘法 ，符号位 $\\odot$ $$ \\frac{\\partial J}{\\partial z}=\\frac{\\partial J}{\\partial \\pmb{h}}\\cdot\\frac{\\partial \\pmb{h}}{\\partial z}=\\frac{\\partial J}{\\partial\\pmb{h}}\\odot\\phi'(z) $$ 最后我们得到靠近输入层的模型参数的梯度 $\\frac{\\partial J}{\\partial\\pmb{W}^{(1)}}$ $$ \\begin{aligned} \\frac{\\partial J}{\\partial \\pmb{W}^{(1)}} \u0026amp;= \\frac{\\partial J}{\\partial z}\\cdot\\frac{\\partial z}{\\partial\\pmb{W}^{(1)}}+\\frac{\\partial J}{\\partial s}\\cdot\\frac{\\partial s}{\\partial \\pmb{W}^{(1)}} \\newline \u0026amp; = \\frac{\\partial J}{\\partial z}x^{\\top}+\\lambda\\pmb{W}^{(1)} \\end{aligned} $$ \n深度学习模型训练 看了这么些公式可能都忘记我们最初是要干什么的，我们为什么要算这么些公式？我们来回顾一下。\n我们训练模型的目的是为了得到最优的参数以满足我们的模型，那么什么样的模型最优呢？可以认为是损失函数最小的模型就是最优的。那么如何降低损失函数呢，我们知道模型在给定样本上的损失函数为 $J$ ，我们要让损失函数最小化，所以通过梯度下降法迭代最接近的参数，使用梯度下降法进行迭代就要通过反向传播算法计算参数梯度。我们回忆一下带有 $L2$ 参数正则化项的参数迭代公式 $$ \\omega\\gets(1-\\frac{\\eta\\lambda}{\\vert \\mathcal{B}\\vert})\\omega-\\frac{\\eta}{\\vert \\mathcal{B}\\vert}\\sum_{i\\in\\mathcal{B}}\\frac{\\partial \\ell^{(i)}(\\omega,b)}{\\partial \\omega} $$ 这么一看，后面的一项正是我们所推导的参数迭代公式。这就是反向传播！\n 一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。\n所以，模型参数初始化之后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。而正是由于神经网络的特殊性，正向传播结束后不能立即释放变量内存，从而导致神经网络的训练往往占用很大的内存的原因。\n 参考 Back Propagation（梯度反向传播）实例讲解 - HexUp的文章 《动手学深度学习》PyTorch 版，阿斯顿·张、李沐 ","permalink":"https://yibocat.github.io/posts/dl/sdxx_5/","summary":"什么是正向传播？正向传播是按照神经网络的输入到输出的顺序，依次计算从模型中间变量的过程。而反向传播则是计算神经网络的参数梯度的过程。在训练神","title":"[深度学习笔记] 深度学习第 5 篇——正向传播与反向传播"},{"content":"权重衰减与丢弃法是常用的正则化方法，本篇文章我们尝试手动实现权重衰减与丢弃法，并且对其进行评估。\n 权重衰减 我们知道高复杂度模型与样本数据不匹配时会发生过拟合与欠拟合现象，而应对过拟合的方法，一方面是增大训练样本数据量，另外一种方法是限制模型复杂度。一般来说增大样本数据量来匹配神经网络模型复杂度的方法并不是一个好方法，而使用正则化手段对模型添加正则化项可以有效地限制模型复杂度从而防止过拟合。权重衰减（Weight Decay） 就是一种有效的正则化方法，即在每次参数更新时，引入一个衰减系数。\n在标准的随机梯度下降中，权重衰减正则化与 $L2$ 范数正则化等价，但是在一些较复杂的优化方法中（Adam）并不等价。这里回顾一下 $L2$ 范数正则化。\n $L2$ 范数正则化\n$L2$ 范数正则化在模型原损失函数的基础上增加了 $L2$ 范数正则化项，以此来限制参数从而使得模型复杂度尽可能简单。$L2$ 范数正则化项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。\n我们看线性回归损失函数为 $$ \\ell(\\pmb{\\omega},b)=\\frac{1}{N}\\sum_{i=1}^N\\frac{1}{2}(\\pmb{x}^{(i)}\\pmb{\\omega}+b-\\pmb{y}^{(i)})^2 $$ 其中 $\\pmb{\\omega}$ 为权重参数向量，$b$ 为偏差参数，样本 $i $ 的输入为 $\\pmb{x}^{(i)}$ ，该样本标签为 $\\pmb{y}^{(i)}$ ，样本数目为 $N$ 。则带有 $L2$ 范数正则化项的损失函数为 $$ \\ell(\\pmb{\\omega},b)+\\frac{\\lambda}{2N}\\vert\\vert\\pmb{\\omega}\\vert\\vert^2 $$ 这里 $\\lambda\u0026gt;0$ 为超参数。当 $\\lambda$ 接近 0 时，正则化项对损失函数的影响将逐渐降低。\n$L2$ 范数平方 $\\vert\\vert\\pmb{\\omega}\\vert\\vert^2$ 展开后得到 $\\sum_{i=1}^N\\omega_i^2$ 。我们在小批量随机梯度下降中将线性回归中的权重 $\\pmb{\\omega}$ 的迭代方式 $$ \\omega \\gets \\omega-\\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\frac{\\partial \\ell^{(i)}(\\omega,b)}{\\partial \\omega} $$ 更改为如下方式 $$ \\omega\\gets(1-\\frac{\\eta\\lambda}{\\vert \\mathcal{B}\\vert})\\omega-\\frac{\\eta}{\\vert \\mathcal{B}\\vert}\\sum_{i\\in\\mathcal{B}}\\frac{\\partial \\ell^{(i)}(\\omega,b)}{\\partial \\omega} $$ 可以看到， $L2$ 范数正则化将权重乘以一个小于 1 的数后再减去不含正则化项的梯度。所以 $L2$ 范数正则化也叫做权重衰减。\n 权重衰减手动编码实验 生成数据集\n设样本数据集特征的维度为 $p$ ，对于数据集中的样本使用如下线性模型 $$ y = \\sum_{i=1}^p0.01x_i+0.05+\\epsilon $$ 这里权重为 0.01，$\\epsilon$ 为噪声项，其服从均值为 0，标准差为0.01 的高斯分布。同时，我们为了更好的显示过拟合效果，设置维度 $p$ 为 200，训练数据集数量设置为 20。\n导入必须包\n%matplotlib inline import torch import torch.nn as nn import numpy as np import sys import d2lzh_pytorch as d2l 生成数据集\nn_train, n_test, num_imputs = 20, 100, 200 true_w, true_b = torch.ones(num_imputs, 1) * 0.01, 0.05 # 生成数据集 features = torch.rand((n_train + n_test, num_imputs)) labels = torch.matmul(features, true_w) + true_b labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float) train_features, test_features = features[:n_train, :], features[n_train:, :] train_labels, test_labels = labels[:n_train], labels[n_train:]  初始化模型参数\n定义一个初始化模型参数的函数\ndef init_params(): # 随机初始化参数 w = torch.randn((num_inputs, 1), requires_grad=True) b = torch.zeros(1, requires_grad=True) return [w, b]  $L2$ 范数正则化项\ndef l2_penalty(w): return (w**2).sum() / 2  定义和训练模型\nbatch_size, num_epochs, lr = 1, 100, 0.003\t# 小批量为 1，迭代周期 100 net, loss = d2l.linreg, d2l.squared_loss\t# net 使用线性回归，损失函数使用平方差损失 dataset = torch.utils.data.TensorDataset(train_features, train_labels) train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True) def fit_and_plot(lambd): w, b = init_params() train_ls, test_ls = [], [] for _ in range(num_epochs): for X, y in train_iter: # 添加了L2范数惩罚项 l = loss(net(X, w, b), y) + lambd * l2_penalty(w) # 增加正则化项的损失函数 l = l.sum() if w.grad is not None:\t# 梯度清零 w.grad.data.zero_() b.grad.data.zero_() l.backward()\t# 反向传播计算 d2l.sgd([w, b], lr, batch_size)\t# 随机梯度下降法 train_ls.append(loss(net(train_features, w, b), train_labels).mean().item()) test_ls.append(loss(net(test_features, w, b), test_labels).mean().item()) d2l.semilogy(range(1, num_epochs + 1), train_ls, \u0026#39;epochs\u0026#39;, \u0026#39;loss\u0026#39;, range(1, num_epochs + 1), test_ls, [\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;])\t# 画图 print(\u0026#39;L2 norm of w:\u0026#39;, w.norm().item())  观察过拟合\n首先我们让 $\\lambda=0.005$ ，因为 $\\lambda$ 接近 0，所以正则化项的影响非常小。\nfit_and_plot(lambd=0.005) 输出\nL2 norm of w: 12.12175464630127 我们可以看到训练误差远远小于测试误差，这是很明显的过拟合。\n然后我们增大 $\\lambda$ ，设置为 $\\lambda=4$\nfit_and_plot(lambd=4) 输出\nL2 norm of w: 0.07619155943393707 可以明显的看到测试误差有所下降，过拟合现象有所缓解。另外，权重参数的L2L_2L2范数比不使用权重衰减时的更小，此时的权重参数更接近0。\n 丢弃法 我们假设一个模型，其输入个数为 4 个，隐藏单元为 5，则隐藏单元 $h_i$ 的计算表达式为 $$ h_i=\\phi(x_1\\omega_{1i}+x_2\\omega_{2i}+x_3\\omega_{3i}+x_4\\omega_{4i}+b_i) $$ 这里 $\\phi$ 表示激活函数。当我们对隐藏层使用丢弃法时，该层的隐藏单元会有一定的概率被丢弃掉，我们设丢弃概率为 $p$ ，则一个隐藏单元有 $p$ 概率会被清零，有 $1-p$ 的概率该隐藏单元除以 $1-p$ 进行拉伸。而丢弃概率是超参数。\n设随机变量 $\\xi_i$ 为 0 和 1 的概率为 $p$ 和 $1-p$ ，则该随机变量服从伯努利分布。我们设隐藏单元 $h'_i$ 为 $$ h'_i=\\frac{\\xi_i}{1-p}h_i $$ 因为 $\\xi_i$ 服从伯努利分布，所以其期望为 $E(\\xi_i)=1-p$ ，所以 $$ E(h'_i)=\\frac{E(\\xi_i)}{1-p}h_i=h_i $$ 得到结论：丢弃法不改变输入的期望值！\n我们看一下上述模型使用丢弃法后的神经网络结构\n可以看到 $h_2$ 和 $h_5$ 都被清零，因此反向传播时与 $h_2$ 和 $h_5$ 相关的权重的梯度为 0。\n为什么丢弃法起到正则化的作用？和正则化一样，丢弃法也是为了从减小模型复杂度出发来防止过拟合的，因为随机减少隐藏单元和缩小维度是一样的道理。邱锡鹏教授的《神经网络与深度学习》中给出了两种解释：集成学习角度的解释和贝叶斯学习角度的解释。\n 集成学习角度的解释\n每做一次丢弃，相当于从原始的网络中采样得到一个子网络，若一个神经网络有 $n$ 个神经元，那么总共可以采样出 $2^n$ 个子网络。每次迭代都相当于训练一个不同的子网络，这些子网络都共享原始网络的参数，所以最终的网络可以近似看作是集成了指数级个不同网络的组合模型。\n 贝叶斯学习角度的解释（不是太明白\u0026hellip;\u0026hellip;）\n丢弃法可以解释为一种贝叶斯学习的近似，用 $y=f(\\pmb{x};\\theta)$ 来表示要学习的神经网络，贝叶斯学习时假设参数 $\\theta$ 为随机向量，并且先验分布为 $q(\\theta)$ ，贝叶斯方法的预测为 $$ \\mathbb{E}_{q(\\theta)}[y]=\\int_qf(\\pmb{x};\\theta)q(\\theta)d\\theta\\approx\\frac{1}{M} \\sum^M f(\\pmb{x},\\theta_i) $$ 其中 $f(\\pmb{x},\\theta_i)$ 为第 $i$ 次使用丢弃法后的网络，其参数 $\\theta_i$ 为对全部参数 $\\theta$ 的一次采样。\n 手动实现丢弃法 定义 dropout() 函数以 drop_prob 的概率丢弃。\ndef dropout(X, drop_prob): X = X.float() assert 0 \u0026lt;= drop_prob \u0026lt;= 1 keep_prob = 1 - drop_prob # 这种情况下把全部元素都丢弃 if keep_prob == 0: return torch.zeros_like(X) mask = (torch.rand(X.shape) \u0026lt; keep_prob).float() return mask * X / keep_prob  定义模型参数\n这里继续使用原书中的 Fashion-MNIST 数据集。然后定义一个包含两个隐藏层的多层感知机，且两个隐藏层的输出个数均为 256。\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256 W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True) b1 = torch.zeros(num_hiddens1, requires_grad=True) W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True) b2 = torch.zeros(num_hiddens2, requires_grad=True) W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True) b3 = torch.zeros(num_outputs, requires_grad=True) params = [W1, b1, W2, b2, W3, b3]  定义模型\n模型定义部分使用 ReLU 激活函数，然后对每个激活函数的输出使用丢弃法。这里我们将第一个隐藏层的丢弃概率设置为 0.1，第二个隐藏层的丢弃概率设为 0.6，然后通过参数 is_training 来判断运行模式是训练还是测试，并在训练模式中使用丢弃法。\ndrop_prob1, drop_prob2 = 0.1, 0.6 def net(X, is_training=True): X = X.view(-1, num_inputs) H1 = (torch.matmul(X, W1) + b1).relu() if is_training: # 只在训练模型时使用丢弃法 H1 = dropout(H1, drop_prob1) # 在第一层全连接后添加丢弃层 H2 = (torch.matmul(H1, W2) + b2).relu() if is_training: H2 = dropout(H2, drop_prob2) # 在第二层全连接后添加丢弃层 return torch.matmul(H2, W3) + b3  模型评估\n在对模型进行评估的时候不应该使用丢弃法\n# 本函数已保存在d2lzh_pytorch def evaluate_accuracy(data_iter, net): acc_sum, n = 0.0, 0 for X, y in data_iter: if isinstance(net, torch.nn.Module): net.eval() # 评估模式, 这会关闭dropout acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() net.train() # 改回训练模式 else: # 自定义的模型 if(\u0026#39;is_training\u0026#39; in net.__code__.co_varnames): # 如果有is_training这个参数 # 将is_training设置成False acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() else: acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() n += y.shape[0] return acc_sum / n  训练和测试\n模型训练 train_ch3() 使用和多层感知机里一样的方法\nnum_epochs, lr = 5, 0.1 # 本函数已保存在d2lzh包中方便以后使用 def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, optimizer=None): for epoch in range(num_epochs): train_l_sum, train_acc_sum, n = 0.0, 0.0, 0 for X, y in train_iter: y_hat = net(X) l = loss(y_hat, y).sum() # 梯度清零 if optimizer is not None: optimizer.zero_grad() elif params is not None and params[0].grad is not None: for param in params: param.grad.data.zero_() l.backward() if optimizer is None: d2l.sgd(params, lr, batch_size) else: optimizer.step() # “softmax回归的简洁实现”一节将用到 train_l_sum += l.item() train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item() n += y.shape[0] test_acc = evaluate_accuracy(test_iter, net) print(\u0026#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f\u0026#39; % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc)) train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr) num_epochs, lr, batch_size = 5, 100.0, 256 loss = torch.nn.CrossEntropyLoss() train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size) d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr) 输出\nepoch 1, loss 0.0047, train acc 0.538, test acc 0.763 epoch 2, loss 0.0024, train acc 0.777, test acc 0.773 epoch 3, loss 0.0020, train acc 0.817, test acc 0.792 epoch 4, loss 0.0018, train acc 0.839, test acc 0.802 epoch 5, loss 0.0017, train acc 0.845, test acc 0.831  最后，原书中还有两种方法的简洁实现，这里不再写了。\n 参考   《动手学深度学习》PyTorch 版，阿斯顿·张、李沐 https://tangshusen.me/Dive-into-DL-PyTorch/#/   《神经网络与深度学习》邱锡鹏 https://nndl.github.io   ","permalink":"https://yibocat.github.io/posts/dl/sdxx_4/","summary":"权重衰减与丢弃法是常用的正则化方法，本篇文章我们尝试手动实现权重衰减与丢弃法，并且对其进行评估。 权重衰减 我们知道高复杂度模型与样本数据不匹配","title":"[深度学习笔记] 深度学习第 4 篇——权重衰减与丢弃法"},{"content":"在学习正则化之前，首先看一下经验误差和泛化误差与过拟合和欠拟合，然后给出模型复杂度用来判定一个模型的好坏，然后学习网络正则化。\n 经验误差与泛化误差 什么是经验误差和泛化误差？通俗的来讲，经验误差（Empirical Error）就是指模型在训练数据集上的表现出来的误差，泛化误差（Generalization Error）是指模型在任意新样本上的误差。\n 具体来看，给定一个模型 $f(x;\\theta)$ ，我们希望的是这个模型有一个小的期望误差，但是我们不知道真实的数据分布和映射函数，所以无法计算期望误差。但是我们可以计算的是经验误差，即模型在训练集上的误差，也就是训练集上的平均损失 $$ E_{D}^{emp}(\\theta)=\\frac{1}{m}\\sum_{i=1}^m\\mathbb{L}(y^{(i)},f(x^{(i)};\\theta)) $$ 这里 $E_D^{emp}(\\theta)$ 表示在训练集 $D$ 上的参数为 $\\theta$ 的平均经验误差，假设有 $m$ 个训练数据，$\\mathbb{L}$ 表示每个训练数据集的真实标签与模型 $f(x^{(i)};\\theta)$ 得到的标签之间的误差函数。\n我们首先希望得到的模型一定是有较小的平均经验误差。通俗的讲，我们的模型最基本的要求首先是要在训练集上的误差最小吧，也就是暂且让模型满足训练集上的数据误差最小（扯了三句啰嗦话，其实是一个意思）。这也就是常说的经验风险最小化准则（Empirical Risk Minimization , ERM）。\n 接着来看泛化误差。\n我们假设任意样本集为 $\\mathcal{D}$ ，则泛化误差的表示为 $$ E_{\\mathcal{D}}(\\theta) = P_{x\\thicksim\\mathcal{D}}(y\\ne f(x;\\theta)) $$ 这个公式可以这么理解，$P_{x\\thicksim\\mathcal{D}}$ 表示 $x$ 在任意样本下的误差（或者说平均误差），而任意新样本的标签不等于该模型对该样本数据下的标签（公式可能不太正确，大概意思是一样的）。\n 以上我们知道了什么是经验误差和泛化误差。我们希望一个模型可以满足 ERM。\n但是！但是！！！我们不能让经验风险无穷小（夸张）！过小的经验风险虽然会让模型在训练集上的表现非常好，但是会造成过拟合！这意味着我们的模型适合且仅适合训练数据集！太好了也不是件好事，所谓物极必反。\n这也就造成了过拟合现象。\n 过拟合与欠拟合 上文讲过，过拟合的意思就是一个训练好的模型只在训练集上表现非常好，但是无法用在新的数据集，或者说测试集上。通常造成过拟合的原因是训练数据集过少，或者可以这么说，过拟合产生的原因是数据集与模型复杂度不匹配，即训练数据集的数量无法支撑高复杂度的模型。而神经网络通常是一个超高复杂度的模型，过少的数据集很容易造成过拟合现象。\n欠拟合则很好理解，指的是模型较弱的学习能力形成的导致泛化能力不高的现象。总的来说，过拟合和欠拟合都是由于模型复杂度与数据集不匹配造成的。\n所以我们看一下什么是模型复杂度这个概念。\n 模型复杂度 首先我们以多项式拟合为例。给定一个由标量数据特征 $x$ 和对应的标量标签 $y$ 组成的训练数据集，多项式函数拟合的目标是找到一个 $K$ 阶多项式函数 $$ \\hat{y}=b+\\sum_{k=1}^Kx^k\\omega_k $$ 来近似 $y$。上式中，$\\omega_k$ 表示模型的权重参数，$b$ 表示偏差参数。而多项式函数拟合也是用平方损失函数。特别的，一阶多项式函数拟合又叫线性函数拟合。\n从上式可以看到，随着 $k$ 越来越大，模型的参数也就越来越多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高，这也就意味着高阶多项式函数比低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。这也就是模型复杂度。\n下图可以很清晰的看出，模型复杂度与误差的关系以及什么是欠拟合与过拟合\n关于过拟合和欠拟合的实验，可以查看原书https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.11_underfit-overfit 正则化 神经网络因其非常强的拟合能力往往使得经验误差可以降到非常低，从而造成过拟合现象。所以如何提高神经网络的泛化能力是非常重要的。\n从上文得知，防止过拟合可以从数据入手，增加数据集的数量，但是因神经网络强大的拟合能力可以知道其必然是一个模型复杂度非常高的模型，这意味着数据集的量呈指数增长从而无法满足神经网络超高的复杂度，所以增加数据集的量显然并不是一个好的方法。所以只能从另一个角度来考虑，即限制模型复杂度，也就是我们说的网络正则化。\n正则化（Regularization） 是一种通过限制模型复杂度来避免过拟合的方法，常用的正则化方法有权重衰减、丢弃法等。\n $\\ell_1,\\ell_2$ 正则化\n$\\ell_1,\\ell_2$ 正则化也称作范数正则化，是机器学习中最常用的正则化方法。其通过约束参数的 $\\ell_1$ 和 $\\ell_2$ 范数来减小模型在训练数据集上的过拟合现象。我们将 $\\ell_1$ 和 $\\ell_2$ 加入到优化问题中 $$ \\mathcal{J}(\\theta) = arg\\min_{\\theta}\\frac{1}{N}\\sum_{n=1}^N\\mathcal{L}(y^{(n)},f(x^{(n)};\\theta))+\\lambda\\ell_p(\\theta) $$ $\\mathcal{L}()$ 表示损失函数，$N$ 为训练样本数量，$f()$ 表示待学习的神经网络，$\\ell_p$ 表示范数函数，这里 $p$ 表示第 $p$ 范数，这里取值为 ${1,2}$ 表示 $\\ell_1,\\ell_2$ 范数，$\\lambda$ 为正则化系数。\n 怎么理解这个式子呢？首先我们看等式右边的前半部分，这其实就是上文中所提到的经验误差最小化式子。我们假设 $\\lambda$ 无限小于 0 即等价于 0 ，此时该式子退化为经验误差最小化，也就是我们所说的只考虑增大训练集数据量来防止过拟合。而加入正则化项后，随着 $\\lambda$ 的值越来越大，范数函数在等式中的作用越来越大，而该函数正是来限制模型复杂度的，所以该式子通过加入了正则项，来限制模型的复杂度。\n至于 $\\ell_1$ 和 $\\ell_2$ 是如何限制模型的，这涉及到数学问题了，这里就不再赘述了。\n 算了\u0026hellip;做笔记 \u0026hellip; 还是详细一点 \u0026hellip;\n 正则化项如何影响模型\n我们将上式等式的右边第一部分简化一下，用 $E^{emp}(\\theta)$ 表示，则优化问题可以写成 $$ \\mathcal{J}(\\theta)=E^{emp}(\\theta)+\\lambda\\ell_p(\\theta) $$ 首先看这个式子，我们的目的是为了让 $\\mathcal{J}(\\theta)$ 最小化，在不考虑正则化项的时候，$\\mathcal{J}(\\theta)$ 等价于经验误差最小化。而加入正则化项之后就要综合考虑两项了（这里看似废话，实则是循序渐进）。而为了使得 $\\mathcal{J}(\\theta)$ 最小化，正则化项也要越小越好。\n这里我们先看一下 1-范数和 2-范数的定义\n 向量的 1-范数：$\\vert\\vert X\\vert\\vert_1 = \\sum_{i=1}^n\\vert x_i\\vert$ ，表示向量内各元素的绝对值之和\n向量的 2-范数：$\\vert\\vert X\\vert\\vert_2=(\\sum_{i=1}^nx_i^2)^{\\frac{1}{2}}$ ，表示元素的平方和再开平方\n  这时候我们再看，1-范数为例，我们要让正则化项越小，意味着 $\\theta$ 要让 $\\sum_{i=1}^n\\vert\\theta_i\\vert$ 尽可能小，这样 $\\theta_i$ 就被限制住了。而 $\\theta_i$ 是什么？不就是参数吗？我们不光要让参数尽可能小，而且还要让参数的数量尽可能得少！$\\theta_i$ 被限制住，曲线也就不会太过陡峭。这种正则化的方式被称为\nLasso 回归（Least Absolute Shrinkage and Selection operator Regression) 。\n 1-范数正则化\nL1 正则化项与稀疏性有关，即 L1 正则化可以使得参数稀疏化从而减小模型复杂度。什么是稀疏性？通常神经网络中的特征会达到一个非常大的数量，如果模型的参数有非常多的 0，那么就可以将很多的参数稀疏掉。而 L1 正则化项相当于对模型进行了一次特征选择，只留下重要的特征，从而提高模型的泛化能力，防止过拟合。\n 我们假设一个二维参数空间，损失函数的等高线如下所示\n这时 L1 正则化为 $\\vert \\theta_1\\vert+\\vert\\theta_2\\vert$ ，对应的等高线是一个菱形。当不加如正则化项的时候，使用梯度下降法优化损失函数，他会沿着梯度方向下降而得到一个近似的最优解\n而加入L1正则化后，如下所示。$P,Q$ 两点在同一等高线上，即 $P$ 和 $Q$ 两个点的损失函数是等价的。但是，$OP$ 的距离大于 $OQ$ 的距离\n因此可以得到 $P$ 点的经验损失函数大于 $Q$ 点的经验损失函数，因为 $Q$ 点的 L1 范数小于 $P$ 点的 L1 范数，所以选择 $Q$ 点，其次，当我们选择 $Q$ 点时，对应的 $\\theta_1=0$ ，这恰好将 $\\theta_1$ 去掉了，并且对原模型并没有影响。\n所以可以得出结论，即加上正则化项，参数空间会被缩小，从而减小模型复杂度。\n 2-范数正则化\n和 L1 正则化是一样的，我们假设一个二维的参数空间，L2 正则化为 $\\theta_1^2+\\theta_2^2$ ，这时 L2 等高线是一个同心圆\n同样我们使用上述损失函数与损失函数等高线，如图所示，L2 正则与损失等高线交于 $P,Q$ 两点，可以看到 $P$ 点距离 $Q$ 点更接近原点，即距离短。但是此时 $\\theta_1,\\theta_2$ 均不为 0 。所以我们说 2-范数正则化可以得到尽可能小的解，但是不具有稀疏性。而 L2 正则化方法也是岭回归方法。\n到这，我们了解了正则化项是如何影响到模型的复杂度的。\n 总结 训练模型的目的是为了经验风险最小化，而过于追求经验风险最小化会造成过拟合。\n 不论是过拟合还是欠拟合，其产生的原因是由于模型复杂度与数据集不匹配造成的，解决方法有两个：1.从数据集入手（不推荐）；2.从模型复杂度入手（正则化）\n 正则化通过添加正则项来对模型复杂度加以限制，从而避免过拟合\n 正则化如何影响模型复杂度：Lasso 回归（具有稀疏性）与岭回归（不具有稀疏性）。\n 参考   《机器学习》周志华\n  《动手学深度学习》PyTorch 版，阿斯顿·张、李沐 https://tangshusen.me/Dive-into-DL-PyTorch/#/   《神经网络与深度学习》邱锡鹏 https://nndl.github.io   概率近似正确？！（PAC Learning） - Haoran Jiang的文章 - 知乎 https://zhuanlan.zhihu.com/p/44589648   [计算学习理论] PAC (Probably Approximately Correct)\n  （理论+代码）理解模型正则化：L1正则、L2正则 - 饼干Japson的文章 - 知乎 https://zhuanlan.zhihu.com/p/113373391   Lasso—原理及最优解 - 风磐的文章 - 知乎 https://zhuanlan.zhihu.com/p/116869931   ","permalink":"https://yibocat.github.io/posts/dl/sdxx_3/","summary":"在学习正则化之前，首先看一下经验误差和泛化误差与过拟合和欠拟合，然后给出模型复杂度用来判定一个模型的好坏，然后学习网络正则化。 经验误差与泛化","title":"[深度学习笔记] 深度学习第 3 篇——网络正则化"},{"content":"其实感知机也就是上一篇所讲的最简单的单层神经网络，即简单线性回归模型，也是属于一个简单二分类或多分类模型。\n模型设计 多层感知机就是在原本感知机的基础上增加了若干层隐藏层。由于输入层不参与到计算，所以下图展示的是一个二层的神经网络模型。\n可以看到，该多层感知机包含 5 个隐藏单元，并且隐藏层中的每个单元和输入层的各个输入是全连接的，输出层的神经元和隐藏层中的各个单元也是全连接的，所以多层感知机中的隐藏层和输出层都是全连接层。\n 我们设一个小批量样本 $X\\in \\mathbb{R}^{n\\times d}$ ，批量大小为 $n$ ，输入个数为 $d$ 。隐藏层层数为一层，隐层单元数为 $h$ ，隐藏层输出为 $H\\in \\mathbb{R}^{n\\times h}$ （也可以记为隐藏层变量）。\n设隐藏层权重参数和偏差分别为 $W_h\\in\\mathbb{R}^{d\\times h}$ 和 $b_h\\in\\mathbb{R}^{l \\times h}$，输出层权重和偏差分别为 $W_o\\in\\mathbb{R}^{h\\times q}$ 和 $b_o\\in\\mathbb{R}^{l\\times q}$ 。由线性回归的公式可以得到多层感知机的设计模型 $$ H=XW_h+b_h \\newline O=HW_o+b_o $$ 将两个式子联立起来，可以得到 $$ O = (XW_h+b_h)W_o+b_o = XW_hW_o+b_hW_o+b_o $$ 可以看到，多层感知机的模型其实和单层神经网络是等价的，只不过输出层权重变成了 $W_hW_o$ ，偏差变成了 $b_hW_o+b_o$ 。\n激活函数 由上面的推导我们知道线性变换只是对数据进行了仿射变换(affine transformation) ，我们不能直接根据其仿射变换得到想要的输出分类，所以需要引入非线性变换——激活函数。\n神经网络中有很多不同的激活函数\n ReLU函数\nReLU 函数（Rectified Linear Unit，修正线性单元），也叫 Rectifier 函数，是目前深度神经网络中经常使用的激活函数，该函数定义为 $$ ReLU(x)=\\max(0,x) $$ 其图像如下所示\n可以看到，当输入为负数时导数为 0 ，当输入为正数时导数为 1。\n Sigmoid 函数\nSigmoid 函数可以将元素的值变换到 0 和 1 之间，该函数如下定义： $$ Sigmoid(x) = \\frac{1}{1+\\exp(-x)} $$ 其图像如下所示\n Tanh 函数\nTanh 函数是一种 Sigmoid 型函数，也叫做双正切函数，其定义为 $$ tanh(x)=\\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)} $$ Tanh 函数可以看成是放大并平移的 Sigmoid 函数，其形状和 Sigmoid 函数看起来很像，但是值域是 (-1,1)。\n我们可以对比一下 Logistic 型 Sigmoid 函数和 Tanh 函数的图像\n多层感知机加入激活函数 通过上文已经知道，多层感知机就是含有至少一个隐藏层的全连接神经网络，每个隐藏层的输出由激活函数进行变换，我们可以将激活函数定义为 $\\phi(x)$ ，那么多层感知机的模型如下所示 $$ H=\\phi(XW_h+b_h) \\newline O=HW_o+b_o $$ \n手动实现 这部分可以参考原书https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.9_mlp-scratch 或者 Github https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/code/chapter03_DL-basics/3.9_mlp-scratch.ipynb 。\n获取数据和读取数据直接使用原书中所给的 Fashion-MNIST 数据集，这是一个图像多分类问题，我们假设小批量数据集的数量为 batch_size = 256 ，并且通过 d2l.load_data_fashion_mnist() 方法得到了训练数据集合测试数据集，该函数是原书中图像多分类问题的读取数据集的函数 。代码如下\nbatch_size = 256 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)  定义参数模型\n原数据集中的图像形状为 28$\\times$28 ，一共有 10 个类别，所以可以用 $28\\times 28=784$ 向量长度来表示一个图像。所以 输入个数为 784，输出个数为 10。因为我们的数据集的小批量数据数为 256，所以我们可以设置隐藏单元数为 256。注意：隐藏单元数是超参数。\n然后对参数进行初始化。\nnum_inputs, num_outputs, num_hiddens = 784, 10, 256 W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float) b1 = torch.zeros(num_hiddens, dtype=torch.float) W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float) b2 = torch.zeros(num_outputs, dtype=torch.float) params = [W1, b1, W2, b2] for param in params: param.requires_grad_(requires_grad=True)  定义激活函数\nPyTorch 已经有了 ReLU 函数，实现起来很简单。在手动实现时我们最好自己写。\ndef relu(X): return torch.max(input=X, other=torch.tensor(0.0))  定义模型\n定义模型的第一步需要将每张图像的形状改为长度为 num_inputs 的向量，以方便将其导入到输入层，然后根据上文中的表达式建立模型\ndef net(X): X = X.view((-1, num_inputs)) H = relu(torch.matmul(X, W1) + b1) return torch.matmul(H, W2) + b2 .view() 函数表示重塑形状，这个函数和 sklearn 中的 reshape() 的功能是一样的，而 view(-1,1) 表示重塑成列向量。同理，view(1,-1) 表示重塑成行向量。 而 torch.matmul() 是矩阵相乘方法。\n 定义损失函数\n损失函数直接使用 PyTorch 中的交叉熵损失函数\nloss = torch.nn.CrossEntropyLoss()  训练模型\n我们设置两个超参数，迭代周期和学习率，分别为 5 和 100.0\nnum_epochs, lr = 5, 100.0 # 本函数已保存在d2lzh包中方便以后使用 def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, optimizer=None): for epoch in range(num_epochs): train_l_sum, train_acc_sum, n = 0.0, 0.0, 0 for X, y in train_iter: y_hat = net(X) l = loss(y_hat, y).sum() # 梯度清零 if optimizer is not None: optimizer.zero_grad() elif params is not None and params[0].grad is not None: for param in params: param.grad.data.zero_() l.backward() if optimizer is None: d2l.sgd(params, lr, batch_size) else: optimizer.step() # “softmax回归的简洁实现”一节将用到 train_l_sum += l.item() train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item() n += y.shape[0] test_acc = evaluate_accuracy(test_iter, net) print(\u0026#39;epoch %d, loss %.4f, train acc %.3f, test acc %.3f\u0026#39; % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc)) train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr) 最后得到输出如下\nepoch 1, loss 0.0030, train acc 0.714, test acc 0.753 epoch 2, loss 0.0019, train acc 0.821, test acc 0.777 epoch 3, loss 0.0017, train acc 0.842, test acc 0.834 epoch 4, loss 0.0015, train acc 0.857, test acc 0.839 epoch 5, loss 0.0014, train acc 0.865, test acc 0.845  全部手工实现多层感知机如上所示，但是这样手动实现其实很容易出现错误，且效率不高。我们可以使用 PyTorch 更简洁地实现多层感知机。\n 简洁实现 模型定义\n模型定义可以直接使用 PyTorch 中的 nn.Sequential 来实现。\nnum_inputs, num_outputs, num_hiddens = 784, 10, 256 net = nn.Sequential( d2l.FlattenLayer(), nn.Linear(num_inputs, num_hiddens),\t# 输入层 nn.ReLU(),\t# 激活函数 nn.Linear(num_hiddens, num_outputs), # 输出层 ) for params in net.parameters():\t# 初始化参数 init.normal_(params, mean=0, std=0.01) 模型从上往下为输入层，激活函数，输出层。\n 读取数据训练模型\n这里优化直接使用 Pytorch 中的 SGD 梯度下降算法\nbatch_size = 256\t# 小批量大小\\隐藏单元数量 train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\t# 读取数据 loss = torch.nn.CrossEntropyLoss()\t# 损失函数 optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\t# 优化方法 num_epochs = 5\t# 迭代周期 train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)# 训练模型 最后输出\nepoch 1, loss 0.0030, train acc 0.712, test acc 0.744 epoch 2, loss 0.0019, train acc 0.823, test acc 0.821 epoch 3, loss 0.0017, train acc 0.844, test acc 0.842 epoch 4, loss 0.0015, train acc 0.856, test acc 0.842 epoch 5, loss 0.0014, train acc 0.864, test acc 0.818  参考 《动手学深度学习》PyTorch 版 《神经网络与深度学习》邱锡鹏教授 ","permalink":"https://yibocat.github.io/posts/dl/sdxx_2/","summary":"其实感知机也就是上一篇所讲的最简单的单层神经网络，即简单线性回归模型，也是属于一个简单二分类或多分类模型。 模型设计 多层感知机就是在原本感知机","title":"[深度学习笔记] 深度学习第 2 篇——多层感知机"},{"content":"线性模型（Linear Model）是机器学习中应用最广泛的模型，指通过样本特征的线性组合来进行预测的模型。线性回归是单层神经网络，其涉及的概念和技术适用于大多数深度学习模型。\n 线性回归 模型定义\n给定一个 $ D $ 维样本 $ x=[x_1,x_2,\\cdots,x_D]^T $ 其线性组合函数为 $$ f(x;\\omega)=\\omega^Tx+b $$ 我们说 $ \\omega $ 是一个权重矩阵（可以理解为斜率）， $ b $ 为偏差（可以理解为截距），其均为标量。这两个参数称为线性回归模型的参数，而我们的目的就是通过训练模型，得到最佳的参数估计。\n 损失函数\n模型训练出的预测值通常需要和真实值进行比对，这种比对也就是误差。在机器学习里把衡量误差的函数称为损失函数（loss function）。这里我们使用平方误差函数，单个样本的平方损失函数可以如下表示 $$ l^{(i)}(\\omega,b) = \\frac{1}{2}(\\hat{y}-y)^2 $$ 这里 $ \\hat{y} $ 表示训练的预测值，$ y $ 表示真实值，$ \\frac{1}{2} $ 是为了在求导时方便化简。训练集中所有样本的误差的平均来衡量模型预测的质量，即 $$ l(\\omega,b)=\\frac{1}{n}\\sum_{i=1}^nl^{(i)}(\\omega,b) $$ 正如上文提到的，深度学习的任务就是找到一组模型参数，使得训练样本的损失最小。\n 算法优化\n求解数值解的优化算法中，小批量随机梯度下降法（mini-batch stochastic gradient descent 在深度学习中被广泛使用。\n 当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。\n 在线性回归模型中，模型的每个参数的迭代如下所示: $$ \\omega \\gets \\omega-\\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\frac{\\partial l^{(i)}(\\omega,b)}{\\partial \\omega} \\newline b\\gets b-\\frac{\\eta}{|\\mathcal{B}|}\\sum_{i\\in\\mathcal{B}}\\frac{\\partial l^{(i)}(\\omega,b)}{\\partial b} $$ 这里 $ |\\mathcal{B}| $ 表示每个小批量的样本个数，$\\eta$ 为学习率。而这里的小批量样本数和学习率是人为设定的而不是学习得来的，所以被称为超参数（hyperparameter）。\n 线性回归实现  线性回归的实现总体上分为以下几个步骤：\n 数据集的准备 初始化模型参数 定义模型 定义损失函数 定义优化算法、 训练模型  在此之前，我们可以定义一个函数，生成特征标签的散点图，这样可以更直接地观察两者间的线性关系。\ndef use_svg_display(): # 用矢量图显示 display.set_matplotlib_formats(\u0026#39;svg\u0026#39;) def set_figsize(figsize=(3.5, 2.5)): use_svg_display() # 设置图的尺寸 plt.rcParams[\u0026#39;figure.figsize\u0026#39;] = figsize #打印散点图 set_figsize() plt.scatter(features[:, 1].numpy(), labels.numpy(), 1);  读取数据\n训练模型的时候需要不断读取小批量数据样本，所以可以定义一个函数来返回小批量的随机样本的特征和标签\ndef data_iter(batch_size, features, labels): # batch_size 表示批量大小 num_examples = len(features) indices = list(range(num_examples)) random.shuffle(indices) # 样本的读取顺序是随机的 for i in range(0, num_examples, batch_size): j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch yield features.index_select(0, j), labels.index_select(0, j) batch_size = 10 for X, y in data_iter(batch_size, features, labels): # 打印 print(X, y) break  初始化模型参数\n权重初始化成均值为 0 ，标准差为 0.01 的正太随机数，偏差初始化为 0。然后因为参数需要求梯度来迭代，所以设置 requires_grad=True。\nw = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32) b = torch.zeros(1, dtype=torch.float32) w.requires_grad_(requires_grad=True) b.requires_grad_(requires_grad=True)  定义模型\n我们已经知道了线性回归的模型表达式，使用 torch.mm 进行矩阵乘法运算\ndef linreg(X, w, b): return torch.mm(X, w) + b  定义损失函数\n损失函数我们使用平方损失函数。注意：由于 $\\hat{y}-y$ 中，$\\hat{y}$ 和 $y$ 的形状是不一样的，所以需要使用 y.view 将 $y$ 的形状变成预测值 $\\hat{y}$ 的形状。\ndef squared_loss(y_hat, y): # 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2 return (y_hat - y.view(y_hat.size())) ** 2 / 2  定义优化算法\n优化算法根据上文提到的使用小批量随机梯度下降算法，其通过不断迭代模型参数来优化损失函数。这里 lr 为迭代步长。\ndef sgd(params, lr, batch_size): for param in params: param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data  训练模型\nlr = 0.03\t# 迭代步长 num_epochs = 3\t# 迭代周期 net = linreg\t# 使用线性模型 loss = squared_loss\t# 损失函数 for epoch in range(num_epochs): # 训练模型一共需要num_epochs个迭代周期 # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X # 和y分别是小批量样本的特征和标签 for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y).sum() # l是有关小批量X和y的损失 l.backward() # 小批量的损失对模型参数求梯度 sgd([w, b], lr, batch_size) # 使用小批量随机梯度下降迭代模型参数 # 不要忘了梯度清零 w.grad.data.zero_() b.grad.data.zero_() train_l = loss(net(features, w, b), labels) print(\u0026#39;epoch %d, loss %f\u0026#39; % (epoch + 1, train_l.mean().item())) 这里我们设置迭代步长为 0.03，迭代周期为 3，每次迭代周期中都会通过 data_iter 读取小批量数据样本，然后设置损失函数时，由于 l 并不是标量，所以需要通过 sum() 方法求和得到标量，再使用 l.backward() 得到模型参数的梯度，然后使用小批量随机梯度下降法迭代模型参数。注意：每次迭代完需要对梯度清零。\n 以上就是全人工实现了一个简单的线性回归模型。当然，PyTorch 提供了简便的模型构造方法和多种损失函数。\n 线性回归简洁实现  读取数据\nPyTorch 提供了 data 包读取数据。\nimport torch.utils.data as Data batch_size = 10 # 将训练数据的特征和标签组合 dataset = Data.TensorDataset(features, labels) # 随机读取小批量 data_iter = Data.DataLoader(dataset, batch_size, shuffle=True) for X, y in data_iter: # 打印 print(X, y) break  定义模型\nPyTorch 提供了大量预定义的层，这使得我们可以很简洁的实现模型的构造。\n导入 torch.nn 模块，nn的核心数据结构是 Module 。Module 是一个抽象概念，既可以表示一个层，又可以表示一个很多层的神经网络，其实它本身就是所有层的一个基类。\nclass LinearNet(nn.Module): def __init__(self, n_feature): super(LinearNet, self).__init__() self.linear = nn.Linear(n_feature, 1) # forward 定义前向传播 def forward(self, x): y = self.linear(x) return y net = LinearNet(num_inputs) print(net) # 使用print可以打印出网络的结构 forward() 定义了模型的前向传播计算方式。\n其实 PyTorch 还有更加简便的网络搭建方法，如 nn.Sequential, 这里不再赘述。\nnet = nn.Sequential( nn.Linear(num_inputs, 1) # 此处还可以传入其他层 )  初始化模型参数\nPyTorch 在 init 模块中提供了很多参数初始化的方法，这里可以通过 init.normal_ 将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布，偏差会初始化为零。\nfrom torch.nn import init init.normal_(net[0].weight, mean=0, std=0.01) init.constant_(net[0].bias, val=0) # 也可以直接修改bias的data: net[0].bias.data.fill_(0)  定义损失函数\nloss = nn.MSELoss()  定义优化算法\ntorch.optim模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等，所以不用再自己实现小批量梯度下降算法了。\nimport torch.optim as optim optimizer = optim.SGD(net.parameters(), lr=0.03) print(optimizer) 输出：\nSGD ( Parameter Group 0 dampening: 0 lr: 0.03 momentum: 0 nesterov: False weight_decay: 0 )  训练模型\nnum_epochs = 3 for epoch in range(1, num_epochs + 1): for X, y in data_iter: output = net(X) l = loss(output, y.view(-1, 1)) optimizer.zero_grad() # 梯度清零，等价于net.zero_grad() l.backward() optimizer.step() print(\u0026#39;epoch %d, loss: %f\u0026#39; % (epoch, l.item()))  以上就是关于简单线性模型的实现，包括所有代码的手工实现以及使用 PyTorch 提供的模块的实现。内容基本上都是根据《动手学深度学习》（PyTorch版）来编写的，代码也是。需要注意的是，应该尽可能采用矢量计算，以提升计算效率。torch.utils.data模块提供了有关数据处理的工具，torch.nn模块定义了大量神经网络的层，torch.nn.init模块定义了各种初始化方法，torch.optim模块提供了很多常用的优化算法。\n 参考 《动手学深度学习》(PyTorch) ","permalink":"https://yibocat.github.io/posts/dl/sdxx_1/","summary":"线性模型（Linear Model）是机器学习中应用最广泛的模型，指通过样本特征的线性组合来进行预测的模型。线性回归是单层神经网络，其涉及的概","title":"[深度学习笔记] 深度学习第 1 篇——简单线性回归"},{"content":"本文通过搭建一个简单的远程 Jupyter 服务端，可以实现不分场合与时间运行代码，或者说是展示代码，这提供了极大的方便。\n搭建 Jupyter 服务器 ssh 远程登录服务器\nssh root@xxx.xxx.xxx.xxx 下载并安装 miniconda\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh 然后设置虚拟 conda 环境，这里我们的环境使用 python 3.9\nconda create -n myenvname python=3.9 切换环境与停用环境\nconda activate myenvname conda deactivate 安装 Jupyter 和配置 安装 jupyter\nconda install -c conda-forge jupyterlab 生成配置文件\njupyter notebook --generate-config 进入 IPython ，设置记住哈希密码（这里会将登录密码转换为哈希密码）\nIpython In[1]: from notebook.auth import passwd In[2]: passwd() Enter password: Verify password: Out[2]: \u0026#39;argon2:$argon2id$v=19$m=10240,t=10,p=8$1Hk7hXkmqH0KUC3JswHy8A$W2Ya\u0026#39; In[3]: exit 修改配置文件 通过 FTP 登录到服务器，找到配置文件 jupyter_notebook_config.py ，添加以下内容到文件末尾\nc.NotebookApp.password = u\u0026#39;argon2:$argon2id$v=19$m=10240,t=10,p=8$1Hk7hXkmqH0KUC3JswHy8A$W2Ya\u0026#39; #就是刚才需要记下的哈希密码 c.NotebookApp.port = 9999 #指定jupyter lab 运行端口，写一个不冲突的端口即可  c.NotebookApp.allow_remote_access = True # 允许远程访问  c.NotebookApp.ip=\u0026#39;*\u0026#39; # 就是设置所有ip皆可访问  c.NotebookApp.open_browser = False # 禁止自动打开浏览器   注意：这里需要将端口放行，否则无法连接登录\n 服务器开启 Jupyterlab 输入代码打开 Jupyterlab\njupyter lab --allow-root 此时已经可以在浏览器上查看远程 Jupyterlab 了，但是当关闭终端时 Jupyter 也会相应关闭，所以我们需要让 Jupyter 保持在后台运行\n后台运行，并将标准输出写入到 jupyter.log 中\nnohup jupyter notebook --allow-root \u0026gt; jupyter.log 2\u0026gt;\u0026amp;1 \u0026amp; nohup jupyter lab --allow-root \u0026gt; jupyter.log 2\u0026gt;\u0026amp;1 \u0026amp; nobup 表示 no hang up，就是不挂起，退出终端后依然可以运行。\n然后找到 jupyter 进程，终止该进程\nps -a kill -9 pid 浏览器中输入 xxx.xxx.xxx.xxx:9999 进入 jupyter notebook\n浏览器中输入 xxx.xxx.xxx.xxx:9999/lab 进入 jupyterlab\n日常登陆 Jupyter 及服务器操作 远程登录服务器\nssh root@xxx.xxx.xxx.xxx 切换环境\nconda activate myenvname 运行 jupyterlab\njupyter lab --allow-root 非挂起运行 JupyterLab （可关闭终端）\nnohup jupyter notebook --allow-root \u0026gt; jupyter.log 2\u0026gt;\u0026amp;1 \u0026amp; nohup jupyter lab --allow-root \u0026gt; jupyter.log 2\u0026gt;\u0026amp;1 \u0026amp; 查看运行进程，并杀死进程\nps -a kill 1000 浏览器中输入 xxx.xxx.xxx.xxx:9999 进入 jupyter notebook\n浏览器中输入 xxx.xxx.xxx.xxx:9999/lab 进入 jupyterlab\n","permalink":"https://yibocat.github.io/posts/fwq/fwq_jupyter/","summary":"\u003cp\u003e本文通过搭建一个简单的远程 \u003ccode\u003eJupyter\u003c/code\u003e 服务端，可以实现不分场合与时间运行代码，或者说是展示代码，这提供了极大的方便。\u003c/p\u003e","title":"[Python 笔记本] 远程搭建 jupyter 服务器"},{"content":"Mac OS 的每个文件夹下都有一个隐藏文件 .DS_Store，该文件保存的是当前文件夹的属性，如图标位置、背景等。\n但是每次使用 Git 提交更改时，都会自动生成 .DS_Store 更改，所以每次提交到版本库和 push 到 Github 总是很麻烦。所以 .DS_Store 是没有必要提交到版本库的，这时可以使用 git.gitignore 来忽略此类文件。\n忽略当前目录下的 .DS_Store 我们在所要忽略 .DS_Store 的目录下创建一个 .gitignore 文件，然后将要忽略的文件名写入进去\ntouch .gitignore 每次忽略任何文件只需将要忽略的文件添加到 .gitignore 文件就可以了。\n但是每次更改完 .gitignore 文件之后，都需要运行以下代码，否则 .gitignore 是不生效的\ngit rm -r --cached . git add . git commit -m \u0026#39;update .gitignore\u0026#39; git 全局忽略 首先在终端输入如下代码查看 git 现有的全局配置\ngit config --list git 全局配置都在一个 .gitconfig 文件中，所以可以使用 git 全局配置进行全局忽略 .DS_Store 。\n具体步骤是在根目录创建一个 .gitignore_global 文件，把要忽略的文件直接添加到该文件中，和上文中的当前目录添加是一样的\n然后在终端输入\ngit config --global core.excludesfile/Users/reon/.gitignore_global 或者直接在 .gitconfig 中添加如下内容\n[core] excludefile = /Users/xiaqunfeng/.gitignore_global 参考  http://xiaqunfeng.cc/2018/04/24/git-ignore-ds-store/#%E5%88%A0%E9%99%A4github%E4%B8%8A%E6%96%87%E4%BB%B6  https://blog.csdn.net/allanGold/article/details/73132606  ","permalink":"https://yibocat.github.io/posts/git/git_ignore/","summary":"\u003cp\u003eMac OS 的每个文件夹下都有一个隐藏文件 .DS_Store，该文件保存的是当前文件夹的属性，如图标位置、背景等。\u003c/p\u003e","title":"[Git 日记] Git 忽略 .DS_Store 与全局忽略"},{"content":"博客是建好了，但是总是想添加一个代码一键复制的功能。一开始是想去 html 标签中直接添加一个 button ，但是发现好像并没有那么简单。查阅了 Hugo 的内置功能也没有发现，很幸运找到了一篇博客 黄忠德的博客 ，正好解决了我的需求。所以也记录一下。\n思考 我们知道，代码片段是使用 markdown code fences 来编写的\n``` jsx import React from 'react'; ``` 以上代码在 Hugo 编译下的 Html 将展示成如下形式\n\u0026lt;div class=\u0026#34;highlight\u0026#34;\u0026gt; \u0026lt;pre style=\u0026#34;background-color:#f0f0f0;tab-size:4\u0026#34;\u0026gt; \u0026lt;code class=\u0026#34;language-jsx\u0026#34; data-lang=\u0026#34;jsx\u0026#34;\u0026gt; \u0026lt;span style=\u0026#34;color:#007020;font-weight:bold\u0026#34;\u0026gt;import\u0026lt;/span\u0026gt; React from \u0026lt;span style=\u0026#34;color:#4070a0\u0026#34;\u0026gt;\u0026#39;react\u0026#39;\u0026lt;/span\u0026gt;; \u0026lt;/code\u0026gt; \u0026lt;/pre\u0026gt; \u0026lt;/div\u0026gt; 我们要解决的问题是：\n 搜索所有突出显示的代码块，特别是所有具有类 highlight 的元素； 如何创建按钮放在代码框中； 给按钮添加一个事件，用于将代码块中的代码复制到剪贴板。  代码 检查复制支持 进行复制之前，我们首先需要对浏览器是否可以使用 document.execCommand('copy') 这个功能，因为这段代码正是我们要使用的复制调用代码，我们需要一个命令来检查一下\nif(!document.queryCommandSupported(\u0026#39;copy\u0026#39;)) { return; } 但是 queryCommandSupported 方法似乎已经弃用，所以其实是不用添加的。\n选择突出显示的代码块 上文提到，突出显示的代码块是包含在类 highlight 中的，我们可以使用内置的 DOM API 来检查所有的在 highlight 内容\nvar highlightBlocks = document.getElementsByClassName(\u0026#39;highlight\u0026#39;); 添加按钮 由于 Hugo 的自动编译使得我们无法直接在 html 中添加按钮，这也是我一开始的疑问之处。但是可以使用 js 创建一个特定的函数来实现这个功能。然后在 for 循环中调用这个函数\nfunction addCopyButton(containerEl) { var copyBtn = document.createElement(\u0026#34;button\u0026#34;); copyBtn.className = \u0026#34;highlight-copy-btn\u0026#34;; copyBtn.textContent = \u0026#34;Copy\u0026#34;; containerEl.appendChild(copyBtn); } for (var i = 0; i \u0026lt; highlightBlocks.length; i++) { addCopyButton(highlightBlocks[i]); } 复制响应 点击按钮，使用 document.execCommand() 方法将代码复制到剪贴板，同时还要保持代码的格式。所以创建一个函数，用来选择给定的 html 中的所有文本\nfunction selectText(node) { var selection = window.getSelection(); var range = document.createRange(); range.selectNodeContents(node); selection.removeAllRanges(); selection.addRange(range); return selection; } 因为代码节点是在  \u0026lt;pre\u0026gt; 所以使用 .firstElementChild 来获取节点，选择文本后添加到剪贴板，然后删除所有选择\nvar codeEl = containerEl.firstElementChild; copyBtn.addEventListener(\u0026#39;click\u0026#39;, function() { var selection = selectText(codeEl); document.execCommand(\u0026#39;copy\u0026#39;); selection.removeAllRanges(); }); 添加样式 这部分是比较简单的，直接放代码了，之后可以自己调试代码按钮样式\n.highlight { position: relative; } .highlight pre { padding-right: 75px; } .highlight-copy-btn { position: absolute; top: 7px; right: 7px; border: 0; border-radius: 4px; padding: 1px; font-size: 0.7em; line-height: 1.8; color: #fff; background-color: #777; min-width: 55px; text-align: center; } .highlight-copy-btn:hover { background-color: #666; } 我们可以看到在代码框的右上方添加了一个灰色的按钮。\n已复制响应 所有功能其实都已经完成了，为了更好的用户体验，在点击按钮后需要有一个已复制的响应返回。\nfunction flashCopyMessage(el, msg) { el.textContent = msg; setTimeout(function() { el.textContent = \u0026#34;Copy\u0026#34;; }, 1000); } try { var selection = selectText(codeEl); document.execCommand(\u0026#39;copy\u0026#39;); selection.removeAllRanges(); flashCopyMessage(copyBtn, \u0026#39;Copied!\u0026#39;) } catch(e) { console \u0026amp;\u0026amp; console.log(e); flashCopyMessage(copyBtn, \u0026#39;Failed :\\\u0026#39;(\u0026#39;) } 所有代码 copy-to-clipboard.css :\n.highlight { position: relative; } .highlight pre { padding-right: 75px; } .highlight-copy-btn { position: absolute; top: 7px; right: 7px; border: 0; border-radius: 4px; padding: 1px; font-size: 0.7em; line-height: 1.8; color: #fff; background-color: #777; min-width: 55px; text-align: center; } .highlight-copy-btn:hover { background-color: #666; } copy-to-clipboard.js :\n(function() { \u0026#39;use strict\u0026#39;; if(!document.queryCommandSupported(\u0026#39;copy\u0026#39;)) { return; } function flashCopyMessage(el, msg) { el.textContent = msg; setTimeout(function() { el.textContent = \u0026#34;Copy\u0026#34;; }, 1000); } function selectText(node) { var selection = window.getSelection(); var range = document.createRange(); range.selectNodeContents(node); selection.removeAllRanges(); selection.addRange(range); return selection; } function addCopyButton(containerEl) { var copyBtn = document.createElement(\u0026#34;button\u0026#34;); copyBtn.className = \u0026#34;highlight-copy-btn\u0026#34;; copyBtn.textContent = \u0026#34;Copy\u0026#34;; var codeEl = containerEl.firstElementChild; copyBtn.addEventListener(\u0026#39;click\u0026#39;, function() { try { var selection = selectText(codeEl); document.execCommand(\u0026#39;copy\u0026#39;); selection.removeAllRanges(); flashCopyMessage(copyBtn, \u0026#39;Copied!\u0026#39;) } catch(e) { console \u0026amp;\u0026amp; console.log(e); flashCopyMessage(copyBtn, \u0026#39;Failed :\\\u0026#39;(\u0026#39;) } }); containerEl.appendChild(copyBtn); } // Add copy button to code blocks  var highlightBlocks = document.getElementsByClassName(\u0026#39;highlight\u0026#39;); Array.prototype.forEach.call(highlightBlocks, addCopyButton); })(); 将这两个文件分别放在 assets/css 和 assets/js 下，然后在配置文件 config.toml 中修改自定义 css 和 js，或者手动添加到 head.html 头文件中。\ncustom_css = [\u0026#34;css/copy-to-clipboard.css\u0026#34;] custom_js = [\u0026#34;js/copy-to-clipboard.js\u0026#34;] 参考  https://huangzhongde.cn/post/2020-02-21-hugo-code-copy-to-clipboard/  https://www.tomspencer.dev/blog/2018/09/14/adding-click-to-copy-buttons-to-a-hugo-powered-blog/  ","permalink":"https://yibocat.github.io/posts/jzrj/jzrj_2/","summary":"\u003cp\u003e博客是建好了，但是总是想添加一个代码一键复制的功能。一开始是想去 html 标签中直接添加一个 \u003ccode\u003ebutton\u003c/code\u003e ，但是发现好像并没有那么简单。查阅了 Hugo 的内置功能也没有发现，很幸运找到了一篇博客 \u003ca href=\"https://huangzhongde.cn/\"target=\"_blank\" rel=\"noopener noreferrer\"\u003e黄忠德的博客\u003c/a\u003e\n，正好解决了我的需求。所以也记录一下。\u003c/p\u003e","title":"[建站日记] Hugo代码拷贝插件"},{"content":"最近花费了近一周搭建自己的网站，准确的说是静态网页。这是网站的第一篇文章，也可以说是近一周来搭建个人博客的踩坑总结。\n网站采用的是 hugo 框架，只是因为 hugo 号称是 \u0026ldquo;The world’s fastest framework for building websites\u0026rdquo;，而其特点就在于轻。因为静态网页无需加载太多插件（wordpress 太重了）， 不需要访问数据库，不需要编写太多的网站功能，并且静态网页使用的是纯 html，对于博客系统来说是完全足够了，这也是我放弃 wordpress 的原因。另外一个原因是 wordpress 虽然拥有各种各样眼花缭乱的插件，但是其维护麻烦，不光 wordpress 本身需要版本更新，插件也需要更新，并且更新起来很艰难，常常自动更新失败从而转为手工更新。并且wordpress 很多主题插件是收费的。\n使用 hugo 搭建个人博客网站，其实理论上来说 hugo 不止可以搭建博客，还可以搭建任何静态网站（甚至是动态网站，不过这也就失去了 hugo 本身的意义了）。而且还有一个更主要的原因：可以学习 git 和 Github。\nhugo 搭建好之后需要进行部署。通常的做法是托管到 Github Pages，也可以部署在腾讯云的静态网页托管（阿里云也有相同的业务），活或者 Gitee Pages，或者自建服务器等等。所以本文从一开始 hugo 与 git 的安装到最后 CDN 网页加速全过程进行总结。\n安装 Hugo 并新建站点 Mac OS 下安装 Hugo 非常方便\n$ brew install hugo 查看版本是否安装正确\n$ hugo version hugo v0.86.1+extended darwin/amd64 BuildDate=unknown Hugo 安装好之后，就可以创建站点了\n$ hugo new site myfirsthugo Congratulations! Your new Hugo site is created in /Users/your_name_path/myfirsthugo. Just a few more steps and you\u0026#39;re ready to go: 1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026#34;hugo new theme \u0026lt;THEMENAME\u0026gt;\u0026#34; command. 2. Perhaps you want to add some content. You can add single files with \u0026#34;hugo new \u0026lt;SECTIONNAME\u0026gt;/\u0026lt;FILENAME\u0026gt;.\u0026lt;FORMAT\u0026gt;\u0026#34;. 3. Start the built-in live server via \u0026#34;hugo server\u0026#34;. Visit https://gohugo.io/ for quickstart guide and full documentation. 进入站点目录，然后查看目录下包含的文件\n$ cd myfirsthugo $ ls -l archetypes\tcontent\tlayouts\tthemes config.toml\tdata\tstatic 这里可以看到 Hugo 自动生成了一堆文件，这些文件也就是 Hugo 站点的结构。\narchetypes：站点要发表的文章的文章模板，在 Hugo官网上称为模板的 Front Matter；\ncontent：发布的文章在这个目录下生成；\nlayouts：Hugo 站点的布局结构，即 html 布局都要在这个文件夹中编写；\nthemes：站点主题；\nconfig.toml：站点配置；\ndata：存放站点的一些数据文件；\nstatic：静态文件则保存在这个文件夹中。\n安装 git git 的安装可以从官方网站上直接下载安装： Mac OS https://git-scm.com/download/mac 或者 Windows https://git-scm.com/download/win Mac OS 也可以直接终端运行\n$ brew install git 安装好之后查看 git 版本\n$ git --version git version 2.32.0 (Apple Git-132) 初始化 git 存储库，安装主题进行配置 Hugo 建站有两种 Github 托管方式，一种先是直接 git 初始化站点目录，在托管部署时直接将整个目录上传至 Github，这种方法可以很方便的在终端下载安装主题；第二种是先从 Github 上下载好主题并安装，然后运行 Hugo 终端命令 hugo 生成 public 文件夹，进入 public 并 git 初始化，然后将 public 托管部署到 Github 上。推荐使用第一种方法，以下文章也是采用第一种方法。\n在 Hugo 站点目录下，进行 git 初始化\nmyfirsthugo $ git init 然后下载主题，可以从 git 命令 clone 一个主题，也可以从 Github 直接下载，将下载好的主题解压并放在 themes 文件夹中。\nmyfirsthugo $ git submodule add https://Github.com/budparr/gohugo-theme-ananke.git themes/ananke 下载好的 anake 主题直接放在了 themes 文件夹中，但是主题还并不能加载使用，需要在配置文件中进行主题配置。\nmyfirsthugo $ echo theme = \\\u0026#34;ananke\\\u0026#34; \u0026gt;\u0026gt; config.toml 或者直接打开 config.toml 在最后一行添加一句：\nbaseURL = \u0026#34;http://example.org/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;ananke\u0026#34; 然后可以新建一个 post 文章\nmyfirsthugo $ hugo new posts/my-first-post.md /Users/your_name_path/myfirsthugo/content/posts/my-first-post.md created 这时可以看到在 content 文件夹中自动创建了一个 posts 文件夹，并且生成了 my-first-post 文章。\n启动 Hugo server\nmyfirsthugo $ hugo server -D Start building sites … hugo v0.86.1+extended darwin/amd64 BuildDate=unknown | EN -------------------+----- Pages | 10 Paginator pages | 0 Non-page files | 0 Static files | 1 Processed images | 0 Aliases | 1 Sitemaps | 1 Cleaned | 0 Built in 72 ms Watching for changes in /Users/your_name_path/myfirsthugo/{archetypes,content,data,layouts,static,themes} Watching for config changes in /Users/your_name_path/myfirsthugo/config.toml, /Users/your_name_path/myfirsthugo/themes/ananke/config.yaml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) 此时在浏览器上打开 http://localhost:1313/ 就可以直接访问了。\n生成站点并发布 以上步骤只是表示网站可以正常运行了，并不意味着这就是 Hugo 要发布的站点。还要通过 hugo 命令发布站点。\nmyfirsthugo $ hugo -D Start building sites … hugo v0.86.1+extended darwin/amd64 BuildDate=unknown | EN -------------------+----- Pages | 10 Paginator pages | 0 Non-page files | 0 Static files | 1 Processed images | 0 Aliases | 1 Sitemaps | 1 Cleaned | 0 Total in 102 ms 注意：这里 -D 表示以草稿（draft）形式发布站点，所有的草稿文章都会显示。\n这时就会在站点目录下生成一个 public 文件夹\nmyfirsthugo $ ls archetypes\tcontent\tlayouts\tresources\tthemes config.toml\tdata\tpublic\tstatic 所有要发布的内容都保存在 public 文件夹中，也就是说，站点呈现的所有内容都已经保存在 public 中了，此时可以将 public 托管到 Github。\n 注意：Github Pages 可以使用 Github Action 自动部署并发布，这意味着并不需要运行 hugo 命令生成 public，我们可以将生成 public 的过程直接部署在 Github 自动发布上。所以可以直接删除掉 public 文件夹，并且在日常浏览中直接使用 hugo server -D 命令就可以了。\n 托管到 Github 浏览器打开网址可以访问，说明网站已经可以正常运行了，但是还需要托管到 Github 上才能进行随时访问。\nGithub Pages 是 Github 官方的博客发布系统，其项目名称需要遵循 username.github.io 形式。在 Github 上新建一个存储库，任何选项都不要勾选，注意：repository name 要和 Owner 名称相同。如下所示\n点击 create repository 创建存储库，会看到这样的界面\n因为之前已经初始化过站点 git 库了，所以只要将远程库的地址加进来就可以了，此时的 Github 库是一个空库。上图中已经告诉了基本步骤，先在本地站点目录创建一个 README.md 文件\n$ echo \u0026#34;# accoppo.github.io\u0026#34; \u0026gt;\u0026gt; README.md 然后创建 master 分支并提交\n$ git add . $ git commit -m \u0026#34;first commit\u0026#34; $ git branch -M master $ git remote add origin git@github.com:accoppo/accoppo.github.io.git 然后 push 上去\n$ git push -u origin master -u 表示库为空，需要整体提交，之后就可以使用 git push 直接 push 了。\n注意：在进行提交推送之前，需要先在 Github 账户设置中新增 SSH 秘钥。\n然后回到 Github 库中，就可以看到所有内容推送上来了。对于 xxx.github.io 这样的项目库名称，Github 直接认定是采用 GIthub Pages 发布的，所以在浏览器直接输入 accoppo.github.io 也是可以显示内容的，只不过显示的 README.md 中的内容。\nGithub Action 自动部署并发布 Action 是 Github 提供的仓库中自动化、自定义和执行软件开发工作流程。\n官方网站 https://docs.github.com/cn/actions Hugo + Github Action 可以参考 https://gohugo.io/hosting-and-deployment/hosting-on-github/#build-hugo-with-github-action 官方提供了一个 yml 文件，文件存在.github/workflows/gh-pages.yml 里，在项目库中选择 Action 并点击 new workflow\n然后点击 set up a workflow yourself\n然后进入到工作流文件设置中，将 main.yml 更改为 gh-pages.yml ，然后将下面代码复制到 Edit new file 中\nname:github pageson:push:branches:- master # Set a branch to deployjobs:deploy:runs-on:ubuntu-18.04steps:- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:\u0026#39;latest\u0026#39;# extended: true- name:Buildrun:hugo --minify- name:Deployuses:peaceiris/actions-gh-pages@v3with:github_token:${{ secrets.GITHUB_TOKEN }}publish_dir:./public如下所示\n然后 Start commit Commit new file ，\n回到 Action 会触发一次构建发布任务，等待几分钟左侧圆圈变成✅\n以上任务完成以后，回到 Code ，并没有看到 public 目录，但是会发现多了一条分支，新增加的这条分支就是 Github Pages 要发布的内容\n然后回到 Setting 的 Pages 选项里将发布源改成 gh-pages.yml\n等待几分钟就可以在浏览器中查看了。\n在发布的站点网页中，并没有看到发布的文章，这是因为在文章的 Front Matter 设置中将 Draft 设置成了 true，这表示文章现在是草稿形式，并没有发布。\n而且因为远程库中的内容已经发生变化，所以先从远程将项目库 pull 下来\nmyfirsthugo $ git pull 然后查看所有分支会发现多出来一条分支 gh-pages\n$ git branch -a * master remotes/origin/gh-pages remotes/origin/master 修改发布文章的 Front Matter 的 Draft 选项，并且在 config.toml 中将 baseURL 修改成 xxx.github.io ,再 push  到 Github\nbaseURL = \u0026#34;http://xxx.github.io\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;ananke\u0026#34; 最后就可以正常访问了。\n域名设置 Github Pages 支持设置自定义域名，如下图所示 custom domain  可以设置自定义域名\n首先在任意的域名注册商那里注册一个域名，腾讯云域名注册链接 https://buy.cloud.tencent.com/domain ，或者进入网站 https://www.dnspod.cn 进行域名注册，然后进行备案。\n等待备案好之后进入 DNSPod 云解析控制台 https://www.dnspod.cn ，可以看到域名已经添加到我的域名中，然后要做的是对域名添加解析，点击域名，进入我的域名管理，点击快速添加解析\n然后将 xxx.github.io 添加到域名映射CNAME 中，点击确认，就会自动生成四条记录。\n回到 GIthub 站点项目库的 setting pages ，将新的域名填入自定义域名框中\n等待几分钟，就可以使用自定义域名访问站点了。\n域名自动更新问题 每次将站点 push 到 Github 并进行 Action 自动发布时，自定义域名总是会丢失，导致每 push 一次，就要手动设置一次域名，很是麻烦。后来查阅资料才知道，Github Action 将项目库主分支进行自动部署时，总是重新部署，这使得我们设置好的域名总会刷新掉。解决办法是在本地站点目录的 static 目录下创建一个 CNAME 文件（注意：没有后缀），写入自定义域名\n这样，在每次 Action 自动发布时，static 目录下的文件会部署到发布目录（即 public目录下）的根目录下，每次进行 push  就不会丢失自定义域名了。\nCDN 静态网页加速 由于 Github 服务器位于国外，所以访问加载时间是非常长的，这反而失去了静态网页的特点。所以我们可以通过 CDN 内容分发网络对我们的网站进行加速，实现快速访问。\n不管是腾讯云还是阿里云或者其他 CDN 服务商，都是一样的流程。我们进入腾讯云 CDN 内容分发网络控制台，进入域名管理，然后添加域名进行域名配置\n然后进行源站配置，注意：源站配置的回源协议再没有设置 SSL 时选择 HTTP，腾讯云有一年的免费 SSL ，等配置完 SSL 修改源站协议即可。\n提交等待几分钟后，需要将生成的加速域名 CNAME 覆盖到 DNSPod 记录管理的 @ 主机记录，这个步骤一般腾讯云会自动替换。\n等待 CDN 部署几分钟后，就可以流畅的浏览自己的博客了。\n注意 CDN 静态网页加速以后，虽然网页浏览速度加快，但是更新缓存资源是延后的，这样造成的后果是发表一篇文章，却不能即时在客户端浏览器看到。幸运的是，在腾讯云的 CDN控制台是有刷新预热功能的，也就是说当网站的源站有资源更新、需要清理违规资源、域名有配置变更的，为避免全网用户受节点缓存影响仍访问到旧的资源、受旧配置的影响，可提交刷新任务，保证全网用户可访问到最新资源或正常访问。\n这里有三种刷新预热方法\n URL 刷新：表示某一个网页例如主页有信息变化时，通常使用 URL 刷新\n目录刷新：网站目录下的信息刷新，例如列表、Hugo 的 Posts 目录等\nURL 预热：当有安装包或者升级包发布时，常采用 URL 预热\n 虽然刷新暂时解决了网页最新的浏览，但是每次手动刷新依然很麻烦。\n很幸运，腾讯云的 CDN 控制台的插件中心中有一项功能是定时刷新预热\n我们可以设置一定的时间或者时间间隔进行自动刷新，避免了每次手动刷新的麻烦。\n总结与其他  添加网页后台统计分析，当前的流形的统计平台包括百度统计、友盟+、腾讯统计等等，都是很不错的。 通过学习 Hugo 的结构，我们不难发现 Hugo 是比较简单的，通过布局嵌套，Hugo 最后部署会将所有的嵌套结构合并成一个网站，这从理论上来说 Hugo 可以搭建任何的网站或者是系统。 有时间有机会了可以自己写一个主题。  参考  https://zhuanlan.zhihu.com/p/350977057  https://gohugo.io/documentation/  https://docs.github.com/cn  https://docs.github.com/cn/pages  ","permalink":"https://yibocat.github.io/posts/jzrj/jzri_1/","summary":"\u003cp\u003e最近花费了近一周搭建自己的网站，准确的说是静态网页。这是网站的第一篇文章，也可以说是近一周来搭建个人博客的踩坑总结。\u003c/p\u003e","title":"[建站日记] Hugo+Github+Tencent CDN博客搭建"},{"content":"CDN 刷新列表 CDN 工作台：https://console.cloud.tencent.com/cdn/refresh URL刷新 https://yibocat.com/ https://yibocat.com/posts https://yibocat.com/archives https://yibocat.com/categories https://yibocat.com/search 目录刷新 https://yibocat.com/ https://yibocat.com/posts/ https://yibocat.com/archives/ https://yibocat.com/categories/ https://yibocat.com/search/ ","permalink":"https://yibocat.github.io/posts/cdn-%E5%88%B7%E6%96%B0%E5%88%97%E8%A1%A8/","summary":"\u003ch3 id=\"cdn-刷新列表\"\u003eCDN 刷新列表\u003c/h3\u003e\n\u003cp\u003eCDN 工作台：\u003ca href=\"https://console.cloud.tencent.com/cdn/refresh\"target=\"_blank\" rel=\"noopener noreferrer\"\u003ehttps://console.cloud.tencent.com/cdn/refresh\u003c/a\u003e\n\u003c/p\u003e","title":"CDN 刷新列表"},{"content":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.com/yuin/goldmark  https://github.com/alecthomas/chroma  https://github.com/muesli/smartcrop  https://github.com/spf13/cobra  https://github.com/spf13/viper   Hugo is ideal for blogs, corporate websites, creative portfolios, online magazines, single page applications or even a website with thousands of pages.\nHugo is for people who want to hand code their own website without worrying about setting up complicated runtimes, dependencies and databases.\nWebsites built with Hugo are extremelly fast, secure and can be deployed anywhere including, AWS, GitHub Pages, Heroku, Netlify and any other hosting provider.\nLearn more and contribute on GitHub .\n","permalink":"https://yibocat.github.io/about/","summary":"Written in Go, Hugo is an open source static site generator available under the Apache Licence 2.0. Hugo supports TOML, YAML and JSON data file types, Markdown and HTML content files and uses shortcodes to add rich content. Other notable features are taxonomies, multilingual mode, image processing, custom output formats, HTML/CSS/JS minification and support for Sass SCSS workflows.\nHugo makes use of a variety of open source projects including:\n https://github.","title":"About"}]