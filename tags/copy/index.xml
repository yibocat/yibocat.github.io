<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>copy on Yibocat</title><link>https://yibocat.github.io/tags/copy/</link><description>Recent content in copy on Yibocat</description><generator>Hugo -- gohugo.io</generator><copyright>©2022 Yibocat.</copyright><lastBuildDate>Sun, 30 Jan 2022 17:26:21 +0800</lastBuildDate><atom:link href="https://yibocat.github.io/tags/copy/index.xml" rel="self" type="application/rss+xml"/><item><title>[笔记手册] Latex 数学符号表</title><link>https://yibocat.github.io/posts/notes/note_1/</link><pubDate>Thu, 10 Mar 2022 23:05:05 +0800</pubDate><guid>https://yibocat.github.io/posts/notes/note_1/</guid><description>&lt;p>经常使用 Latex 写公式，还是总结一下数学符号&amp;hellip;&lt;/p>
&lt;h3 id="数学模式重音符">数学模式重音符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;/th>
&lt;th style="text-align:left">&lt;/th>
&lt;th style="text-align:left">&lt;/th>
&lt;th style="text-align:left">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">$\hat{a}$   \hat{a}&lt;/td>
&lt;td style="text-align:left">$\check{a}$   \check{a}&lt;/td>
&lt;td style="text-align:left">$\tilde{a}$   \tilde{a}&lt;/td>
&lt;td style="text-align:left">$\acute{a}$   \acute{a}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$\grave{a}$   \grave{a}&lt;/td>
&lt;td style="text-align:left">$\dot{a}$   \dot{a}&lt;/td>
&lt;td style="text-align:left">$\ddot{a}$   \ddpt{a}&lt;/td>
&lt;td style="text-align:left">$\breve{a}$   \breve{a}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$\bar{a}$   \bar{a}&lt;/td>
&lt;td style="text-align:left">$\vec{a}$   \vec{a}&lt;/td>
&lt;td style="text-align:left">$\widehat{A}$   \widehat{A}&lt;/td>
&lt;td style="text-align:left">$\widetilde{A}$   \widetilde{A}&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="小写希腊字母">小写希腊字母&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\alpha$   \alpha&lt;/td>
&lt;td>$\theta$   \theta&lt;/td>
&lt;td>$o$   o&lt;/td>
&lt;td>$\upsilon$   \upsilon&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\beta$   \beta&lt;/td>
&lt;td>$\vartheta$   \vartheta&lt;/td>
&lt;td>$\pi$   \pi&lt;/td>
&lt;td>$\phi$   \phi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\gamma$   \gamma&lt;/td>
&lt;td>$\iota$   \iota&lt;/td>
&lt;td>$\varpi$   \varpi&lt;/td>
&lt;td>$\varphi$   \varphi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\delta$   \delta&lt;/td>
&lt;td>$\kappa$   \kappa&lt;/td>
&lt;td>$\rho$   \rho&lt;/td>
&lt;td>$\chi$   \chi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\epsilon$   \epsilon&lt;/td>
&lt;td>$\lambda$   \lambda&lt;/td>
&lt;td>$\varrho$   \varrho&lt;/td>
&lt;td>$\psi$   \psi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\varepsilon$   \varepailon&lt;/td>
&lt;td>$\mu$   \mu&lt;/td>
&lt;td>$\sigma$   \sigma&lt;/td>
&lt;td>$\omega$   \omega&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\zeta$   \zeta&lt;/td>
&lt;td>$\nu$   \nu&lt;/td>
&lt;td>$\varsigma$   \varsigma&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\eta$   \eta&lt;/td>
&lt;td>$\xi$   \xi&lt;/td>
&lt;td>$\tau$   \tau&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="大写希腊字母">大写希腊字母&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">&lt;/th>
&lt;th style="text-align:left">&lt;/th>
&lt;th style="text-align:left">&lt;/th>
&lt;th style="text-align:left">&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">$\Gamma$   \Gamma&lt;/td>
&lt;td style="text-align:left">$\Lambda$   \Lambda&lt;/td>
&lt;td style="text-align:left">$\Sigma$   \Sigma&lt;/td>
&lt;td style="text-align:left">$\Psi$   \Psi&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$\Delta$   \Delta&lt;/td>
&lt;td style="text-align:left">$\Xi$   \Xi&lt;/td>
&lt;td style="text-align:left">$\Upsilon$   \Upsilon&lt;/td>
&lt;td style="text-align:left">$\Omega$   \Omega&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">$\Theta$   \Theta&lt;/td>
&lt;td style="text-align:left">$\Pi$   \Pi&lt;/td>
&lt;td style="text-align:left">$\Phi$   \Phi&lt;/td>
&lt;td style="text-align:left">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="二元关系符">二元关系符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$&amp;lt;$   &amp;lt;&lt;/td>
&lt;td>$&amp;gt;$   &amp;gt;&lt;/td>
&lt;td>$=$   =&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leq$   \leq or \le&lt;/td>
&lt;td>$\geq$   \geq or \ge&lt;/td>
&lt;td>$\equiv$   \equiv&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\ll$   \ll&lt;/td>
&lt;td>$\gg$   \gg&lt;/td>
&lt;td>$\doteq$   \doteq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\prec$   \prec&lt;/td>
&lt;td>$\succ$   \succ&lt;/td>
&lt;td>$\sim$   \sim&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\preceq$   \preceq&lt;/td>
&lt;td>$\succeq$   \succeq&lt;/td>
&lt;td>$\simeq$   \simeq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\subset$   \subset&lt;/td>
&lt;td>$\supset$   \supset&lt;/td>
&lt;td>$\approx$   \approx&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\subseteq$   \subseteq&lt;/td>
&lt;td>$\subseteq$   \supseteq&lt;/td>
&lt;td>$\cong$   \cong&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\sqsubset$   \sqsubset&lt;/td>
&lt;td>$\sqsupset$   \sqsupset&lt;/td>
&lt;td>$\Join$   \Join&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\sqsubseteq$   \sqsubseteq&lt;/td>
&lt;td>$\sqsupseteq$   \sqsupseteq&lt;/td>
&lt;td>$\bowtie$   \bowtie&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\in$   \in&lt;/td>
&lt;td>$\ni$   \ni or \owns&lt;/td>
&lt;td>$\propto$   \propto&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vdash$   \vdash&lt;/td>
&lt;td>$\dashv$   \dashv&lt;/td>
&lt;td>$\models$   \models&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mid$   \mid&lt;/td>
&lt;td>$\parallel$   \parallel&lt;/td>
&lt;td>$\perp$   \perp&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\smile$   \smile&lt;/td>
&lt;td>$\frown$   \frown&lt;/td>
&lt;td>$\asymp$   \asymp&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$:$   :&lt;/td>
&lt;td>$\notin$   \notin&lt;/td>
&lt;td>$\neq$   \neq or \ne&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="二元运算符">二元运算符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$+$   +&lt;/td>
&lt;td>$-$   -&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\pm$   \pm&lt;/td>
&lt;td>$\mp$   \mp&lt;/td>
&lt;td>$\triangleleft$   \triangleleft&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\cdot$   \cdot&lt;/td>
&lt;td>$\div$   \div&lt;/td>
&lt;td>$\triangleright$   \triangleright&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\times$   \times&lt;/td>
&lt;td>$\setminus$   \setminus&lt;/td>
&lt;td>$\star$   \star&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\cup$   \cup&lt;/td>
&lt;td>$\cap$   \cap&lt;/td>
&lt;td>$\ast$   \ast&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\sqcup$   \sqcup&lt;/td>
&lt;td>$\sqcap$   \sqcap&lt;/td>
&lt;td>$\circ$   \circ&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vee$   \vee or \lor&lt;/td>
&lt;td>$\wedge$   \wedge or \land&lt;/td>
&lt;td>$\bullet$   \bullet&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\oplus$   \oplus&lt;/td>
&lt;td>$\ominus$   \ominus&lt;/td>
&lt;td>$\diamond$   \diamond&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\odot$   \odot&lt;/td>
&lt;td>$\oslash$   \oslash&lt;/td>
&lt;td>$\uplus$   \uplus&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\otimes$   \otimes&lt;/td>
&lt;td>$\bigcirc$   \bigcirc&lt;/td>
&lt;td>$\amalg$   \amalg&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\bigtriangleup$   \bigtriangleup&lt;/td>
&lt;td>$\bigtriangledown$   \bigtriangledown&lt;/td>
&lt;td>$\dagger$   \dagger&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lhd$   \lhd&lt;/td>
&lt;td>$\rhd$   \rhd&lt;/td>
&lt;td>$\ddagger$   \ddagger&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\unlhd$   \unlhd&lt;/td>
&lt;td>$\unrhd$   \unlhd&lt;/td>
&lt;td>$\wr$   \wr&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="大尺寸运算符">大尺寸运算符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\sum$   \sum&lt;/td>
&lt;td>$\bigcup$   \bigcup&lt;/td>
&lt;td>$\bigvee$   \bigvee&lt;/td>
&lt;td>$\bigoplus$   \bigoplus&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\prod$   \prod&lt;/td>
&lt;td>$\bigcap$   \bigcap&lt;/td>
&lt;td>$\bigwedge$   \bigwedge&lt;/td>
&lt;td>$\bigotimes$   \bigotimes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\coprod$   \coprod&lt;/td>
&lt;td>$\bigsqcup$   \bigsqcup&lt;/td>
&lt;td>&lt;/td>
&lt;td>$\bigodot$   \bigodot&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\int$   \int&lt;/td>
&lt;td>$\oint$   \oint&lt;/td>
&lt;td>&lt;/td>
&lt;td>$\biguplus$   \biguplus&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="箭头">箭头&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\gets$   \gets&lt;/td>
&lt;td>$\longleftarrow$   \longleftarrow&lt;/td>
&lt;td>$\uparrow$   \uparrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\to$   \to&lt;/td>
&lt;td>$\longrightarrow$   \longrightarrow&lt;/td>
&lt;td>$\downarrow$   \downarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftrightarrow$   \leftrightarrow&lt;/td>
&lt;td>$\longleftrightarrow$   \longleftrightarrow&lt;/td>
&lt;td>$\updownarrow$   \updownarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Leftarrow$   \Lafearrow&lt;/td>
&lt;td>$\Longleftarrow$   \Longleftarrow&lt;/td>
&lt;td>$\Uparrow$   \Uparrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Rightarrow$   \Rightarrow&lt;/td>
&lt;td>$\Longrightarrow$   \Longrightarrow&lt;/td>
&lt;td>$\Downarrow$   \Downarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Leftrightarrow$   \Leftrightarrow&lt;/td>
&lt;td>$\Longleftrightarrow$   \Longleftrightarrow&lt;/td>
&lt;td>$\Updownarrow$   \Updownarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mapsto$   \mapsto&lt;/td>
&lt;td>$\longmapsto$   \longmapsto&lt;/td>
&lt;td>$\nearrow$   \nearrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\hookleftarrow$   \hookleftarrow&lt;/td>
&lt;td>$\hookrightarrow$   \hookrightarrow&lt;/td>
&lt;td>$\searrow$   \searrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftharpoonup$   \leftharpoonup&lt;/td>
&lt;td>$\rightharpoonup$   \rightharpoonup&lt;/td>
&lt;td>$\swarrow$   \swarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftharpoondown$   \leftharpoondown&lt;/td>
&lt;td>$\rightharpoondown$   \rightharpoondown&lt;/td>
&lt;td>$\nwarrow$   \nwarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\rightleftharpoons$   \rightleftharpoons&lt;/td>
&lt;td>$\iff$   \iff&lt;/td>
&lt;td>$\leadsto$   \leadsto&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="定界符">定界符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$($   (&lt;/td>
&lt;td>$)$   )&lt;/td>
&lt;td>$\uparrow$   \uparrow&lt;/td>
&lt;td>$\Uparrow$   \Uparrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lbrack$   [ or \lbrack&lt;/td>
&lt;td>$]$   ] or \rbrack&lt;/td>
&lt;td>$\downarrow$   \downarrow&lt;/td>
&lt;td>$\Downarrow$   \Downarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lbrace$   \{ or \lbrace&lt;/td>
&lt;td>$\rbrace$   \} or \rbrace&lt;/td>
&lt;td>$\updownarrow$   \updownarrow&lt;/td>
&lt;td>$\Updownarrow$   \Updownarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\langle$   \langle&lt;/td>
&lt;td>$\rangle$   \rangle&lt;/td>
&lt;td>$\vert$   \vert or |&lt;/td>
&lt;td>$|$   \| or \Vert&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lfloor$   \lfloor&lt;/td>
&lt;td>$\rfloor$   \rfloor&lt;/td>
&lt;td>$\lceil$   \lceil&lt;/td>
&lt;td>$\rceil$   \rceil&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$/$   /&lt;/td>
&lt;td>$\backslash$   \backslash&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="大尺寸定界符">大尺寸定界符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\lgroup$   \lgroup&lt;/td>
&lt;td>$\rgroup$   \rgroup&lt;/td>
&lt;td>$\lmoustache$   \lmoustache&lt;/td>
&lt;td>$\rmoustache$   \rmoustache&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="其他符号">其他符号&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\dots$   \dots&lt;/td>
&lt;td>$\cdots$   \cdots&lt;/td>
&lt;td>$\vdots$   \vdots&lt;/td>
&lt;td>$\ddots$   \ddots&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\hbar$   \hbar&lt;/td>
&lt;td>$\imath$   \imath&lt;/td>
&lt;td>$\jmath$   \jmath&lt;/td>
&lt;td>$\ell$   \ell&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Re$   \Re&lt;/td>
&lt;td>$\Im$   \Im&lt;/td>
&lt;td>$\aleph$   \aleph&lt;/td>
&lt;td>$\wp$   \wp&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\forall$   \forall&lt;/td>
&lt;td>$\exists$   \exists&lt;/td>
&lt;td>$\mho$   \mho&lt;/td>
&lt;td>$\partial$   \partial&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$&amp;rsquo;$   '&lt;/td>
&lt;td>$\prime$   \prime&lt;/td>
&lt;td>$\emptyset$   \emptyset&lt;/td>
&lt;td>$\infty$   \infty&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nabla$   \nabla&lt;/td>
&lt;td>$\triangle$   \triangle&lt;/td>
&lt;td>$\Box$   \Box&lt;/td>
&lt;td>$\Diamond$   \Diamond&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\bot$   \bot&lt;/td>
&lt;td>$\top$   \top&lt;/td>
&lt;td>$\angle$   \angle&lt;/td>
&lt;td>$\surd$   \surd&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\diamondsuit$   \diamondsuit&lt;/td>
&lt;td>$\heartsuit$   \heartsuit&lt;/td>
&lt;td>$\clubsuit$   \clubsuit&lt;/td>
&lt;td>$\spadesuit$   \spadesuit&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\neg$   \neg&lt;/td>
&lt;td>$\flat$   \flat&lt;/td>
&lt;td>$\natural$   \natural&lt;/td>
&lt;td>$\sharp$   \sharp&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="非数学符号">非数学符号&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\S$   \S&lt;/td>
&lt;td>$\P$   \P&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="ams定界符">AMS定界符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\ulcorner$   \ulcorner&lt;/td>
&lt;td>$\urcorner$   \urcorner&lt;/td>
&lt;td>$\llcorner$   \llcorner&lt;/td>
&lt;td>$\lrcorner$   \lrcorner&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lvert$   \lvert&lt;/td>
&lt;td>$\rvert$   \rvert&lt;/td>
&lt;td>$\lVert$   \lVert&lt;/td>
&lt;td>$\rVert$   \rVert&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="ams-希腊和希伯来字母">AMS 希腊和希伯来字母&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\digamma$   \digamma&lt;/td>
&lt;td>$\varkappa$   \varkappa&lt;/td>
&lt;td>$\beth$   \beth&lt;/td>
&lt;td>$\daleth$   \delath&lt;/td>
&lt;td>$\gimel$   \gimel&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="ams-二元关系符">AMS 二元关系符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\lessdot$   \lessdot&lt;/td>
&lt;td>$\gtrdot$   \gtrdot&lt;/td>
&lt;td>$\doteqdot$   \doteqdot or \Doteq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leqslant$   \leqslant&lt;/td>
&lt;td>$\geqslant$   \geqslant&lt;/td>
&lt;td>$\risingdotseq$   \risingdotseq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\eqslantless$   \eqslantless&lt;/td>
&lt;td>$\eqslantgtr$   \eqslantgtr&lt;/td>
&lt;td>$\fallingdotseq$   \fallingdotseq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leqq$   \leqq&lt;/td>
&lt;td>$\geqq$   \geqq&lt;/td>
&lt;td>$\eqcirc$   \eqcirc&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lll$   \lll&lt;/td>
&lt;td>$\ggg$   \ggg&lt;/td>
&lt;td>$\circeq$   \circeq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lesssim$   \lesssim&lt;/td>
&lt;td>$\gtrsim$   \gtrsim&lt;/td>
&lt;td>$\triangleq$   \triangleq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lessapprox$   \lessapprox&lt;/td>
&lt;td>$\gtrapprox$   \gtrapprox&lt;/td>
&lt;td>$\bumpeq$   \bumpeq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lessgtr$   \lessgtr&lt;/td>
&lt;td>$\gtrless$   \gtrless&lt;/td>
&lt;td>$\Bumpeq$   \Bumpeq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lesseqgtr$   \lesseqgtr&lt;/td>
&lt;td>$\gtreqless$   \gtreqless&lt;/td>
&lt;td>$\thicksim$   \thicksim&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lesseqqgtr$   \lesseqqgtr&lt;/td>
&lt;td>$\gtreqqless$   \gtreqqless&lt;/td>
&lt;td>$\thickapprox$   \thickapprox&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\preccurlyeq$   \preccurlyeq&lt;/td>
&lt;td>$\succcurlyeq$   \succcurlyeq&lt;/td>
&lt;td>$\approxeq$   \approxeq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\curlyeqprec$   \curlyeqprec&lt;/td>
&lt;td>$\curlyeqsucc$   \curlyeqsucc&lt;/td>
&lt;td>$\backsim$   \backsim&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\precsim$   \precsim&lt;/td>
&lt;td>$\succsim$   \succsim&lt;/td>
&lt;td>$\backsimeq$   \backsimeq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\precapprox$   \precapprox&lt;/td>
&lt;td>$\succapprox$   \succapprox&lt;/td>
&lt;td>$\vDash$   \vDash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\subseteqq$   \subseteqq&lt;/td>
&lt;td>$\supseteqq$   \supseteqq&lt;/td>
&lt;td>$\Vdash$   \Vdash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Subset$   \Subset&lt;/td>
&lt;td>$\Supset$   \Supset&lt;/td>
&lt;td>$\Vvdash$   \Vvdash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\sqsubset$   \sqsubset&lt;/td>
&lt;td>$\sqsupset$   \sqsupset&lt;/td>
&lt;td>$\backepsilon$   \backepsilon&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\therefore$   \therefore&lt;/td>
&lt;td>$\because$   \because&lt;/td>
&lt;td>$\varpropto$   \varpropto&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\shortmid$   \shortmid&lt;/td>
&lt;td>$\shortparallel$   \shortparallel&lt;/td>
&lt;td>$\between$   \between&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\smallsmile$   \smallsmile&lt;/td>
&lt;td>$\smallfrown$   \smallfrown&lt;/td>
&lt;td>$\pitchfork$   \pitchfork&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vartriangleleft$   \vartriangleleft&lt;/td>
&lt;td>$\vartriangleright$   \vartriangleleft&lt;/td>
&lt;td>$\blacktriangleleft$   \blacktriangleleft&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\trianglelefteq$   \trianglelefteq&lt;/td>
&lt;td>$\trianglerighteq$   \trianglerighteq&lt;/td>
&lt;td>$\blacktriangleright$   \blacktriangleright&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="ams-箭头">AMS 箭头&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\dashleftarrow$  \dashleftarrow&lt;/td>
&lt;td>$\dashrightarrow$   \dashrightarrow&lt;/td>
&lt;td>$\multimap$   \multimap&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftleftarrows$   \leftleftarrows&lt;/td>
&lt;td>$\rightrightarrows$   \rightrightarrows&lt;/td>
&lt;td>$\upuparrows$   \upuparrows&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftrightarrows$   \leftrightarrows&lt;/td>
&lt;td>$\rightleftarrows$   \rightleftarrows&lt;/td>
&lt;td>$\downdownarrows$  \downdownarrows&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Lleftarrow$   \Lleftarrow&lt;/td>
&lt;td>$\Rrightarrow$   \Rrightarrow&lt;/td>
&lt;td>$\upharpoonleft$   \upharpoonleft&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\twoheadleftarrow$   \twoheadleftarrow&lt;/td>
&lt;td>$\twoheadrightarrow$   \twoheadrightarrow&lt;/td>
&lt;td>$\upharpoonright$   \upharpoonright&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftarrowtail$   \leftarrowtail&lt;/td>
&lt;td>$\rightarrowtail$   \rightarrowtail&lt;/td>
&lt;td>$\downharpoonleft$   \downharpoonleft&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftrightharpoons$   \leftrightharpoons&lt;/td>
&lt;td>$\rightleftharpoons$   \rightleftharpoons&lt;/td>
&lt;td>$\downharpoonright$   \downharpoonright&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Lsh$   \Lsh&lt;/td>
&lt;td>$\Rsh$   \Rsh&lt;/td>
&lt;td>$\rightsquigarrow$   \rightsquigarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\looparrowleft$   \looparrowleft&lt;/td>
&lt;td>$\looparrowright$   \looparrowright&lt;/td>
&lt;td>$\leftrightsquigarrow$   \leftrightsquigarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\curvearrowleft$   \curvearrowleft&lt;/td>
&lt;td>$\curvearrowright$   \curvearrowright&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\circlearrowleft$   \circlearrowleft&lt;/td>
&lt;td>$\circlearrowright$   \circlearrowright&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="ams-二元否定关系符合箭头">AMS 二元否定关系符合箭头&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\nless$   \nless&lt;/td>
&lt;td>$\ngtr$   \ngtr&lt;/td>
&lt;td>$\varsubsetneqq$   \varsubsetneqq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lneq$   \lneq&lt;/td>
&lt;td>$\gneq$  \gneq&lt;/td>
&lt;td>$\varsupsetneqq$   \varsupsetneqq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nleq$   \nleq&lt;/td>
&lt;td>$\ngeq$   \ngeq&lt;/td>
&lt;td>$\nsubseteqq$   \nsubseteqq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nleqslant$   \nleqslant&lt;/td>
&lt;td>$\ngeqslant$   \ngeqslant&lt;/td>
&lt;td>$\nsupseteqq$   \nsupseteqq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lneqq$   \lneqq&lt;/td>
&lt;td>$\gneqq$   \gneqq&lt;/td>
&lt;td>$\nmid$   \nmid&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lvertneqq$   \lvertneqq&lt;/td>
&lt;td>$\gvertneqq$   \gvertneqq&lt;/td>
&lt;td>$\nparallel$   \nparallel&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nleqq$   \nleqq&lt;/td>
&lt;td>$\ngeqq$   \ngeqq&lt;/td>
&lt;td>$\nshortmid$   \nshortmid&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lnsim$   \lnsim&lt;/td>
&lt;td>$\gnsim$   \gnsim&lt;/td>
&lt;td>$\nshortparallel$   \nshortparallel&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lnapprox$  \lnapprox&lt;/td>
&lt;td>$\gnapprox$   \gnapprox&lt;/td>
&lt;td>$\nsim$   \nsim&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nprec$   \nprec&lt;/td>
&lt;td>$\nsucc$   \nsucc&lt;/td>
&lt;td>$\ncong$   \ncong&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\npreceq$   \npreceq&lt;/td>
&lt;td>$\nsucceq$   \nsucceq&lt;/td>
&lt;td>$\nvdash$   \nvdash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\precneqq$   \precneqq&lt;/td>
&lt;td>$\succneqq$   \succneqq&lt;/td>
&lt;td>$\nvDash$   \nvDash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\precnsim$   \precnsim&lt;/td>
&lt;td>$\succnsim$   \succnsim&lt;/td>
&lt;td>$\nVdash$   \nVdash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\precnapprox$   \precnapprox&lt;/td>
&lt;td>$\succnapprox$   \succnapprox&lt;/td>
&lt;td>$\nVDash$   \nVDash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\subsetneq$   \subsetneq&lt;/td>
&lt;td>$\supsetneq$   \supsetneq&lt;/td>
&lt;td>$\ntriangleleft$   \ntriangleleft&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\varsubsetneq$   \varsubsetneq&lt;/td>
&lt;td>$\varsupsetneq$   \varsupsetneq&lt;/td>
&lt;td>$\ntriangleright$   \ntriangleright&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nsubseteq$   \nsubseteq&lt;/td>
&lt;td>$\nsupseteq$   \nsupseteq&lt;/td>
&lt;td>$\ntrianglelefteq$   \ntrianglelefteq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\subsetneqq$   \subsetneqq&lt;/td>
&lt;td>$\supsetneqq$   \supsetneqq&lt;/td>
&lt;td>$\ntrianglerighteq$   \ntrianglerighteq&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nleftarrow$   \nleftarrow&lt;/td>
&lt;td>$\nrightarrow$   \nrightarrow&lt;/td>
&lt;td>$\nleftrightarrow$   \nleftrightarrow&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nLeftarrow$   \nLeftarrow&lt;/td>
&lt;td>$\nRightarrow$   \nRightarrow&lt;/td>
&lt;td>$\nLeftrightarrow$   \nLeftrightarrow&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="ams-二元运算符">AMS 二元运算符&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\dotplus$   \dotplus&lt;/td>
&lt;td>$\centerdot$   \centerdot&lt;/td>
&lt;td>$\intercal$   \intercal&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\ltimes$  \ltimes&lt;/td>
&lt;td>$\rtimes$   \rtimes&lt;/td>
&lt;td>$\divideontimes$   \divideontimes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\Cup$   \Cup or \doublecup&lt;/td>
&lt;td>$\Cap$   \Cap or doublecap&lt;/td>
&lt;td>$\smallsetminus$   \smallsetminus&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\veebar$   \veebar&lt;/td>
&lt;td>$\barwedge$  \barwedge&lt;/td>
&lt;td>$\doublebarwedge$   \doublebarwedge&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\boxplus$   \boxplus&lt;/td>
&lt;td>$\boxminus$   \boxminus&lt;/td>
&lt;td>$\circleddash$   \circleddash&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\boxtimes$   \boxtimes&lt;/td>
&lt;td>$\boxdot$   \boxdot&lt;/td>
&lt;td>$\circledcirc$   \circledcirc&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\leftthreetimes$   \leftthreetimes&lt;/td>
&lt;td>$\rightthreetimes$   \rightthreetimes&lt;/td>
&lt;td>$\circledast$   \circledast&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\curlyvee$   \curlyvee&lt;/td>
&lt;td>$\curlywedge$   \curlywedge&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="ams-其他符号">AMS 其他符号&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\hbar$   \hbar&lt;/td>
&lt;td>$\hslash$   \hslash&lt;/td>
&lt;td>$\Bbbk$   \Bbbk&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\square$   \square&lt;/td>
&lt;td>$\blacksquare$   \blacksquare&lt;/td>
&lt;td>$\circledS$   \circledS&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\vartriangle$   \vartriangle&lt;/td>
&lt;td>$\blacktriangle$   \blacktriangle&lt;/td>
&lt;td>$\complement$   \complement&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\triangledown$   \triangledown&lt;/td>
&lt;td>$\blacktriangledown$   \blacktriangledown&lt;/td>
&lt;td>$\Game$   \Game&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\lozenge$   \lozenge&lt;/td>
&lt;td>$\blacklozenge$   \blacklozenge&lt;/td>
&lt;td>$\bigstar$   \bigstar&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\angle$   \angle&lt;/td>
&lt;td>$\measuredangle$   \measuredangle&lt;/td>
&lt;td>$\sphericalangle$   \sphericalangle&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\diagup$   \diagup&lt;/td>
&lt;td>$\diagdown$   \diagdown&lt;/td>
&lt;td>$\backprime$   \backprime&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\nexists$   \nexists&lt;/td>
&lt;td>$\Finv$   \Finv&lt;/td>
&lt;td>$\varnothing$   \varnothing&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\eth$   \eth&lt;/td>
&lt;td>$\mho$   \mho&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br>
&lt;h3 id="数学字母">数学字母&lt;/h3>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$\mathrm{ABCdef}$   \mathrm{ABCdef}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathit{ABCdef}$   \mathit{ABCdef}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathnormal{ABCdef}$   \mathnormal{ABCdef}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathcal{ABC}$   \mathcal{ABC}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathscr{ABC}$   \mathscr{ABC}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathfrak{ABCdef}$   \mathfrak{ABCdef}&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>$\mathbb{ABC}$   \mathbb{ABC}&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/br></description></item><item><title>[建站日记] Hugo 添加文章日期热力图</title><link>https://yibocat.github.io/posts/jzrj/jzrj_4/</link><pubDate>Fri, 25 Feb 2022 20:11:23 +0800</pubDate><guid>https://yibocat.github.io/posts/jzrj/jzrj_4/</guid><description>&lt;p>每次看到 Github 上的代码贡献图时，总想自己搞一个。但是直接搞似乎很麻烦，于是上网一搜，嘿，有两个开源图表库正好满足我的需求，果断开搞。&lt;/p>
&lt;p>这两个图图表库如下：&lt;/p>
&lt;p>蚂蚁数据可视化 AntV &lt;a href="https://antv-2018.alipay.com/zh-cn/index.html"target="_blank" rel="noopener noreferrer">https://antv-2018.alipay.com/zh-cn/index.html&lt;/a>
&lt;/p>
&lt;p>Apache ECharts &lt;a href="https://echarts.apache.org/zh/index.html"target="_blank" rel="noopener noreferrer">https://echarts.apache.org/zh/index.html&lt;/a>
&lt;/p>
&lt;p>这两个都是非常好的图表库，而我使用的 ECharts（只是因为第一个找到的是 ECharts）。不过有一些功能还是没能实现，有机会尝试用 AntV 再实现一下。&lt;/p>
&lt;p>改造后最后的效果图如下&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/jzrj/jzrj-20220225-01.png" style="width:80%" />&lt;/p>
&lt;/br>
&lt;h3 id="获取">获取&lt;/h3>
&lt;p>ECharts 官方给的安装方式有三种： Github 获取、npm 安装和 CDN 获取，官方建议使用 npm 安装，这里直接使用 CDN 获取：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-html" data-lang="html">&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">script&lt;/span> &lt;span class="na">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;https://cdn.jsdelivr.net/npm/echarts@5.3.0/dist/echarts.min.js&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&amp;lt;/&lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>而下载网址是在 &lt;a href="https://www.jsdelivr.com/package/npm/echarts"target="_blank" rel="noopener noreferrer">https://www.jsdelivr.com/package/npm/echarts&lt;/a>
。&lt;/p>
&lt;p>npm 下载如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">npm install echarts --save
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>另外，官网还提供在线构建，具体可以前往 &lt;a href="https://echarts.apache.org/zh/builder.html"target="_blank" rel="noopener noreferrer">https://echarts.apache.org/zh/builder.html&lt;/a>
。&lt;/p>
&lt;/br>
&lt;h3 id="配置文章数据">配置文章数据&lt;/h3>
&lt;p>首先配置生成热力图的模块&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-html" data-lang="html">&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">div&lt;/span> &lt;span class="na">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;heatmap&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&amp;lt;/&lt;/span>&lt;span class="nt">div&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>然后编写 javascript 脚本。&lt;/p>
&lt;p>初始化图标并绑定到热力图模块，并设定图表响应容器的大小变化&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-javascript" data-lang="javascript">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">var&lt;/span> &lt;span class="nx">myChart&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">echarts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">document&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">getElementById&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;heatmap&amp;#39;&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">onresize&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kd">function&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">myChart&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">resize&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;h3 id="生成文章数据">生成文章数据&lt;/h3>
&lt;p>生成热力图的 option 结构在 &lt;a href="https://echarts.apache.org/zh/option.html#calendar"target="_blank" rel="noopener noreferrer">https://echarts.apache.org/zh/option.html#calendar&lt;/a>
。这里我们主要要解决的是两个问题：&lt;/p>
&lt;blockquote>
&lt;p>1.如何将日历的日期设置为去年今日到今日日期区间&lt;/p>
&lt;p>2.将文章的日期和字数传入 ECharts&lt;/p>
&lt;/blockquote>
&lt;p>日历时间在 &lt;code>series&lt;/code> 的 &lt;code>data&lt;/code> 中设置，支持 &lt;code>data([day,number])&lt;/code> 的列表形式，我们的目的则是将所有发布的文章的数字作统计，然后传入列表 &lt;code>data&lt;/code> 中。&lt;/p>
&lt;p>好消息是 Hugo 支持在 &lt;code>html&lt;/code> 中写入代码，随后 Hugo 会自动对其进行编码。接着上面代码&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">var&lt;/span> &lt;span class="nx">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">{{&lt;/span> &lt;span class="nx">range&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">Site&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">RegularPages&lt;/span> &lt;span class="p">}}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nx">data&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">push&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{{&lt;/span> &lt;span class="p">.&lt;/span>&lt;span class="nx">PublishDate&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">Format&lt;/span> &lt;span class="s2">&amp;#34;2006-01-02&amp;#34;&lt;/span> &lt;span class="p">}},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{{&lt;/span> &lt;span class="p">.&lt;/span>&lt;span class="nx">WordCount&lt;/span> &lt;span class="p">}},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">{{&lt;/span>&lt;span class="o">-&lt;/span> &lt;span class="nx">end&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="p">}}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这里我们将所有发布文章的数据保存在了 &lt;code>data&lt;/code> 中（注意：该段代码需要写在 html 中，若单独编写 js 文件，最后并不会奏效，因为 Hugo 并不会去编译 js 文件）。&lt;/p>
&lt;/br>
&lt;h3 id="设置日期时间">设置日期时间&lt;/h3>
&lt;p>官方给出的日期设置时间非常简单，从当前一年的一月一日开始，随机生成 365 个数据。而 javascript 中有一段代码可以获取当前时间和当前时间前推任意时间&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">var&lt;/span> &lt;span class="nx">sdtime2&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="nx">setHours&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">sdtime1&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">getHours&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="c1">//小时
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">var&lt;/span> &lt;span class="nx">sdtime3&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="nx">setDate&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="nx">getDate&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">7&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="c1">//7天
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">var&lt;/span> &lt;span class="nx">sdtime4&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="nx">setMonth&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="nx">getMonth&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="c1">//一个月
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kd">var&lt;/span> &lt;span class="nx">sdtime5&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="nx">setFullYear&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">().&lt;/span>&lt;span class="nx">getFullYear&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="c1">//一年
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="nx">console&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">sdtime2&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">Format&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;yyyy-MM-dd HH:mm:ss&amp;#34;&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>我们可以下一个函数，生成当前时间推至前若干个月的日历区间&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">function&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">months&lt;/span>&lt;span class="p">){&lt;/span> &lt;span class="c1">// 计算转换日期
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kd">var&lt;/span> &lt;span class="nx">startDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">mill&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">startDate&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">setMonth&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="nx">startDate&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">getMonth&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="nx">months&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">endDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">+&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">startDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">+&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">mill&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">endDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">echarts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">format&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">formatTime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;yyyy-MM-dd&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">endDate&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">startDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">echarts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">format&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">formatTime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;yyyy-MM-dd&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">startDate&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">showmonth&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">showmonth&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">push&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">startDate&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">endDate&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nx">showmonth&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>该函数返回一个 &lt;code>[startDate,endDate]&lt;/code> 数据，表示日历的开始日期和结束日期。&lt;/p>
&lt;/br>
&lt;h3 id="图表配置">图表配置&lt;/h3>
&lt;p>上文已经提到过，日历的配置在 &lt;a href="https://echarts.apache.org/zh/option.html#calendar"target="_blank" rel="noopener noreferrer">https://echarts.apache.org/zh/option.html#calendar&lt;/a>
有非常详细的说明，我们可以直接使用官网样例给出的配置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="nx">option&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">title&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">top&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">left&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;center&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">text&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;数数敲了几个字？&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">tooltip&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">visualMap&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">min&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">max&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">5000&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">type&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;piecewise&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">orient&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;horizontal&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">left&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;center&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">top&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">65&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">calendar&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">top&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">120&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">left&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">right&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">cellSize&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;auto&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">12&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">range&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">12&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="c1">// 设置转换的日历日期区间
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nx">itemStyle&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">borderWidth&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">yearLabel&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="nx">show&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">series&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">type&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;heatmap&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">coordinateSystem&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;calendar&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">data&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="nx">data&lt;/span> &lt;span class="c1">// 加载文章数据
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nx">myChart&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">setOption&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">option&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>大功告成！日历贡献图就完成了！所有代码 &lt;a href="https://shared.snipper.app/snippet/D67FD48F-2089-4A13-9A2B-37E6B48D3F06"target="_blank" rel="noopener noreferrer">https://shared.snipper.app/snippet/D67FD48F-2089-4A13-9A2B-37E6B48D3F06&lt;/a>
或者下面的未设置 hugo 获取文章数据的代码，转而使用随机数据&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-html" data-lang="html">&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">div&lt;/span> &lt;span class="na">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;ttt&amp;#34;&lt;/span> &lt;span class="na">style&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> width: 600px;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> height: 300px;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> margin-top: 15px;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> margin-bottom: 10px;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> padding: 2px;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> border: 1px solid;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> border-radius: 10px;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> border-width: 0.5px;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s"> text-align: center;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;gt;&amp;lt;/&lt;/span>&lt;span class="nt">div&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">script&lt;/span> &lt;span class="na">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;https://cdn.jsdelivr.net/npm/echarts@5.3.0/dist/echarts.min.js&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&amp;lt;/&lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">script&lt;/span> &lt;span class="na">type&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;text/javascript&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">chartDom&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">document&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">getElementById&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;ttt&amp;#39;&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">myChart&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">echarts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">init&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">chartDom&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">option&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// 计算日期并转换格式
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kd">var&lt;/span> &lt;span class="nx">startDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">year_Mill&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">startDate&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">setFullYear&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="nx">startDate&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">getFullYear&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">));&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">startDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">+&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">year_Mill&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">endDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">+&lt;/span>&lt;span class="k">new&lt;/span> &lt;span class="nb">Date&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">dayTime&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3600&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">24&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">1000&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="kd">var&lt;/span> &lt;span class="nx">time&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">startDate&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="nx">time&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="nx">endDate&lt;/span>&lt;span class="p">;&lt;/span> &lt;span class="nx">time&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="nx">dayTime&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">data&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">push&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">echarts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">format&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">formatTime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;yyyy-MM-dd&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="nx">time&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">floor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">Math&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">random&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mi">10000&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">startDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">echarts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">format&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">formatTime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;yyyy-MM-dd&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">startDate&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">endDate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">echarts&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">format&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">formatTime&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;yyyy-MM-dd&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">endDate&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="nx">startDate&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="nx">endDate&lt;/span>&lt;span class="p">];&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nx">option&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">title&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">top&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">left&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;center&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">text&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;Daily Step Count&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">tooltip&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">visualMap&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">min&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">max&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">10000&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">type&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;piecewise&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">orient&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;horizontal&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">left&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;center&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">top&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">65&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">calendar&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">top&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">120&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">left&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">right&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">30&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">cellSize&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;auto&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">13&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">range&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="nx">rangeArr&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">itemStyle&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">borderWidth&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">yearLabel&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="nx">show&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">series&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">type&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;heatmap&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">coordinateSystem&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;calendar&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">data&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="nx">data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nx">option&lt;/span> &lt;span class="o">&amp;amp;&amp;amp;&lt;/span> &lt;span class="nx">myChart&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">setOption&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">option&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;/&lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>但是，但是！等等&amp;hellip;这样配置的热力图似乎并不完美，在不同屏幕宽度下的展示效果会非常差，所以要设置一下适配屏幕宽度的热力图。&lt;/p>
&lt;/br>
&lt;h3 id="优化">优化&lt;/h3>
&lt;p>我们分别设置在不同屏幕宽度下要显示的日期区间，将如下代码放至 &lt;code>heatmap_width&lt;/code> 函数下面，&lt;code>option&lt;/code> 上面(javascript 并不是特别拿手，只能使用最笨的方法了😥)&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(max-width: 320px)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(max-width: 400px)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(max-width: 550px)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(max-width: 1021px)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">12&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(max-width: 1400px)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">9&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(max-width: 1920px)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">12&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>&lt;span class="k">else&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(max-width: 2560px)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">rangeArr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nx">heatmap_width&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">24&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;h3 id="暗模式">暗模式&lt;/h3>
&lt;p>该图表其实本身自带暗模式，但是为了和本网站相匹配，所以决定自己动手实现暗模式。&lt;/p>
&lt;p>为此我分别写了两个函数&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">function&lt;/span> &lt;span class="nx">LightOption&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kd">function&lt;/span> &lt;span class="nx">DarkOption&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>将 &lt;code>option&lt;/code> 配置分别放入两个函数中，再单独进行颜色配置&lt;/p>
&lt;p>然后使用 &lt;code>(window.matchMedia('(prefers-color-scheme: dark)').matches)&lt;/code> 获取当前系统的暗模式方法&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(prefers-color-scheme: dark)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span> &lt;span class="c1">// 暗模式
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nx">DarkOption&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">else&lt;/span>&lt;span class="p">{&lt;/span> &lt;span class="c1">// 亮模式
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nx">LightOption&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>加入屏幕监听功能&lt;/strong>&lt;/p>
&lt;p>当我们切换明暗模式时，以上代码对于网站并不会直接生效，需要刷新一遍才可以生效。所以加入监听功能使得网站一旦切换明暗模式则理科生效&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">var&lt;/span> &lt;span class="nx">listeners&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span> &lt;span class="c1">// 模式监听
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="nx">dark&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">mediaQueryList&lt;/span> &lt;span class="p">)=&amp;gt;{&lt;/span> &lt;span class="c1">// 暗模式
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">mediaQueryList&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">DarkOption&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">light&lt;/span>&lt;span class="o">:&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">mediaQueryList&lt;/span>&lt;span class="p">)=&amp;gt;{&lt;/span> &lt;span class="c1">// 亮模式
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">mediaQueryList&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matches&lt;/span>&lt;span class="p">){&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">LightOption&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(prefers-color-scheme: dark)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">addListener&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">listeners&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">dark&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">window&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">matchMedia&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;(prefers-color-scheme: light)&amp;#39;&lt;/span>&lt;span class="p">).&lt;/span>&lt;span class="nx">addListener&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">listeners&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">light&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>至此，总算完成了。&lt;/p>
&lt;/br>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>ECharts 功能很强大，但是却并不能生成如同 Github 代码贡献图的样子，对于每一天的小方块不能设置更好的样式。不知道 AntV 行不行。总之，以后有时间再研究研究。&lt;/p></description></item><item><title>[建站日记] 动态打字特效</title><link>https://yibocat.github.io/posts/jzrj/jzrj_3/</link><pubDate>Sat, 19 Feb 2022 00:32:35 +0800</pubDate><guid>https://yibocat.github.io/posts/jzrj/jzrj_3/</guid><description>&lt;p>在网站上有一个动态打字特效当然是一件很酷的事啊。无意之间找到了一个动态打字特效插件 &lt;a href="https://mattboldt.com/demos/typed-js/"target="_blank" rel="noopener noreferrer">Typed-js&lt;/a>
。该插件还实现了各种自定义满足不同的需求。可以说是非常非常好用了，而且安装使用也是非常的简单。&lt;/p>
&lt;p>该插件的作者是 Matt Boldt，这是他的主页：&lt;a href="https://mattboldt.com/"target="_blank" rel="noopener noreferrer">https://mattboldt.com/&lt;/a>
。&lt;/p>
&lt;p>Typed-js 的 Github：&lt;a href="https://github.com/mattboldt/typed.js/"target="_blank" rel="noopener noreferrer">https://github.com/mattboldt/typed.js/&lt;/a>
。&lt;/p>
&lt;p>该插件的自定义 Demo 如下：&lt;a href="http://mattboldt.github.io/typed.js/"target="_blank" rel="noopener noreferrer">http://mattboldt.github.io/typed.js/&lt;/a>
。&lt;/p>
&lt;p>下面我们大概讲一下该插件的使用方法。&lt;/p>
&lt;/br>
&lt;h3 id="安装方法">安装方法&lt;/h3>
&lt;p>官方提供了三种安装方法：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">npm install typed.js
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">yarn add typed.js
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bower install typed.js
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>或者使用 &lt;code>script&lt;/code> CDN 导入&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-html" data-lang="html">&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">script&lt;/span> &lt;span class="na">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;https://cdn.jsdelivr.net/npm/typed.js@2.0.12&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&amp;lt;/&lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;h3 id="使用">使用&lt;/h3>
&lt;p>从官方给出的例子来看&lt;/p>
&lt;p>This is really all you need to get going.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">// Can also be included with a regular script tag
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span>&lt;span class="kr">import&lt;/span> &lt;span class="nx">Typed&lt;/span> &lt;span class="nx">from&lt;/span> &lt;span class="s1">&amp;#39;typed.js&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kd">var&lt;/span> &lt;span class="nx">options&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">strings&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;&amp;lt;i&amp;gt;First&amp;lt;/i&amp;gt; sentence.&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;&amp;amp;amp; a second sentence.&amp;#39;&lt;/span>&lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">typeSpeed&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">40&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">};&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kd">var&lt;/span> &lt;span class="nx">typed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nx">Typed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;.element&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">options&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>上面这个例子展示了基本的使用方法。我们再来看一个更具体的例子&lt;/p>
&lt;script>
var typed = new Typed('#typed', {
stringsElement: '#typed-strings'
});
&lt;/script>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-html" data-lang="html">&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">script&lt;/span> &lt;span class="na">src&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;https://cdn.jsdelivr.net/npm/typed.js@2.0.12&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&amp;lt;/&lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">typed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nx">Typed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;#typed&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">stringsElement&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;#typed-strings&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;/&lt;/span>&lt;span class="nt">script&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">div&lt;/span> &lt;span class="na">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;typed-strings&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">p&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>Typed.js is a &lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">strong&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>JavaScript&lt;span class="p">&amp;lt;/&lt;/span>&lt;span class="nt">strong&lt;/span>&lt;span class="p">&amp;gt;&lt;/span> library.&lt;span class="p">&amp;lt;/&lt;/span>&lt;span class="nt">p&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">p&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>It &lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">em&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>types&lt;span class="p">&amp;lt;/&lt;/span>&lt;span class="nt">em&lt;/span>&lt;span class="p">&amp;gt;&lt;/span> out sentences.&lt;span class="p">&amp;lt;/&lt;/span>&lt;span class="nt">p&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;/&lt;/span>&lt;span class="nt">div&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">&amp;lt;&lt;/span>&lt;span class="nt">span&lt;/span> &lt;span class="na">id&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;typed&amp;#34;&lt;/span>&lt;span class="p">&amp;gt;&amp;lt;/&lt;/span>&lt;span class="nt">span&lt;/span>&lt;span class="p">&amp;gt;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>stringElement&lt;/code> 为定义的外部动态打字文字，这里也可以直接使用 &lt;code>strings:[]&lt;/code> 来替代。 &lt;code>typed-strings&lt;/code> 表示要插入动态打字特效的模块。一般的自定义则在 &lt;code>new Typed()&lt;/code> 中设置。&lt;/p>
&lt;/br>
&lt;h3 id="样式设置">样式设置&lt;/h3>
&lt;p>&lt;code>css&lt;/code> 动画是基于 &lt;code>javascript&lt;/code> 初始化构建的，所以可以在 &lt;code>css&lt;/code> 中设置各种样式&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-css" data-lang="css">&lt;span class="line">&lt;span class="cl">&lt;span class="c">/* 光标 */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">typed-cursor&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c">/* 如果设置了淡出选项 */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">.&lt;/span>&lt;span class="nc">typed-fade-out&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;h3 id="自定义">自定义&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">var&lt;/span> &lt;span class="nx">typed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="nx">Typed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;.element&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {array} strings 要输入的字符
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {string} stringsElement 包含字符串 children 的元素的 ID
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">strings&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">[&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;These are the default values...&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;You know what you should do?&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;Use your own!&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;Have a great day!&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">],&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">stringsElement&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">null&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {number} typeSpeed 类型速度，单位：毫秒
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">typeSpeed&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {number} startDelay 输入开始前的时间，单位：毫秒
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">startDelay&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {number} backSpeed 退格速度，单位：毫秒
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">backSpeed&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} smartBackspace 只退格与前一个字符串不匹配的内容
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">smartBackspace&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">true&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} shuffle 随机播放字符串
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">shuffle&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {number} backDelay 退格前的延迟时间，单位：毫秒
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">backDelay&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">700&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} fadeOut 用淡出替代退格
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {string} fadeOutClass 淡出动画
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} fadeOutDelay 淡出延迟，单位：毫秒
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">fadeOut&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">fadeOutClass&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;typed-fade-out&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">fadeOutDelay&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="mi">500&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} loop 进行循环播放
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {number} loopCount 循环播放次数
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">loop&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">loopCount&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">Infinity&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} showCursor 显示光标
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {string} cursorChar 光标字符串，一般为&amp;#39;|&amp;#39;，我个人比较喜欢设置为背景定宽
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} autoInsertCss 插入 CSS 光标并淡出到 HTML &amp;lt;head&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">showCursor&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">true&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">cursorChar&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;|&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">autoInsertCss&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">true&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {string} attr 输出的字符串属性
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Ex: input placeholder, value, or just HTML text
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">attr&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">null&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {boolean} bindInputFocusEvents 如果 el 是文本输入的话则绑定到焦点和模糊
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">bindInputFocusEvents&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="kc">false&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @property {string} contentType 内容类型为 html 或者 null
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">contentType&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s1">&amp;#39;html&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 开始输入之前执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onBegin&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 所有输入完成执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onComplete&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 每个字符串输入之前执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {number} arrayPos
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">preStringTyped&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">arrayPos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 每个字符串输入之后执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {number} arrayPos
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onStringTyped&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">arrayPos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 循环过程中输入最后一个字符串后执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onLastStringBackspaced&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 输入停止执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {number} arrayPos
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onTypingPaused&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">arrayPos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * Typing has been started after being stopped
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {number} arrayPos
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onTypingResumed&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">arrayPos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 重置之后执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onReset&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 停止之后执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {number} arrayPos
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onStop&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">arrayPos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 开始之后执行
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {number} arrayPos
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onStart&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">arrayPos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * 销毁之后
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> * @param {Typed} self
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm"> */&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">onDestroy&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="nx">self&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">=&amp;gt;&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">});&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;h3 id="举例">举例&lt;/h3>
&lt;p>&lt;a href="http://mattboldt.github.io/typed.js/"target="_blank" rel="noopener noreferrer">http://mattboldt.github.io/typed.js/&lt;/a>
&lt;/p>
&lt;/br></description></item><item><title>[深度学习笔记] 深度学习第 5 篇——正向传播与反向传播</title><link>https://yibocat.github.io/posts/dl/sdxx_5/</link><pubDate>Thu, 17 Feb 2022 21:48:32 +0800</pubDate><guid>https://yibocat.github.io/posts/dl/sdxx_5/</guid><description>&lt;p>什么是正向传播？正向传播是按照神经网络的输入到输出的顺序，依次计算从模型中间变量的过程。而反向传播则是计算神经网络的参数梯度的过程。在训练神经网络时，正向传播与反向传播是相互依赖的。正向传播依次计算模型中间变量，反向传播再根据变量对参数进行梯度求导。所以可以这么理解，在参数进行初始化之后对模型进行交替的正向传播与反向传播，并根据反向传播计算的梯度迭代模型参数。&lt;/p>
&lt;/br>
&lt;h3 id="正向传播">正向传播&lt;/h3>
&lt;p>前几篇文章讲的基本上都是正向传播，这里我们简单地再推导一下。&lt;/p>
&lt;p>假设输入为 $x\in \mathbb{R}^d$ ，不考虑偏差，中间变量为 $z\in\mathbb{R}^h$，$\pmb{W}^{(1)}\in\mathbb{R}^{h\times d}$ 为隐藏层的权重参数。设 $\phi()$ 为激活函数，则有
$$
z=\pmb{W}^{(1)}x,\newline
\pmb{h} = \phi(z)
$$
这里 $\pmb{h}$ 表示得到的隐藏层变量，也就是中间变量。&lt;/p>
&lt;p>假设输出层参数为 $\pmb{W}^{(2)}\in\mathbb{R}^{q\times h}$ ，则得到的向量长度为 $q$ 的输出层变量为
$$
\pmb{o}=\pmb{W}^{(2)}\pmb{h}
$$
我们假设损失函数为 $\ell$ ，样本标签为 $y$ ，则样本损失项为
$$
L=\ell(\pmb{o},y)
$$
引入正则化项 $s$
$$
s=\frac{\lambda}{2}(\vert\vert\pmb{W}^{(1)}\vert\vert^2_F+\vert\vert\pmb{W}^{(2)}\vert\vert^2_F)
$$
得到模型在给定样本上的损失为 $J = L+s$ ，我们称 $J$ 为给定样本的&lt;strong>目标函数&lt;/strong>。&lt;/p>
&lt;/br>
&lt;h3 id="反向传播">反向传播&lt;/h3>
&lt;p>我们说反向传播是计算神经网络参数梯度的方法，其基本原理是微积分中的链式法则。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>链式法则&lt;/strong>&lt;/p>
&lt;p>假设 $y=g(x)，z=f(y)$，则 $z=h(x)=f(g(x))$ 。对这两个函数分别求导有
$$
\frac{dy}{dx}=g&amp;rsquo;(x) \newline
\frac{dz}{dy}=f&amp;rsquo;(y)
$$
根据链式法则，有
$$
h&amp;rsquo;(x)=\frac{dz}{dx}=\frac{dz}{dy}\cdot\frac{dy}{dx}
$$
即复合函数求导使用&lt;strong>乘法法则&lt;/strong>，或称为&lt;strong>链式法则&lt;/strong>。在反向传播中我们可以完美地使用这个法则。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>反向传播&lt;/strong>&lt;/p>
&lt;p>我们将链式法则应用到反向传播中。在上文中，我们的模型参数为 $\pmb{W}^{(1)}$ 和 $\pmb{W}^{(2)}$ ，因此反向传播所要作的计算是 $\frac{\partial J}{\partial\pmb{W}^{(1)}}$ 和 $\frac{\partial J}{\partial\pmb{W}^{(2)}}$，反向传播的计算次序与正向传播计算次序恰恰相反。我们求目标函数关于 $L$ 和 $s$ 的梯度，有
$$
\frac{\partial J}{\partial L}=1,\quad \frac{\partial J}{\partial s}=1
$$
然后根据链式法则计算目标函数对于输出层变量的梯度
$$
\frac{\partial J}{\partial o}=\frac{\partial J}{\partial L}\cdot\frac{\partial L}{\partial o}=\frac{\partial L}{\partial o}
$$
接着计算正则化项对于两个参数的梯度
$$
\frac{\partial s}{\partial\pmb{W}^{(1)}}=\lambda\pmb{W}^{(1)},\quad
\frac{\partial s}{\partial\pmb{W}^{(2)}}=\lambda\pmb{W}^{(2)}
$$
然后先计算靠近输出层参数的梯度 $\frac{\partial J}{\partial\pmb{W}^{(2)}}\in\mathbb{R}^{q\times h}$
$$
\begin{aligned}
\frac{\partial J}{\partial\pmb{W}^{(2)}} &amp;amp; = \frac{\partial J}{\partial o}\centerdot\frac{\partial o}{\partial\pmb{W}^{(2)}}+\frac{\partial J}{\partial s}\centerdot\frac{\partial s}{\partial \pmb{W}^{(2)}} \newline
&amp;amp; = \frac{\partial J}{\partial o}\pmb{h}^{\top}+\lambda\pmb{W}^{(2)}
\end{aligned}
$$
然后继续沿着输出层向隐藏层反向传播，隐藏层变量的梯度为
$$
\frac{\partial J}{\partial \pmb{h}} = \frac{\partial J}{\partial o}\centerdot\frac{\partial o}{\partial\pmb{h}}={\pmb{W}^{(2)}}^{\top}\frac{\partial J}{\partial o}
$$
由于激活函数 $\phi$ 按元素运算，中间变量 $z$ 的梯度 $\frac{\partial J}{\partial z}$ 的计算需要使用&lt;strong>按元素乘法&lt;/strong> ，符号位 $\odot$
$$
\frac{\partial J}{\partial z}=\frac{\partial J}{\partial \pmb{h}}\cdot\frac{\partial \pmb{h}}{\partial z}=\frac{\partial J}{\partial\pmb{h}}\odot\phi&amp;rsquo;(z)
$$
最后我们得到靠近输入层的模型参数的梯度 $\frac{\partial J}{\partial\pmb{W}^{(1)}}$
$$
\begin{aligned}
\frac{\partial J}{\partial \pmb{W}^{(1)}} &amp;amp;= \frac{\partial J}{\partial z}\cdot\frac{\partial z}{\partial\pmb{W}^{(1)}}+\frac{\partial J}{\partial s}\cdot\frac{\partial s}{\partial \pmb{W}^{(1)}} \newline
&amp;amp; = \frac{\partial J}{\partial z}x^{\top}+\lambda\pmb{W}^{(1)}
\end{aligned}
$$
&lt;/br>&lt;/p>
&lt;h3 id="深度学习模型训练">深度学习模型训练&lt;/h3>
&lt;p>看了这么些公式可能都忘记我们最初是要干什么的，我们为什么要算这么些公式？我们来回顾一下。&lt;/p>
&lt;p>我们训练模型的目的是为了得到最优的参数以满足我们的模型，那么什么样的模型最优呢？可以认为是损失函数最小的模型就是最优的。那么如何降低损失函数呢，我们知道模型在给定样本上的损失函数为 $J$ ，我们要让损失函数最小化，所以通过梯度下降法迭代最接近的参数，使用梯度下降法进行迭代就要通过反向传播算法计算参数梯度。我们回忆一下带有 $L2$ 参数正则化项的参数迭代公式
$$
\omega\gets(1-\frac{\eta\lambda}{\vert \mathcal{B}\vert})\omega-\frac{\eta}{\vert \mathcal{B}\vert}\sum_{i\in\mathcal{B}}\frac{\partial \ell^{(i)}(\omega,b)}{\partial \omega}
$$
这么一看，后面的一项正是我们所推导的参数迭代公式。这就是反向传播！&lt;/p>
&lt;/br>
&lt;p>一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。&lt;/p>
&lt;p>所以，模型参数初始化之后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。而正是由于神经网络的特殊性，正向传播结束后不能立即释放变量内存，从而导致神经网络的训练往往占用很大的内存的原因。&lt;/p>
&lt;/br>
&lt;h3 id="参考">参考&lt;/h3>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/40378224"target="_blank" rel="noopener noreferrer">Back Propagation（梯度反向传播）实例讲解 - HexUp的文章&lt;/a>
&lt;/p>
&lt;p>&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/"target="_blank" rel="noopener noreferrer">《动手学深度学习》PyTorch 版，阿斯顿·张、李沐&lt;/a>
&lt;/p></description></item><item><title>[深度学习笔记] 深度学习第 4 篇——权重衰减与丢弃法</title><link>https://yibocat.github.io/posts/dl/sdxx_4/</link><pubDate>Tue, 15 Feb 2022 14:26:12 +0800</pubDate><guid>https://yibocat.github.io/posts/dl/sdxx_4/</guid><description>&lt;p>权重衰减与丢弃法是常用的正则化方法，本篇文章我们尝试手动实现权重衰减与丢弃法，并且对其进行评估。&lt;/p>
&lt;/br>
&lt;h3 id="权重衰减">权重衰减&lt;/h3>
&lt;p>我们知道高复杂度模型与样本数据不匹配时会发生过拟合与欠拟合现象，而应对过拟合的方法，一方面是增大训练样本数据量，另外一种方法是限制模型复杂度。一般来说增大样本数据量来匹配神经网络模型复杂度的方法并不是一个好方法，而使用正则化手段对模型添加正则化项可以有效地限制模型复杂度从而防止过拟合。&lt;strong>权重衰减（Weight Decay）&lt;/strong> 就是一种有效的正则化方法，即在每次参数更新时，引入一个衰减系数。&lt;/p>
&lt;p>在标准的随机梯度下降中，权重衰减正则化与 $L2$ 范数正则化等价，但是在一些较复杂的优化方法中（Adam）并不等价。这里回顾一下 $L2$ 范数正则化。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>$L2$ 范数正则化&lt;/strong>&lt;/p>
&lt;p>$L2$ 范数正则化在模型原损失函数的基础上增加了 $L2$ 范数正则化项，以此来限制参数从而使得模型复杂度尽可能简单。$L2$ 范数正则化项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。&lt;/p>
&lt;p>我们看线性回归损失函数为
$$
\ell(\pmb{\omega},b)=\frac{1}{N}\sum_{i=1}^N\frac{1}{2}(\pmb{x}^{(i)}\pmb{\omega}+b-\pmb{y}^{(i)})^2
$$
其中 $\pmb{\omega}$ 为权重参数向量，$b$ 为偏差参数，样本 $i $ 的输入为 $\pmb{x}^{(i)}$ ，该样本标签为 $\pmb{y}^{(i)}$ ，样本数目为 $N$ 。则带有 $L2$ 范数正则化项的损失函数为
$$
\ell(\pmb{\omega},b)+\frac{\lambda}{2N}\vert\vert\pmb{\omega}\vert\vert^2
$$
这里 $\lambda&amp;gt;0$ 为超参数。当 $\lambda$ 接近 0 时，正则化项对损失函数的影响将逐渐降低。&lt;/p>
&lt;p>$L2$ 范数平方 $\vert\vert\pmb{\omega}\vert\vert^2$ 展开后得到 $\sum_{i=1}^N\omega_i^2$ 。我们在小批量随机梯度下降中将线性回归中的权重 $\pmb{\omega}$ 的迭代方式
$$
\omega \gets \omega-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\frac{\partial \ell^{(i)}(\omega,b)}{\partial \omega}
$$
更改为如下方式
$$
\omega\gets(1-\frac{\eta\lambda}{\vert \mathcal{B}\vert})\omega-\frac{\eta}{\vert \mathcal{B}\vert}\sum_{i\in\mathcal{B}}\frac{\partial \ell^{(i)}(\omega,b)}{\partial \omega}
$$
可以看到， $L2$ 范数正则化将权重乘以一个小于 1 的数后再减去不含正则化项的梯度。所以 $L2$ 范数正则化也叫做权重衰减。&lt;/p>
&lt;/br>
&lt;h3 id="权重衰减手动编码实验">权重衰减手动编码实验&lt;/h3>
&lt;p>&lt;strong>生成数据集&lt;/strong>&lt;/p>
&lt;p>设样本数据集特征的维度为 $p$ ，对于数据集中的样本使用如下线性模型
$$
y = \sum_{i=1}^p0.01x_i+0.05+\epsilon
$$
这里权重为 0.01，$\epsilon$ 为噪声项，其服从均值为 0，标准差为0.01 的高斯分布。同时，我们为了更好的显示过拟合效果，设置维度 $p$ 为 200，训练数据集数量设置为 20。&lt;/p>
&lt;p>导入必须包&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="o">%&lt;/span>&lt;span class="n">matplotlib&lt;/span> &lt;span class="n">inline&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.nn&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">nn&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">sys&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">d2lzh_pytorch&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">d2l&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>生成数据集&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">n_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_imputs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">20&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">100&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">200&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">true_w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">true_b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_imputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.05&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 生成数据集&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rand&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">n_train&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">n_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_imputs&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">labels&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">true_w&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">true_b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">labels&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_features&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">features&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="n">n_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:],&lt;/span> &lt;span class="n">features&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n_train&lt;/span>&lt;span class="p">:,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_labels&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_labels&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="n">n_train&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">n_train&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>初始化模型参数&lt;/strong>&lt;/p>
&lt;p>定义一个初始化模型参数的函数&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">init_params&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 随机初始化参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>$L2$ 范数正则化项&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">l2_penalty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义和训练模型&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">100&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.003&lt;/span> &lt;span class="c1"># 小批量为 1，迭代周期 100&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linreg&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">squared_loss&lt;/span> &lt;span class="c1"># net 使用线性回归，损失函数使用平方差损失&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dataset&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">utils&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TensorDataset&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">train_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_labels&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_iter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">utils&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DataLoader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dataset&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">shuffle&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">fit_and_plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lambd&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">init_params&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_ls&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_ls&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_epochs&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 添加了L2范数惩罚项&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">lambd&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">l2_penalty&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 增加正则化项的损失函数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># 梯度清零&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">backward&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># 反向传播计算&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sgd&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 随机梯度下降法&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_ls&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">train_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">train_labels&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">test_ls&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">test_features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">test_labels&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">semilogy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">train_ls&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;epochs&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;loss&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">test_ls&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;train&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;test&amp;#39;&lt;/span>&lt;span class="p">])&lt;/span> &lt;span class="c1"># 画图&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;L2 norm of w:&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">norm&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">())&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>观察过拟合&lt;/strong>&lt;/p>
&lt;p>首先我们让 $\lambda=0.005$ ，因为 $\lambda$ 接近 0，所以正则化项的影响非常小。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">fit_and_plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lambd&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.005&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>输出&lt;/p>
&lt;pre tabindex="0">&lt;code>L2 norm of w: 12.12175464630127
&lt;/code>&lt;/pre>&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-06.png" style="width:50%;border:none;" />&lt;/p>
&lt;p>我们可以看到训练误差远远小于测试误差，这是很明显的过拟合。&lt;/p>
&lt;p>然后我们增大 $\lambda$ ，设置为 $\lambda=4$&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">fit_and_plot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">lambd&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>输出&lt;/p>
&lt;pre tabindex="0">&lt;code>L2 norm of w: 0.07619155943393707
&lt;/code>&lt;/pre>&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-07.png" style="width:50%;border:none;" />&lt;/p>
&lt;p>可以明显的看到测试误差有所下降，过拟合现象有所缓解。另外，权重参数的L2L_2&lt;em>L&lt;/em>2范数比不使用权重衰减时的更小，此时的权重参数更接近0。&lt;/p>
&lt;/br>
&lt;h3 id="丢弃法">丢弃法&lt;/h3>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-08.png" style="width:50%;" />&lt;/p>
&lt;p>我们假设一个模型，其输入个数为 4 个，隐藏单元为 5，则隐藏单元 $h_i$ 的计算表达式为
$$
h_i=\phi(x_1\omega_{1i}+x_2\omega_{2i}+x_3\omega_{3i}+x_4\omega_{4i}+b_i)
$$
这里 $\phi$ 表示激活函数。当我们对隐藏层使用丢弃法时，该层的隐藏单元会有一定的概率被丢弃掉，我们设丢弃概率为 $p$ ，则一个隐藏单元有 $p$ 概率会被清零，有 $1-p$ 的概率该隐藏单元除以 $1-p$ 进行拉伸。而丢弃概率是超参数。&lt;/p>
&lt;p>设随机变量 $\xi_i$ 为 0 和 1 的概率为 $p$ 和 $1-p$ ，则该随机变量服从伯努利分布。我们设隐藏单元 $h&amp;rsquo;_i$ 为
$$
h&amp;rsquo;_i=\frac{\xi_i}{1-p}h_i
$$
因为 $\xi_i$ 服从伯努利分布，所以其期望为 $E(\xi_i)=1-p$ ，所以
$$
E(h&amp;rsquo;_i)=\frac{E(\xi_i)}{1-p}h_i=h_i
$$
得到结论：&lt;strong>丢弃法不改变输入的期望值&lt;/strong>！&lt;/p>
&lt;p>我们看一下上述模型使用丢弃法后的神经网络结构&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-09.png" style="width:50%;" />&lt;/p>
&lt;p>可以看到 $h_2$ 和 $h_5$ 都被清零，因此反向传播时与 $h_2$ 和 $h_5$ 相关的权重的梯度为 0。&lt;/p>
&lt;p>为什么丢弃法起到正则化的作用？和正则化一样，丢弃法也是为了从减小模型复杂度出发来防止过拟合的，因为随机减少隐藏单元和缩小维度是一样的道理。邱锡鹏教授的《神经网络与深度学习》中给出了两种解释：&lt;strong>集成学习角度&lt;/strong>的解释和&lt;strong>贝叶斯学习角度&lt;/strong>的解释。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>集成学习角度的解释&lt;/strong>&lt;/p>
&lt;p>每做一次丢弃，相当于从原始的网络中采样得到一个子网络，若一个神经网络有 $n$ 个神经元，那么总共可以采样出 $2^n$ 个子网络。每次迭代都相当于训练一个不同的子网络，这些子网络都共享原始网络的参数，所以最终的网络可以近似看作是集成了指数级个不同网络的组合模型。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>贝叶斯学习角度的解释&lt;/strong>（不是太明白&amp;hellip;&amp;hellip;）&lt;/p>
&lt;p>丢弃法可以解释为一种贝叶斯学习的近似，用 $y=f(\pmb{x};\theta)$ 来表示要学习的神经网络，贝叶斯学习时假设参数 $\theta$ 为随机向量，并且先验分布为 $q(\theta)$ ，贝叶斯方法的预测为
$$
\mathbb{E}_{q(\theta)}[y]=\int_qf(\pmb{x};\theta)q(\theta)d\theta\approx\frac{1}{M} \sum^M f(\pmb{x},\theta_i)
$$
其中 $f(\pmb{x},\theta_i)$ 为第 $i$ 次使用丢弃法后的网络，其参数 $\theta_i$ 为对全部参数 $\theta$ 的一次采样。&lt;/p>
&lt;/br>
&lt;h3 id="手动实现丢弃法">手动实现丢弃法&lt;/h3>
&lt;p>定义 &lt;code>dropout()&lt;/code> 函数以 &lt;code>drop_prob&lt;/code> 的概率丢弃。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">drop_prob&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">assert&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="n">drop_prob&lt;/span> &lt;span class="o">&amp;lt;=&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">keep_prob&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">drop_prob&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 这种情况下把全部元素都丢弃&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">keep_prob&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros_like&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">mask&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">keep_prob&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">mask&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">X&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">keep_prob&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义模型参数&lt;/strong>&lt;/p>
&lt;p>这里继续使用原书中的 Fashion-MNIST 数据集。然后定义一个包含两个隐藏层的多层感知机，且两个隐藏层的输出个数均为 256。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_outputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">784&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">256&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">256&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens1&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_hiddens1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_hiddens1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens2&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_hiddens2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_hiddens2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_outputs&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b3&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_outputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">params&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">W1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b3&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义模型&lt;/strong>&lt;/p>
&lt;p>模型定义部分使用 &lt;code>ReLU&lt;/code> 激活函数，然后对每个激活函数的输出使用丢弃法。这里我们将第一个隐藏层的丢弃概率设置为 0.1，第二个隐藏层的丢弃概率设为 0.6，然后通过参数 &lt;code>is_training&lt;/code> 来判断运行模式是训练还是测试，并在训练模式中使用丢弃法。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">drop_prob1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">drop_prob2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">is_training&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b1&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">relu&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">is_training&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># 只在训练模型时使用丢弃法&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">drop_prob1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 在第一层全连接后添加丢弃层&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">relu&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">is_training&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">drop_prob2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 在第二层全连接后添加丢弃层&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b3&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>模型评估&lt;/strong>&lt;/p>
&lt;p>在对模型进行评估的时候不应该使用丢弃法&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 本函数已保存在d2lzh_pytorch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">evaluate_accuracy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">acc_sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">data_iter&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">isinstance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">eval&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># 评估模式, 这会关闭dropout&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">acc_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># 改回训练模式&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="c1"># 自定义的模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;is_training&amp;#39;&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="vm">__code__&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">co_varnames&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 如果有is_training这个参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 将is_training设置成False&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">acc_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">is_training&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">acc_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">acc_sum&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">n&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>训练和测试&lt;/strong>&lt;/p>
&lt;p>模型训练 &lt;code>train_ch3()&lt;/code> 使用和多层感知机里一样的方法&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 本函数已保存在d2lzh包中方便以后使用&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">train_ch3&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">params&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_epochs&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_l_sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_acc_sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y_hat&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 梯度清零&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">optimizer&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_grad&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">params&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">param&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">backward&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">optimizer&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sgd&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">params&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># “softmax回归的简洁实现”一节将用到&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_l_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_acc_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">test_acc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">evaluate_accuracy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;epoch &lt;/span>&lt;span class="si">%d&lt;/span>&lt;span class="s1">, loss &lt;/span>&lt;span class="si">%.4f&lt;/span>&lt;span class="s1">, train acc &lt;/span>&lt;span class="si">%.3f&lt;/span>&lt;span class="s1">, test acc &lt;/span>&lt;span class="si">%.3f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">%&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">epoch&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_l_sum&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_acc_sum&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_acc&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_ch3&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cross_entropy&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">100.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">256&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CrossEntropyLoss&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load_data_fashion_mnist&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">train_ch3&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>输出&lt;/p>
&lt;pre tabindex="0">&lt;code>epoch 1, loss 0.0047, train acc 0.538, test acc 0.763
epoch 2, loss 0.0024, train acc 0.777, test acc 0.773
epoch 3, loss 0.0020, train acc 0.817, test acc 0.792
epoch 4, loss 0.0018, train acc 0.839, test acc 0.802
epoch 5, loss 0.0017, train acc 0.845, test acc 0.831
&lt;/code>&lt;/pre>&lt;/br>
&lt;p>最后，原书中还有两种方法的简洁实现，这里不再写了。&lt;/p>
&lt;/br>
&lt;h3 id="参考">参考&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>《动手学深度学习》PyTorch 版，阿斯顿·张、李沐 &lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/"target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>《神经网络与深度学习》邱锡鹏 &lt;a href="https://nndl.github.io/"target="_blank" rel="noopener noreferrer">https://nndl.github.io&lt;/a>
&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>[深度学习笔记] 深度学习第 3 篇——网络正则化</title><link>https://yibocat.github.io/posts/dl/sdxx_3/</link><pubDate>Mon, 14 Feb 2022 21:54:56 +0800</pubDate><guid>https://yibocat.github.io/posts/dl/sdxx_3/</guid><description>&lt;p>在学习正则化之前，首先看一下&lt;strong>经验误差&lt;/strong>和&lt;strong>泛化误差&lt;/strong>与&lt;strong>过拟合&lt;/strong>和&lt;strong>欠拟合&lt;/strong>，然后给出模型复杂度用来判定一个模型的好坏，然后学习网络正则化。&lt;/p>
&lt;/br>
&lt;h3 id="经验误差与泛化误差">经验误差与泛化误差&lt;/h3>
&lt;p>什么是经验误差和泛化误差？通俗的来讲，经验误差（Empirical Error）就是指模型在训练数据集上的表现出来的误差，泛化误差（Generalization Error）是指模型在任意新样本上的误差。&lt;/p>
&lt;/br>
&lt;p>具体来看，给定一个模型 $f(x;\theta)$ ，我们希望的是这个模型有一个小的期望误差，但是我们不知道真实的数据分布和映射函数，所以无法计算期望误差。但是我们可以计算的是经验误差，即模型在训练集上的误差，也就是训练集上的&lt;strong>平均损失&lt;/strong>
$$
E_{D}^{emp}(\theta)=\frac{1}{m}\sum_{i=1}^m\mathbb{L}(y^{(i)},f(x^{(i)};\theta))
$$
这里 $E_D^{emp}(\theta)$ 表示在训练集 $D$ 上的参数为 $\theta$ 的平均经验误差，假设有 $m$ 个训练数据，$\mathbb{L}$ 表示每个训练数据集的真实标签与模型 $f(x^{(i)};\theta)$ 得到的标签之间的误差函数。&lt;/p>
&lt;p>我们首先希望得到的模型一定是有较小的平均经验误差。通俗的讲，我们的模型最基本的要求首先是要在训练集上的误差最小吧，也就是暂且让模型满足训练集上的数据误差最小（扯了三句啰嗦话，其实是一个意思）。这也就是常说的&lt;strong>经验风险最小化准则（Empirical Risk Minimization , ERM）&lt;/strong>。&lt;/p>
&lt;/br>
&lt;p>接着来看泛化误差。&lt;/p>
&lt;p>我们假设任意样本集为 $\mathcal{D}$ ，则泛化误差的表示为
$$
E_{\mathcal{D}}(\theta) = P_{x\thicksim\mathcal{D}}(y\ne f(x;\theta))
$$
这个公式可以这么理解，$P_{x\thicksim\mathcal{D}}$ 表示 $x$ 在任意样本下的误差（或者说平均误差），而任意新样本的标签不等于该模型对该样本数据下的标签（公式可能不太正确，大概意思是一样的）。&lt;/p>
&lt;/br>
&lt;p>以上我们知道了什么是经验误差和泛化误差。我们希望一个模型可以满足 ERM。&lt;/p>
&lt;p>但是！但是！！！我们不能让经验风险无穷小（夸张）！过小的经验风险虽然会让模型在训练集上的表现非常好，但是会造成过拟合！这意味着我们的模型适合且仅适合训练数据集！太好了也不是件好事，所谓物极必反。&lt;/p>
&lt;p>这也就造成了过拟合现象。&lt;/p>
&lt;/br>
&lt;h3 id="过拟合与欠拟合">过拟合与欠拟合&lt;/h3>
&lt;p>上文讲过，过拟合的意思就是一个训练好的模型只在训练集上表现非常好，但是无法用在新的数据集，或者说测试集上。通常造成过拟合的原因是训练数据集过少，或者可以这么说，过拟合产生的原因是数据集与模型复杂度不匹配，即训练数据集的数量无法支撑高复杂度的模型。而神经网络通常是一个超高复杂度的模型，过少的数据集很容易造成过拟合现象。&lt;/p>
&lt;p>欠拟合则很好理解，指的是模型较弱的学习能力形成的导致泛化能力不高的现象。总的来说，过拟合和欠拟合都是由于模型复杂度与数据集不匹配造成的。&lt;/p>
&lt;p>所以我们看一下什么是模型复杂度这个概念。&lt;/p>
&lt;/br>
&lt;h3 id="模型复杂度">模型复杂度&lt;/h3>
&lt;p>首先我们以多项式拟合为例。给定一个由标量数据特征 $x$ 和对应的标量标签 $y$ 组成的训练数据集，多项式函数拟合的目标是找到一个 $K$ 阶多项式函数
$$
\hat{y}=b+\sum_{k=1}^Kx^k\omega_k
$$
来近似 $y$。上式中，$\omega_k$ 表示模型的权重参数，$b$ 表示偏差参数。而多项式函数拟合也是用平方损失函数。特别的，一阶多项式函数拟合又叫线性函数拟合。&lt;/p>
&lt;p>从上式可以看到，随着 $k$ 越来越大，模型的参数也就越来越多，模型函数的选择空间更大，所以高阶多项式函数比低阶多项式函数的复杂度更高，这也就意味着高阶多项式函数比低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。这也就是模型复杂度。&lt;/p>
&lt;p>下图可以很清晰的看出，模型复杂度与误差的关系以及什么是欠拟合与过拟合&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220214-07.png" style="width:50%;" />&lt;/p>
&lt;p>关于过拟合和欠拟合的实验，可以查看原书&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.11_underfit-overfit"target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.11_underfit-overfit&lt;/a>
&lt;/p>
&lt;h3 id="正则化">正则化&lt;/h3>
&lt;p>神经网络因其非常强的拟合能力往往使得经验误差可以降到非常低，从而造成过拟合现象。所以如何提高神经网络的泛化能力是非常重要的。&lt;/p>
&lt;p>从上文得知，防止过拟合可以从数据入手，增加数据集的数量，但是因神经网络强大的拟合能力可以知道其必然是一个模型复杂度非常高的模型，这意味着数据集的量呈指数增长从而无法满足神经网络超高的复杂度，所以增加数据集的量显然并不是一个好的方法。所以只能从另一个角度来考虑，即限制模型复杂度，也就是我们说的网络正则化。&lt;/p>
&lt;p>&lt;strong>正则化（Regularization）&lt;/strong> 是一种通过&lt;strong>限制模型复杂度&lt;/strong>来避免过拟合的方法，常用的正则化方法有权重衰减、丢弃法等。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>$\ell_1,\ell_2$ 正则化&lt;/strong>&lt;/p>
&lt;p>$\ell_1,\ell_2$ 正则化也称作范数正则化，是机器学习中最常用的正则化方法。其通过约束参数的 $\ell_1$ 和 $\ell_2$ 范数来减小模型在训练数据集上的过拟合现象。我们将 $\ell_1$ 和 $\ell_2$ 加入到优化问题中
$$
\mathcal{J}(\theta) = arg\min_{\theta}\frac{1}{N}\sum_{n=1}^N\mathcal{L}(y^{(n)},f(x^{(n)};\theta))+\lambda\ell_p(\theta)
$$
$\mathcal{L}()$ 表示损失函数，$N$ 为训练样本数量，$f()$ 表示待学习的神经网络，$\ell_p$ 表示范数函数，这里 $p$ 表示第 $p$ 范数，这里取值为 ${1,2}$ 表示 $\ell_1,\ell_2$ 范数，$\lambda$ 为正则化系数。&lt;/p>
&lt;/br>
&lt;p>怎么理解这个式子呢？首先我们看等式右边的前半部分，这其实就是上文中所提到的经验误差最小化式子。我们假设 $\lambda$ 无限小于 0 即等价于 0 ，此时该式子退化为经验误差最小化，也就是我们所说的只考虑增大训练集数据量来防止过拟合。而加入正则化项后，随着 $\lambda$ 的值越来越大，范数函数在等式中的作用越来越大，而该函数正是来限制模型复杂度的，所以该式子通过加入了正则项，来限制模型的复杂度。&lt;/p>
&lt;p>至于 $\ell_1$ 和 $\ell_2$ 是如何限制模型的，这涉及到数学问题了，这里就不再赘述了。&lt;/p>
&lt;/br>
&lt;p>算了&amp;hellip;做笔记 &amp;hellip; 还是详细一点 &amp;hellip;&lt;/p>
&lt;/br>
&lt;p>&lt;strong>正则化项如何影响模型&lt;/strong>&lt;/p>
&lt;p>我们将上式等式的右边第一部分简化一下，用 $E^{emp}(\theta)$ 表示，则优化问题可以写成
$$
\mathcal{J}(\theta)=E^{emp}(\theta)+\lambda\ell_p(\theta)
$$
首先看这个式子，我们的目的是为了让 $\mathcal{J}(\theta)$ 最小化，在不考虑正则化项的时候，$\mathcal{J}(\theta)$ 等价于经验误差最小化。而加入正则化项之后就要综合考虑两项了（这里看似废话，实则是循序渐进）。而为了使得 $\mathcal{J}(\theta)$ 最小化，正则化项也要越小越好。&lt;/p>
&lt;p>这里我们先看一下 1-范数和 2-范数的定义&lt;/p>
&lt;blockquote>
&lt;p>向量的 1-范数：$\vert\vert X\vert\vert_1 = \sum_{i=1}^n\vert x_i\vert$ ，表示向量内各元素的绝对值之和&lt;/p>
&lt;p>向量的 2-范数：$\vert\vert X\vert\vert_2=(\sum_{i=1}^nx_i^2)^{\frac{1}{2}}$ ，表示元素的平方和再开平方&lt;/p>
&lt;/blockquote>
&lt;/br>
&lt;p>这时候我们再看，1-范数为例，我们要让正则化项越小，意味着 $\theta$ 要让 $\sum_{i=1}^n\vert\theta_i\vert$ 尽可能小，这样 $\theta_i$ 就被限制住了。而 $\theta_i$ 是什么？不就是参数吗？我们不光要让参数尽可能小，而且还要让参数的数量尽可能得少！$\theta_i$ 被限制住，曲线也就不会太过陡峭。这种正则化的方式被称为&lt;/p>
&lt;p>&lt;strong>Lasso 回归（Least Absolute Shrinkage and Selection operator Regression)&lt;/strong> 。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>1-范数正则化&lt;/strong>&lt;/p>
&lt;p>L1 正则化项与稀疏性有关，即 L1 正则化可以使得参数稀疏化从而减小模型复杂度。什么是稀疏性？通常神经网络中的特征会达到一个非常大的数量，如果模型的参数有非常多的 0，那么就可以将很多的参数稀疏掉。而 L1 正则化项相当于对模型进行了一次特征选择，只留下重要的特征，从而提高模型的泛化能力，防止过拟合。&lt;/p>
&lt;/br>
&lt;p>我们假设一个二维参数空间，损失函数的等高线如下所示&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-01.jpeg" style="width:40%" />&lt;/p>
&lt;p>这时 L1 正则化为 $\vert \theta_1\vert+\vert\theta_2\vert$ ，对应的等高线是一个菱形。当不加如正则化项的时候，使用梯度下降法优化损失函数，他会沿着梯度方向下降而得到一个近似的最优解&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-02.jpeg" style="width:40%" />&lt;/p>
&lt;p>而加入L1正则化后，如下所示。$P,Q$ 两点在同一等高线上，即 $P$ 和 $Q$ 两个点的损失函数是等价的。但是，$OP$ 的距离大于 $OQ$ 的距离&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-03.jpeg" style="width:40%" />&lt;/p>
&lt;p>因此可以得到 $P$ 点的经验损失函数大于 $Q$ 点的经验损失函数，因为 $Q$ 点的 L1 范数小于 $P$ 点的 L1 范数，所以选择 $Q$ 点，其次，当我们选择 $Q$ 点时，对应的 $\theta_1=0$ ，这恰好将 $\theta_1$ 去掉了，并且对原模型并没有影响。&lt;/p>
&lt;p>所以可以得出结论，即加上正则化项，参数空间会被缩小，从而减小模型复杂度。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>2-范数正则化&lt;/strong>&lt;/p>
&lt;p>和 L1 正则化是一样的，我们假设一个二维的参数空间，L2 正则化为 $\theta_1^2+\theta_2^2$ ，这时 L2 等高线是一个同心圆&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-04.jpeg" style="width:40%" />&lt;/p>
&lt;p>同样我们使用上述损失函数与损失函数等高线，如图所示，L2 正则与损失等高线交于 $P,Q$ 两点，可以看到 $P$ 点距离 $Q$ 点更接近原点，即距离短。但是此时 $\theta_1,\theta_2$ 均不为 0 。所以我们说 2-范数正则化可以得到尽可能小的解，但是不具有稀疏性。而 L2 正则化方法也是岭回归方法。&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220215-05.jpeg" style="width:40%" />&lt;/p>
&lt;p>到这，我们了解了正则化项是如何影响到模型的复杂度的。&lt;/p>
&lt;/br>
&lt;h3 id="总结">总结&lt;/h3>
&lt;p>训练模型的目的是为了经验风险最小化，而过于追求经验风险最小化会造成过拟合。&lt;/p>
&lt;/br>
&lt;p>不论是过拟合还是欠拟合，其产生的原因是由于模型复杂度与数据集不匹配造成的，解决方法有两个：1.从数据集入手（不推荐）；2.从模型复杂度入手（正则化）&lt;/p>
&lt;/br>
&lt;p>正则化通过添加正则项来对模型复杂度加以限制，从而避免过拟合&lt;/p>
&lt;/br>
&lt;p>正则化如何影响模型复杂度：Lasso 回归（具有稀疏性）与岭回归（不具有稀疏性）。&lt;/p>
&lt;/br>
&lt;h3 id="参考">参考&lt;/h3>
&lt;ol>
&lt;li>
&lt;p>《机器学习》周志华&lt;/p>
&lt;/li>
&lt;li>
&lt;p>《动手学深度学习》PyTorch 版，阿斯顿·张、李沐 &lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/"target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>《神经网络与深度学习》邱锡鹏 &lt;a href="https://nndl.github.io/"target="_blank" rel="noopener noreferrer">https://nndl.github.io&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>概率近似正确？！（PAC Learning） - Haoran Jiang的文章 - 知乎 &lt;a href="https://zhuanlan.zhihu.com/p/44589648"target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/44589648&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[计算学习理论] PAC (Probably Approximately Correct)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>（理论+代码）理解模型正则化：L1正则、L2正则 - 饼干Japson的文章 - 知乎 &lt;a href="https://zhuanlan.zhihu.com/p/113373391"target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/113373391&lt;/a>
&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Lasso—原理及最优解 - 风磐的文章 - 知乎 &lt;a href="https://zhuanlan.zhihu.com/p/116869931"target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/116869931&lt;/a>
&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>[深度学习笔记] 深度学习第 2 篇——多层感知机</title><link>https://yibocat.github.io/posts/dl/sdxx_2/</link><pubDate>Mon, 14 Feb 2022 12:12:13 +0800</pubDate><guid>https://yibocat.github.io/posts/dl/sdxx_2/</guid><description>&lt;p>其实感知机也就是上一篇所讲的最简单的单层神经网络，即简单线性回归模型，也是属于一个简单二分类或多分类模型。&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220214-01.png" style="zoom:16%;" />&lt;/p>
&lt;h3 id="模型设计">模型设计&lt;/h3>
&lt;p>多层感知机就是在原本感知机的基础上增加了若干层隐藏层。由于输入层不参与到计算，所以下图展示的是一个二层的神经网络模型。&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220214-02.png" style="zoom:50%;" />&lt;/p>
&lt;p>可以看到，该多层感知机包含 5 个隐藏单元，并且隐藏层中的每个单元和输入层的各个输入是全连接的，输出层的神经元和隐藏层中的各个单元也是全连接的，所以多层感知机中的隐藏层和输出层都是全连接层。&lt;/p>
&lt;/br>
&lt;p>我们设一个小批量样本 $X\in \mathbb{R}^{n\times d}$ ，批量大小为 $n$ ，输入个数为 $d$ 。隐藏层层数为一层，隐层单元数为 $h$ ，隐藏层输出为 $H\in \mathbb{R}^{n\times h}$ （也可以记为隐藏层变量）。&lt;/p>
&lt;p>设隐藏层&lt;strong>权重参数&lt;/strong>和&lt;strong>偏差&lt;/strong>分别为 $W_h\in\mathbb{R}^{d\times h}$ 和 $b_h\in\mathbb{R}^{l \times h}$，输出层&lt;strong>权重&lt;/strong>和&lt;strong>偏差&lt;/strong>分别为 $W_o\in\mathbb{R}^{h\times q}$ 和 $b_o\in\mathbb{R}^{l\times q}$ 。由线性回归的公式可以得到多层感知机的设计模型
$$
H=XW_h+b_h \newline
O=HW_o+b_o
$$
将两个式子联立起来，可以得到
$$
O = (XW_h+b_h)W_o+b_o = XW_hW_o+b_hW_o+b_o
$$
可以看到，多层感知机的模型其实和单层神经网络是等价的，只不过输出层权重变成了 $W_hW_o$ ，偏差变成了 $b_hW_o+b_o$ 。&lt;/p>
&lt;h3 id="激活函数">激活函数&lt;/h3>
&lt;p>由上面的推导我们知道线性变换只是对数据进行了&lt;strong>仿射变换(affine transformation)&lt;/strong> ，我们不能直接根据其仿射变换得到想要的输出分类，所以需要引入非线性变换——&lt;strong>激活函数&lt;/strong>。&lt;/p>
&lt;p>神经网络中有很多不同的激活函数&lt;/p>
&lt;/br>
&lt;p>&lt;strong>ReLU函数&lt;/strong>&lt;/p>
&lt;p>ReLU 函数（Rectified Linear Unit，修正线性单元），也叫 Rectifier 函数，是目前深度神经网络中经常使用的激活函数，该函数定义为
$$
ReLU(x)=\max(0,x)
$$
其图像如下所示&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220214-03.png" style="zoom:36%;border:none;"/>&lt;/p>
&lt;p>可以看到，当输入为负数时导数为 0 ，当输入为正数时导数为 1。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>Sigmoid 函数&lt;/strong>&lt;/p>
&lt;p>Sigmoid 函数可以将元素的值变换到 0 和 1 之间，该函数如下定义：
$$
Sigmoid(x) = \frac{1}{1+\exp(-x)}
$$
其图像如下所示&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220214-04.png" style="zoom:36%;border:none;"/>&lt;/p>
&lt;/br>
&lt;p>&lt;strong>Tanh 函数&lt;/strong>&lt;/p>
&lt;p>Tanh 函数是一种 Sigmoid 型函数，也叫做双正切函数，其定义为
$$
tanh(x)=\frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)}
$$
Tanh 函数可以看成是放大并平移的 Sigmoid 函数，其形状和 Sigmoid 函数看起来很像，但是值域是 (-1,1)。&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220214-06.png" style="zoom:36%;border:none;"/>&lt;/p>
&lt;p>我们可以对比一下 Logistic 型 Sigmoid 函数和 Tanh 函数的图像&lt;/p>
&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/dl/dl-20220214-05.png" style="zoom:32%;"/>&lt;/p>
&lt;h3 id="多层感知机加入激活函数">多层感知机加入激活函数&lt;/h3>
&lt;p>通过上文已经知道，多层感知机就是含有至少一个隐藏层的全连接神经网络，每个隐藏层的输出由激活函数进行变换，我们可以将激活函数定义为 $\phi(x)$ ，那么多层感知机的模型如下所示
$$
H=\phi(XW_h+b_h) \newline
O=HW_o+b_o
$$
&lt;/br>&lt;/p>
&lt;h3 id="手动实现">手动实现&lt;/h3>
&lt;p>这部分可以参考原书&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.9_mlp-scratch"target="_blank" rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.9_mlp-scratch&lt;/a>
或者 Github &lt;a href="https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/code/chapter03_DL-basics/3.9_mlp-scratch.ipynb"target="_blank" rel="noopener noreferrer">https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/code/chapter03_DL-basics/3.9_mlp-scratch.ipynb&lt;/a>
。&lt;/p>
&lt;p>获取数据和读取数据直接使用原书中所给的 Fashion-MNIST 数据集，这是一个图像多分类问题，我们假设小批量数据集的数量为 &lt;code>batch_size = 256&lt;/code> ，并且通过 &lt;code>d2l.load_data_fashion_mnist()&lt;/code> 方法得到了训练数据集合测试数据集，该函数是原书中图像多分类问题的读取数据集的函数 。代码如下&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">256&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load_data_fashion_mnist&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义参数模型&lt;/strong>&lt;/p>
&lt;p>原数据集中的图像形状为 28$\times$28 ，一共有 10 个类别，所以可以用 $28\times 28=784$ 向量长度来表示一个图像。所以 输入个数为 784，输出个数为 10。因为我们的数据集的小批量数据数为 256，所以我们可以设置隐藏单元数为 256。注意：隐藏单元数是超参数。&lt;/p>
&lt;p>然后对参数进行初始化。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_outputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">784&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">256&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_hiddens&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num_hiddens&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_outputs&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_outputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">params&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">W1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b2&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">param&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">requires_grad_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义激活函数&lt;/strong>&lt;/p>
&lt;p>&lt;code>PyTorch&lt;/code> 已经有了 ReLU 函数，实现起来很简单。在手动实现时我们最好自己写。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">relu&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">input&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">other&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.0&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义模型&lt;/strong>&lt;/p>
&lt;p>定义模型的第一步需要将每张图像的形状改为长度为 &lt;code>num_inputs&lt;/code> 的向量，以方便将其导入到输入层，然后根据上文中的表达式建立模型&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_inputs&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">H&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">relu&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">H&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">W2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>.view()&lt;/code> 函数表示重塑形状，这个函数和 &lt;code>sklearn&lt;/code> 中的 &lt;code>reshape()&lt;/code> 的功能是一样的，而 &lt;code>view(-1,1)&lt;/code> 表示重塑成列向量。同理，&lt;code>view(1,-1)&lt;/code> 表示重塑成行向量。 而 &lt;code>torch.matmul()&lt;/code> 是矩阵相乘方法。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>定义损失函数&lt;/strong>&lt;/p>
&lt;p>损失函数直接使用 &lt;code>PyTorch&lt;/code> 中的交叉熵损失函数&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CrossEntropyLoss&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>训练模型&lt;/strong>&lt;/p>
&lt;p>我们设置两个超参数，迭代周期和学习率，分别为 5 和 100.0&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">100.0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 本函数已保存在d2lzh包中方便以后使用&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">train_ch3&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">params&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_epochs&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_l_sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_acc_sum&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y_hat&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 梯度清零&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">optimizer&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_grad&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">elif&lt;/span> &lt;span class="n">params&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">param&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">backward&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">optimizer&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sgd&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">params&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># “softmax回归的简洁实现”一节将用到&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_l_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_acc_sum&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n&lt;/span> &lt;span class="o">+=&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">test_acc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">evaluate_accuracy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;epoch &lt;/span>&lt;span class="si">%d&lt;/span>&lt;span class="s1">, loss &lt;/span>&lt;span class="si">%.4f&lt;/span>&lt;span class="s1">, train acc &lt;/span>&lt;span class="si">%.3f&lt;/span>&lt;span class="s1">, test acc &lt;/span>&lt;span class="si">%.3f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">%&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">epoch&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_l_sum&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_acc_sum&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">n&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_acc&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_ch3&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>最后得到输出如下&lt;/p>
&lt;pre tabindex="0">&lt;code>epoch 1, loss 0.0030, train acc 0.714, test acc 0.753
epoch 2, loss 0.0019, train acc 0.821, test acc 0.777
epoch 3, loss 0.0017, train acc 0.842, test acc 0.834
epoch 4, loss 0.0015, train acc 0.857, test acc 0.839
epoch 5, loss 0.0014, train acc 0.865, test acc 0.845
&lt;/code>&lt;/pre>&lt;/br>
&lt;p>全部手工实现多层感知机如上所示，但是这样手动实现其实很容易出现错误，且效率不高。我们可以使用 &lt;code>PyTorch&lt;/code> 更简洁地实现多层感知机。&lt;/p>
&lt;/br>
&lt;h3 id="简洁实现">简洁实现&lt;/h3>
&lt;p>&lt;strong>模型定义&lt;/strong>&lt;/p>
&lt;p>模型定义可以直接使用 &lt;code>PyTorch&lt;/code> 中的 &lt;code>nn.Sequential&lt;/code> 来实现。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_outputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">784&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">256&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">net&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">FlattenLayer&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_hiddens&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="c1"># 输入层&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReLU&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="c1"># 激活函数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_hiddens&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_outputs&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="c1"># 输出层&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">params&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">():&lt;/span> &lt;span class="c1"># 初始化参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">init&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">params&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mean&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>模型从上往下为输入层，激活函数，输出层。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>读取数据训练模型&lt;/strong>&lt;/p>
&lt;p>这里优化直接使用 &lt;code>Pytorch&lt;/code> 中的 &lt;code>SGD&lt;/code> 梯度下降算法&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">256&lt;/span> &lt;span class="c1"># 小批量大小\隐藏单元数量&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">d2l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load_data_fashion_mnist&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 读取数据&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">CrossEntropyLoss&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># 损失函数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.5&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 优化方法&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="c1"># 迭代周期&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_ch3&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">test_iter&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">optimizer&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="c1"># 训练模型&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>最后输出&lt;/p>
&lt;pre tabindex="0">&lt;code>epoch 1, loss 0.0030, train acc 0.712, test acc 0.744
epoch 2, loss 0.0019, train acc 0.823, test acc 0.821
epoch 3, loss 0.0017, train acc 0.844, test acc 0.842
epoch 4, loss 0.0015, train acc 0.856, test acc 0.842
epoch 5, loss 0.0014, train acc 0.864, test acc 0.818
&lt;/code>&lt;/pre>&lt;/br>
&lt;h3 id="参考">参考&lt;/h3>
&lt;p>&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/"target="_blank" rel="noopener noreferrer">《动手学深度学习》PyTorch 版&lt;/a>
&lt;/p>
&lt;p>&lt;a href="https://nndl.github.io"target="_blank" rel="noopener noreferrer">《神经网络与深度学习》邱锡鹏教授&lt;/a>
&lt;/p></description></item><item><title>[深度学习笔记] 深度学习第 1 篇——简单线性回归</title><link>https://yibocat.github.io/posts/dl/sdxx_1/</link><pubDate>Tue, 08 Feb 2022 22:06:10 +0800</pubDate><guid>https://yibocat.github.io/posts/dl/sdxx_1/</guid><description>&lt;p>线性模型（Linear Model）是机器学习中应用最广泛的模型，指通过样本特征的线性组合来进行预测的模型。线性回归是单层神经网络，其涉及的概念和技术适用于大多数深度学习模型。&lt;/p>
&lt;/br>
&lt;h3 id="线性回归">线性回归&lt;/h3>
&lt;p>&lt;strong>模型定义&lt;/strong>&lt;/p>
&lt;p>给定一个 $ D $ 维样本 $ x=[x_1,x_2,\cdots,x_D]^T $ 其线性组合函数为
$$
f(x;\omega)=\omega^Tx+b
$$
我们说 $ \omega $ 是一个权重矩阵（可以理解为斜率）， $ b $ 为偏差（可以理解为截距），其均为标量。这两个参数称为线性回归模型的参数，而我们的目的就是通过训练模型，得到最佳的参数估计。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>损失函数&lt;/strong>&lt;/p>
&lt;p>模型训练出的预测值通常需要和真实值进行比对，这种比对也就是&lt;strong>误差&lt;/strong>。在机器学习里把衡量误差的函数称为&lt;strong>损失函数（loss function）&lt;/strong>。这里我们使用平方误差函数，单个样本的平方损失函数可以如下表示
$$
l^{(i)}(\omega,b) = \frac{1}{2}(\hat{y}-y)^2
$$
这里 $ \hat{y} $ 表示训练的预测值，$ y $ 表示真实值，$ \frac{1}{2} $ 是为了在求导时方便化简。训练集中所有样本的误差的平均来衡量模型预测的质量，即
$$
l(\omega,b)=\frac{1}{n}\sum_{i=1}^nl^{(i)}(\omega,b)
$$
正如上文提到的，深度学习的任务就是找到一组模型参数，使得训练样本的损失最小。&lt;/p>
&lt;/br>
&lt;p>&lt;strong>算法优化&lt;/strong>&lt;/p>
&lt;p>求解数值解的优化算法中，&lt;strong>小批量随机梯度下降法（mini-batch stochastic gradient descent&lt;/strong> 在深度学习中被广泛使用。&lt;/p>
&lt;blockquote>
&lt;p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。&lt;/p>
&lt;/blockquote>
&lt;p>在线性回归模型中，模型的每个参数的迭代如下所示:
$$
\omega \gets \omega-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\frac{\partial l^{(i)}(\omega,b)}{\partial \omega} \newline
b\gets b-\frac{\eta}{|\mathcal{B}|}\sum_{i\in\mathcal{B}}\frac{\partial l^{(i)}(\omega,b)}{\partial b}
$$
这里 $ |\mathcal{B}| $ 表示每个小批量的样本个数，$\eta$ 为学习率。而这里的小批量样本数和学习率是人为设定的而不是学习得来的，所以被称为超参数（hyperparameter）。&lt;/p>
&lt;/br>
&lt;h3 id="线性回归实现">线性回归实现&lt;/h3>
&lt;/br>
&lt;p>线性回归的实现总体上分为以下几个步骤：&lt;/p>
&lt;ol>
&lt;li>数据集的准备&lt;/li>
&lt;li>初始化模型参数&lt;/li>
&lt;li>定义模型&lt;/li>
&lt;li>定义损失函数&lt;/li>
&lt;li>定义优化算法、&lt;/li>
&lt;li>训练模型&lt;/li>
&lt;/ol>
&lt;p>在此之前，我们可以定义一个函数，生成特征标签的散点图，这样可以更直接地观察两者间的线性关系。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">use_svg_display&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 用矢量图显示&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">display&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">set_matplotlib_formats&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;svg&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">set_figsize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">figsize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">3.5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.5&lt;/span>&lt;span class="p">)):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">use_svg_display&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 设置图的尺寸&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rcParams&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;figure.figsize&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">figsize&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#打印散点图&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">set_figsize&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plt&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">features&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">numpy&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">numpy&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>读取数据&lt;/strong>&lt;/p>
&lt;p>训练模型的时候需要不断读取小批量数据样本，所以可以定义一个函数来返回小批量的随机样本的特征和标签&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">data_iter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># batch_size 表示批量大小&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num_examples&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">features&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">indices&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_examples&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shuffle&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">indices&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 样本的读取顺序是随机的&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_examples&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">j&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LongTensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">indices&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_examples&lt;/span>&lt;span class="p">)])&lt;/span> &lt;span class="c1"># 最后一次可能不足一个batch&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">yield&lt;/span> &lt;span class="n">features&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index_select&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">index_select&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">j&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">data_iter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 打印&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">break&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>初始化模型参数&lt;/strong>&lt;/p>
&lt;p>权重初始化成均值为 0 ，标准差为 0.01 的正太随机数，偏差初始化为 0。然后因为参数需要求梯度来迭代，所以设置 &lt;code>requires_grad=True&lt;/code>。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">w&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.01&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">requires_grad_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">requires_grad_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">requires_grad&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义模型&lt;/strong>&lt;/p>
&lt;p>我们已经知道了线性回归的模型表达式，使用 &lt;code>torch.mm&lt;/code> 进行矩阵乘法运算&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">linreg&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义损失函数&lt;/strong>&lt;/p>
&lt;p>损失函数我们使用平方损失函数。注意：由于 $\hat{y}-y$ 中，$\hat{y}$ 和 $y$ 的形状是不一样的，所以需要使用 &lt;code>y.view&lt;/code> 将 $y$ 的形状变成预测值 $\hat{y}$ 的形状。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">squared_loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_hat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">size&lt;/span>&lt;span class="p">()))&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="mi">2&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义优化算法&lt;/strong>&lt;/p>
&lt;p>优化算法根据上文提到的使用小批量随机梯度下降算法，其通过不断迭代模型参数来优化损失函数。这里 &lt;code>lr&lt;/code> 为迭代步长。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">sgd&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">params&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">param&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">params&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span> &lt;span class="o">-=&lt;/span> &lt;span class="n">lr&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">param&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">batch_size&lt;/span> &lt;span class="c1"># 注意这里更改param时用的param.data&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>训练模型&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.03&lt;/span> &lt;span class="c1"># 迭代步长&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="c1"># 迭代周期&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">net&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">linreg&lt;/span> &lt;span class="c1"># 使用线性模型&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">squared_loss&lt;/span> &lt;span class="c1"># 损失函数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_epochs&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="c1"># 训练模型一共需要num_epochs个迭代周期&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 和y分别是小批量样本的特征和标签&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">data_iter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># l是有关小批量X和y的损失&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">backward&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># 小批量的损失对模型参数求梯度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sgd&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 使用小批量随机梯度下降迭代模型参数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 不要忘了梯度清零&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">w&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">b&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">grad&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">train_l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">w&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;epoch &lt;/span>&lt;span class="si">%d&lt;/span>&lt;span class="s1">, loss &lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">epoch&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这里我们设置迭代步长为 0.03，迭代周期为 3，每次迭代周期中都会通过 &lt;code>data_iter&lt;/code> 读取小批量数据样本，然后设置损失函数时，由于 &lt;code>l&lt;/code> 并不是标量，所以需要通过 &lt;code>sum()&lt;/code> 方法求和得到标量，再使用 &lt;code>l.backward()&lt;/code> 得到模型参数的梯度，然后使用小批量随机梯度下降法迭代模型参数。注意：每次迭代完需要对梯度清零。&lt;/p>
&lt;/br>
&lt;p>以上就是全人工实现了一个简单的线性回归模型。当然，&lt;code>PyTorch&lt;/code> 提供了简便的模型构造方法和多种损失函数。&lt;/p>
&lt;/br>
&lt;h3 id="线性回归简洁实现">线性回归简洁实现&lt;/h3>
&lt;/br>
&lt;p>&lt;strong>读取数据&lt;/strong>&lt;/p>
&lt;p>&lt;code>PyTorch&lt;/code> 提供了 &lt;code>data&lt;/code> 包读取数据。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.utils.data&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">Data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 将训练数据的特征和标签组合&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dataset&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TensorDataset&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">features&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 随机读取小批量&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">data_iter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Data&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">DataLoader&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dataset&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">batch_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">shuffle&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">data_iter&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 打印&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">break&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义模型&lt;/strong>&lt;/p>
&lt;p>&lt;code>PyTorch&lt;/code> 提供了大量预定义的层，这使得我们可以很简洁的实现模型的构造。&lt;/p>
&lt;p>导入 &lt;code>torch.nn&lt;/code> 模块，&lt;code>nn&lt;/code>的核心数据结构是 &lt;code>Module&lt;/code> 。&lt;code>Module&lt;/code> 是一个抽象概念，既可以表示一个层，又可以表示一个很多层的神经网络，其实它本身就是所有层的一个基类。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">LinearNet&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">n_feature&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">LinearNet&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linear&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_feature&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># forward 定义前向传播&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">y&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">net&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LinearNet&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 使用print可以打印出网络的结构&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>forward()&lt;/code> 定义了模型的前向传播计算方式。&lt;/p>
&lt;p>其实 &lt;code>PyTorch&lt;/code> 还有更加简便的网络搭建方法，如 &lt;code>nn.Sequential&lt;/code>, 这里不再赘述。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">net&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_inputs&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 此处还可以传入其他层&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>初始化模型参数&lt;/strong>&lt;/p>
&lt;p>&lt;code>PyTorch&lt;/code> 在 &lt;code>init&lt;/code> 模块中提供了很多参数初始化的方法，这里可以通过 &lt;code>init.normal_&lt;/code> 将权重参数每个元素初始化为随机采样于均值为0、标准差为0.01的正态分布，偏差会初始化为零。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">torch.nn&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">init&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">normal_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">weight&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">mean&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">std&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.01&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant_&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">bias&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">val&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 也可以直接修改bias的data: net[0].bias.data.fill_(0)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义损失函数&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">MSELoss&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>定义优化算法&lt;/strong>&lt;/p>
&lt;p>&lt;code>torch.optim&lt;/code>模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等，所以不用再自己实现小批量梯度下降算法了。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">torch.optim&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">optim&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optim&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">SGD&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">net&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parameters&lt;/span>&lt;span class="p">(),&lt;/span> &lt;span class="n">lr&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mf">0.03&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">optimizer&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>输出：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">SGD&lt;/span> &lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Parameter&lt;/span> &lt;span class="n">Group&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dampening&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">lr&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mf">0.03&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">momentum&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nesterov&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="kc">False&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">weight_decay&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>&lt;strong>训练模型&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">epoch&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">num_epochs&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">data_iter&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">net&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zero_grad&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="c1"># 梯度清零，等价于net.zero_grad()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">backward&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">step&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;epoch &lt;/span>&lt;span class="si">%d&lt;/span>&lt;span class="s1">, loss: &lt;/span>&lt;span class="si">%f&lt;/span>&lt;span class="s1">&amp;#39;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">epoch&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">l&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/br>
&lt;p>以上就是关于简单线性模型的实现，包括所有代码的手工实现以及使用 &lt;code>PyTorch&lt;/code> 提供的模块的实现。内容基本上都是根据《动手学深度学习》（PyTorch版）来编写的，代码也是。需要注意的是，应该尽可能采用矢量计算，以提升计算效率。&lt;code>torch.utils.data&lt;/code>模块提供了有关数据处理的工具，&lt;code>torch.nn&lt;/code>模块定义了大量神经网络的层，&lt;code>torch.nn.init&lt;/code>模块定义了各种初始化方法，&lt;code>torch.optim&lt;/code>模块提供了很多常用的优化算法。&lt;/p>
&lt;/br>
&lt;h3 id="参考">参考&lt;/h3>
&lt;p>&lt;a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.1_linear-regression"target="_blank" rel="noopener noreferrer">《动手学深度学习》(PyTorch)&lt;/a>
&lt;/p></description></item><item><title>[Python 笔记本] 远程搭建 jupyter 服务器</title><link>https://yibocat.github.io/posts/fwq/fwq_jupyter/</link><pubDate>Wed, 02 Feb 2022 21:47:35 +0800</pubDate><guid>https://yibocat.github.io/posts/fwq/fwq_jupyter/</guid><description>&lt;p>本文通过搭建一个简单的远程 &lt;code>Jupyter&lt;/code> 服务端，可以实现不分场合与时间运行代码，或者说是展示代码，这提供了极大的方便。&lt;/p>
&lt;h3 id="搭建-jupyter-服务器">搭建 &lt;code>Jupyter&lt;/code> 服务器&lt;/h3>
&lt;p>&lt;code>ssh&lt;/code> 远程登录服务器&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">ssh root@xxx.xxx.xxx.xxx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>下载并安装 &lt;code>miniconda&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">bash Miniconda3-latest-Linux-x86_64.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>然后设置虚拟 &lt;code>conda&lt;/code> 环境，这里我们的环境使用 &lt;code>python 3.9&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">conda create -n myenvname &lt;span class="nv">python&lt;/span>&lt;span class="o">=&lt;/span>3.9
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>切换环境与停用环境&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">conda activate myenvname
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">conda deactivate
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="安装-jupyter-和配置">安装 &lt;code>Jupyter&lt;/code> 和配置&lt;/h3>
&lt;p>安装 &lt;code>jupyter&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">conda install -c conda-forge jupyterlab
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>生成配置文件&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">jupyter notebook --generate-config
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>进入 &lt;code>IPython&lt;/code> ，设置记住哈希密码（这里会将登录密码转换为哈希密码）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">Ipython
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">In&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]:&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">notebook.auth&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">passwd&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">In&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]:&lt;/span> &lt;span class="n">passwd&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Enter&lt;/span> &lt;span class="n">password&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Verify&lt;/span> &lt;span class="n">password&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Out&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">]:&lt;/span> &lt;span class="s1">&amp;#39;argon2:$argon2id$v=19$m=10240,t=10,p=8$1Hk7hXkmqH0KUC3JswHy8A$W2Ya&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">In&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">]:&lt;/span> &lt;span class="n">exit&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="修改配置文件">修改配置文件&lt;/h3>
&lt;p>通过 FTP 登录到服务器，找到配置文件 &lt;code>jupyter_notebook_config.py&lt;/code> ，添加以下内容到文件末尾&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">NotebookApp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">password&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="sa">u&lt;/span>&lt;span class="s1">&amp;#39;argon2:$argon2id$v=19$m=10240,t=10,p=8$1Hk7hXkmqH0KUC3JswHy8A$W2Ya&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#就是刚才需要记下的哈希密码&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">NotebookApp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">port&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">9999&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#指定jupyter lab 运行端口，写一个不冲突的端口即可 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">NotebookApp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">allow_remote_access&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">True&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 允许远程访问 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">NotebookApp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ip&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;*&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 就是设置所有ip皆可访问 &lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">NotebookApp&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">open_browser&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">False&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 禁止自动打开浏览器 &lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>注意：这里需要将端口放行，否则无法连接登录&lt;/p>
&lt;/blockquote>
&lt;h3 id="服务器开启-jupyterlab">服务器开启 &lt;code>Jupyterlab&lt;/code>&lt;/h3>
&lt;p>输入代码打开 &lt;code>Jupyterlab&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">jupyter lab --allow-root
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>此时已经可以在浏览器上查看远程 &lt;code>Jupyterlab&lt;/code> 了，但是当关闭终端时 &lt;code>Jupyter&lt;/code> 也会相应关闭，所以我们需要让 &lt;code>Jupyter&lt;/code> 保持在后台运行&lt;/p>
&lt;p>后台运行，并将标准输出写入到 &lt;code>jupyter.log&lt;/code> 中&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">nohup jupyter notebook --allow-root &amp;gt; jupyter.log 2&amp;gt;&lt;span class="p">&amp;amp;&lt;/span>&lt;span class="m">1&lt;/span> &lt;span class="p">&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nohup jupyter lab --allow-root &amp;gt; jupyter.log 2&amp;gt;&lt;span class="p">&amp;amp;&lt;/span>&lt;span class="m">1&lt;/span> &lt;span class="p">&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>nobup&lt;/code> 表示 &lt;code>no hang up&lt;/code>，就是不挂起，退出终端后依然可以运行。&lt;/p>
&lt;p>然后找到 &lt;code>jupyter&lt;/code> 进程，终止该进程&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">ps -a
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">kill&lt;/span> -9 pid
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>浏览器中输入 &lt;code>xxx.xxx.xxx.xxx:9999&lt;/code> 进入 &lt;code>jupyter notebook&lt;/code>&lt;/p>
&lt;p>浏览器中输入 &lt;code>xxx.xxx.xxx.xxx:9999/lab&lt;/code> 进入 &lt;code>jupyterlab&lt;/code>&lt;/p>
&lt;h3 id="日常登陆-jupyter-及服务器操作">日常登陆 &lt;code>Jupyter&lt;/code> 及服务器操作&lt;/h3>
&lt;p>远程登录服务器&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">ssh root@xxx.xxx.xxx.xxx
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>切换环境&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">conda activate myenvname
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>运行 &lt;code>jupyterlab&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">jupyter lab --allow-root
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>非挂起运行 &lt;code>JupyterLab&lt;/code> （可关闭终端）&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">nohup jupyter notebook --allow-root &amp;gt; jupyter.log 2&amp;gt;&lt;span class="p">&amp;amp;&lt;/span>&lt;span class="m">1&lt;/span> &lt;span class="p">&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">nohup jupyter lab --allow-root &amp;gt; jupyter.log 2&amp;gt;&lt;span class="p">&amp;amp;&lt;/span>&lt;span class="m">1&lt;/span> &lt;span class="p">&amp;amp;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>查看运行进程，并杀死进程&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">ps -a
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">kill&lt;/span> &lt;span class="m">1000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>浏览器中输入 &lt;code>xxx.xxx.xxx.xxx:9999&lt;/code> 进入 &lt;code>jupyter notebook&lt;/code>&lt;/p>
&lt;p>浏览器中输入 &lt;code>xxx.xxx.xxx.xxx:9999/lab&lt;/code> 进入 &lt;code>jupyterlab&lt;/code>&lt;/p></description></item><item><title>[Git 日记] Git 忽略 .DS_Store 与全局忽略</title><link>https://yibocat.github.io/posts/git/git_ignore/</link><pubDate>Tue, 01 Feb 2022 10:53:07 +0800</pubDate><guid>https://yibocat.github.io/posts/git/git_ignore/</guid><description>&lt;p>Mac OS 的每个文件夹下都有一个隐藏文件 .DS_Store，该文件保存的是当前文件夹的属性，如图标位置、背景等。&lt;/p>
&lt;p>但是每次使用 Git 提交更改时，都会自动生成 .DS_Store 更改，所以每次提交到版本库和 &lt;code>push&lt;/code> 到 Github 总是很麻烦。所以 .DS_Store 是没有必要提交到版本库的，这时可以使用 git.gitignore 来忽略此类文件。&lt;/p>
&lt;h3 id="忽略当前目录下的-ds_store">忽略当前目录下的 .DS_Store&lt;/h3>
&lt;p>我们在所要忽略 .DS_Store 的目录下创建一个 &lt;code>.gitignore&lt;/code> 文件，然后将要忽略的文件名写入进去&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">touch .gitignore
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p style="text-align:center">&lt;img src="https://yibocat.github.io/images/git/git-20220201-1.png" style="width:90%" />&lt;/p>
&lt;p>每次忽略任何文件只需将要忽略的文件添加到 &lt;code>.gitignore&lt;/code> 文件就可以了。&lt;/p>
&lt;p>但是每次更改完 &lt;code>.gitignore&lt;/code> 文件之后，都需要运行以下代码，否则 &lt;code>.gitignore&lt;/code> 是不生效的&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">git rm -r --cached .
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">git add .
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">git commit -m &lt;span class="s1">&amp;#39;update .gitignore&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="git-全局忽略">git 全局忽略&lt;/h3>
&lt;p>首先在终端输入如下代码查看 git 现有的全局配置&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">git config --list
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>git 全局配置都在一个 &lt;code>.gitconfig&lt;/code> 文件中，所以可以使用 git 全局配置进行全局忽略 .DS_Store 。&lt;/p>
&lt;p>具体步骤是在根目录创建一个 &lt;code>.gitignore_global&lt;/code> 文件，把要忽略的文件直接添加到该文件中，和上文中的当前目录添加是一样的&lt;/p>
&lt;p>然后在终端输入&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">git config --global core.excludesfile/Users/reon/.gitignore_global
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>或者直接在 &lt;code>.gitconfig&lt;/code> 中添加如下内容&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">&lt;span class="line">&lt;span class="cl">&lt;span class="o">[&lt;/span>core&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nv">excludefile&lt;/span> &lt;span class="o">=&lt;/span> /Users/xiaqunfeng/.gitignore_global
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="参考">参考&lt;/h3>
&lt;ol>
&lt;li>&lt;a href="http://xiaqunfeng.cc/2018/04/24/git-ignore-ds-store/#%E5%88%A0%E9%99%A4github%E4%B8%8A%E6%96%87%E4%BB%B6"target="_blank" rel="noopener noreferrer">http://xiaqunfeng.cc/2018/04/24/git-ignore-ds-store/#%E5%88%A0%E9%99%A4github%E4%B8%8A%E6%96%87%E4%BB%B6&lt;/a>
&lt;/li>
&lt;li>&lt;a href="https://blog.csdn.net/allanGold/article/details/73132606"target="_blank" rel="noopener noreferrer">https://blog.csdn.net/allanGold/article/details/73132606&lt;/a>
&lt;/li>
&lt;/ol></description></item></channel></rss>