<!doctype html><html lang=zh><meta charset=utf-8><meta name=generator content="Hugo 0.94.2"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=color-scheme content="light dark"><meta name=supported-color-schemes content="light dark"><link rel=bookmark type=image/x-icon href=images/yibo_1.jpeg><link rel=icon type=image/x-icon href=images/yibo_1.jpeg><link rel="shortcut icon" type=image/x-icon href=images/yibo_1.jpeg><script>var _hmt=_hmt||[];(function(){var e=document.createElement("script"),t;e.src="https://hm.baidu.com/hm.js?e5030d0d5c8f146cc58d0a4fe660754f",t=document.getElementsByTagName("script")[0],t.parentNode.insertBefore(e,t)})()</script><title>[深度学习笔记] 深度学习第 2 篇——多层感知机&nbsp;&ndash;&nbsp;Yibocat</title><link rel=stylesheet href=/css/core.min.0a0307d00613a4a7364263921bc1fbafe7f3eff5f15937907925d4c6a18e9130a5c9d54ad7b4631345308f0335629fed.css integrity=sha384-CgMH0AYTpKc2QmOSG8H7r+fz7/XxWTeQeSXUxqGOkTClydVK17RjE0UwjwM1Yp/t><meta name=twitter:card content="summary"><meta name=twitter:title content="[深度学习笔记] 深度学习第 2 篇——多层感知机"><body><section id=header><div class="header wrap"><span class="header left-side"><a class="site home" href=/><span class="site name">Yibocat</span></a></span>
<span class="header right-side"><div class="nav wrap"><nav class=nav><a class="nav item" href=/posts/>文章 </a><a class="nav item" href=/archives/>归档 </a><a class="nav item" href=/categories/>分类 </a><a class="nav item" href=/tags/>标签 </a><a class="nav item" href=/search/>搜索 </a><a class="nav item" href></a></nav></div></span></div><div class="site slogan"><span id=type></span></div><script src=https://cdn.jsdelivr.net/npm/typed.js@2.0.12></script>
<script>var typed=new Typed("#type",{strings:["Minecraft","Basketball","Code and code","Grand Theft Auto V","Photography","Back to campus",'',"<strong>Python<strong>","<strong>C++<strong>","<strong><i>Swift<i><strong>","Java","<strong>Machine Learning<strong>","don't like <strong>algorithm<strong>",'',"一个不会编程的研究僧"],typeSpeed:60,backSpeed:60,backDelay:1250,smartBackspace:!0,loop:!0})</script></section><section id=content><div class=article-container><section class="article header"><h1 class="article title">[深度学习笔记] 深度学习第 2 篇——多层感知机</h1><p class="article date">2022-02-14
<span class=wordcount>&nbsp;•&nbsp;共&nbsp;2006&nbsp;个字</span><span class=reading-time> • 预计阅读时间 5 分钟</span></p></section><aside id=toc-container class="toc-container wide"><div class=toc><h3>文章目录</h3><div class=inner><ul><li><a href=#%e6%a8%a1%e5%9e%8b%e8%ae%be%e8%ae%a1 aria-label=模型设计>模型设计</a></li><li><a href=#%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 aria-label=激活函数>激活函数</a></li><li><a href=#%e5%a4%9a%e5%b1%82%e6%84%9f%e7%9f%a5%e6%9c%ba%e5%8a%a0%e5%85%a5%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0 aria-label=多层感知机加入激活函数>多层感知机加入激活函数</a></li><li><a href=#%e6%89%8b%e5%8a%a8%e5%ae%9e%e7%8e%b0 aria-label=手动实现>手动实现</a></li><li><a href=#%e7%ae%80%e6%b4%81%e5%ae%9e%e7%8e%b0 aria-label=简洁实现>简洁实现</a></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></div></aside><article class="article markdown-body"><div><p>其实感知机也就是上一篇所讲的最简单的单层神经网络，即简单线性回归模型，也是属于一个简单二分类或多分类模型。</p><p style=text-align:center><img src=/images/dl/dl-20220214-01.png style=zoom:16%></p><h3 id=模型设计>模型设计</h3><p>多层感知机就是在原本感知机的基础上增加了若干层隐藏层。由于输入层不参与到计算，所以下图展示的是一个二层的神经网络模型。</p><p style=text-align:center><img src=/images/dl/dl-20220214-02.png style=zoom:50%></p><p>可以看到，该多层感知机包含 5 个隐藏单元，并且隐藏层中的每个单元和输入层的各个输入是全连接的，输出层的神经元和隐藏层中的各个单元也是全连接的，所以多层感知机中的隐藏层和输出层都是全连接层。</p></br><p>我们设一个小批量样本 $X\in \mathbb{R}^{n\times d}$ ，批量大小为 $n$ ，输入个数为 $d$ 。隐藏层层数为一层，隐层单元数为 $h$ ，隐藏层输出为 $H\in \mathbb{R}^{n\times h}$ （也可以记为隐藏层变量）。</p><p>设隐藏层<strong>权重参数</strong>和<strong>偏差</strong>分别为 $W_h\in\mathbb{R}^{d\times h}$ 和 $b_h\in\mathbb{R}^{l \times h}$，输出层<strong>权重</strong>和<strong>偏差</strong>分别为 $W_o\in\mathbb{R}^{h\times q}$ 和 $b_o\in\mathbb{R}^{l\times q}$ 。由线性回归的公式可以得到多层感知机的设计模型
$$
H=XW_h+b_h \newline
O=HW_o+b_o
$$
将两个式子联立起来，可以得到
$$
O = (XW_h+b_h)W_o+b_o = XW_hW_o+b_hW_o+b_o
$$
可以看到，多层感知机的模型其实和单层神经网络是等价的，只不过输出层权重变成了 $W_hW_o$ ，偏差变成了 $b_hW_o+b_o$ 。</p><h3 id=激活函数>激活函数</h3><p>由上面的推导我们知道线性变换只是对数据进行了<strong>仿射变换(affine transformation)</strong> ，我们不能直接根据其仿射变换得到想要的输出分类，所以需要引入非线性变换——<strong>激活函数</strong>。</p><p>神经网络中有很多不同的激活函数</p></br><p><strong>ReLU函数</strong></p><p>ReLU 函数（Rectified Linear Unit，修正线性单元），也叫 Rectifier 函数，是目前深度神经网络中经常使用的激活函数，该函数定义为
$$
ReLU(x)=\max(0,x)
$$
其图像如下所示</p><p style=text-align:center><img src=/images/dl/dl-20220214-03.png style=zoom:36%;border:none></p><p>可以看到，当输入为负数时导数为 0 ，当输入为正数时导数为 1。</p></br><p><strong>Sigmoid 函数</strong></p><p>Sigmoid 函数可以将元素的值变换到 0 和 1 之间，该函数如下定义：
$$
Sigmoid(x) = \frac{1}{1+\exp(-x)}
$$
其图像如下所示</p><p style=text-align:center><img src=/images/dl/dl-20220214-04.png style=zoom:36%;border:none></p></br><p><strong>Tanh 函数</strong></p><p>Tanh 函数是一种 Sigmoid 型函数，也叫做双正切函数，其定义为
$$
tanh(x)=\frac{\exp(x)-\exp(-x)}{\exp(x)+\exp(-x)}
$$
Tanh 函数可以看成是放大并平移的 Sigmoid 函数，其形状和 Sigmoid 函数看起来很像，但是值域是 (-1,1)。</p><p style=text-align:center><img src=/images/dl/dl-20220214-06.png style=zoom:36%;border:none></p><p>我们可以对比一下 Logistic 型 Sigmoid 函数和 Tanh 函数的图像</p><p style=text-align:center><img src=/images/dl/dl-20220214-05.png style=zoom:32%></p><h3 id=多层感知机加入激活函数>多层感知机加入激活函数</h3><p>通过上文已经知道，多层感知机就是含有至少一个隐藏层的全连接神经网络，每个隐藏层的输出由激活函数进行变换，我们可以将激活函数定义为 $\phi(x)$ ，那么多层感知机的模型如下所示
$$
H=\phi(XW_h+b_h) \newline
O=HW_o+b_o
$$</br></p><h3 id=手动实现>手动实现</h3><p>这部分可以参考原书<a href=https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.9_mlp-scratch target=_blank rel="noopener noreferrer">https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.9_mlp-scratch</a>
或者 Github <a href=https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/code/chapter03_DL-basics/3.9_mlp-scratch.ipynb target=_blank rel="noopener noreferrer">https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/code/chapter03_DL-basics/3.9_mlp-scratch.ipynb</a>
。</p><p>获取数据和读取数据直接使用原书中所给的 Fashion-MNIST 数据集，这是一个图像多分类问题，我们假设小批量数据集的数量为 <code>batch_size = 256</code> ，并且通过 <code>d2l.load_data_fashion_mnist()</code> 方法得到了训练数据集合测试数据集，该函数是原书中图像多分类问题的读取数据集的函数 。代码如下</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl><span class=n>train_iter</span><span class=p>,</span> <span class=n>test_iter</span> <span class=o>=</span> <span class=n>d2l</span><span class=o>.</span><span class=n>load_data_fashion_mnist</span><span class=p>(</span><span class=n>batch_size</span><span class=p>)</span>
</span></span></code></pre></div></br><p><strong>定义参数模型</strong></p><p>原数据集中的图像形状为 28$\times$28 ，一共有 10 个类别，所以可以用 $28\times 28=784$ 向量长度来表示一个图像。所以 输入个数为 784，输出个数为 10。因为我们的数据集的小批量数据数为 256，所以我们可以设置隐藏单元数为 256。注意：隐藏单元数是超参数。</p><p>然后对参数进行初始化。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_inputs</span><span class=p>,</span> <span class=n>num_outputs</span><span class=p>,</span> <span class=n>num_hiddens</span> <span class=o>=</span> <span class=mi>784</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>,</span> <span class=p>(</span><span class=n>num_inputs</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>)),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>num_hiddens</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>,</span> <span class=p>(</span><span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_outputs</span><span class=p>)),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>num_outputs</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>params</span> <span class=o>=</span> <span class=p>[</span><span class=n>W1</span><span class=p>,</span> <span class=n>b1</span><span class=p>,</span> <span class=n>W2</span><span class=p>,</span> <span class=n>b2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>params</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>param</span><span class=o>.</span><span class=n>requires_grad_</span><span class=p>(</span><span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div></br><p><strong>定义激活函数</strong></p><p><code>PyTorch</code> 已经有了 ReLU 函数，实现起来很简单。在手动实现时我们最好自己写。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>relu</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=nb>input</span><span class=o>=</span><span class=n>X</span><span class=p>,</span> <span class=n>other</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=mf>0.0</span><span class=p>))</span>
</span></span></code></pre></div></br><p><strong>定义模型</strong></p><p>定义模型的第一步需要将每张图像的形状改为长度为 <code>num_inputs</code> 的向量，以方便将其导入到输入层，然后根据上文中的表达式建立模型</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>net</span><span class=p>(</span><span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>view</span><span class=p>((</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_inputs</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>H</span> <span class=o>=</span> <span class=n>relu</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>W1</span><span class=p>)</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>H</span><span class=p>,</span> <span class=n>W2</span><span class=p>)</span> <span class=o>+</span> <span class=n>b2</span>
</span></span></code></pre></div><p><code>.view()</code> 函数表示重塑形状，这个函数和 <code>sklearn</code> 中的 <code>reshape()</code> 的功能是一样的，而 <code>view(-1,1)</code> 表示重塑成列向量。同理，<code>view(1,-1)</code> 表示重塑成行向量。 而 <code>torch.matmul()</code> 是矩阵相乘方法。</p></br><p><strong>定义损失函数</strong></p><p>损失函数直接使用 <code>PyTorch</code> 中的交叉熵损失函数</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>
</span></span></code></pre></div></br><p><strong>训练模型</strong></p><p>我们设置两个超参数，迭代周期和学习率，分别为 5 和 100.0</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_epochs</span><span class=p>,</span> <span class=n>lr</span> <span class=o>=</span> <span class=mi>5</span><span class=p>,</span> <span class=mf>100.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 本函数已保存在d2lzh包中方便以后使用</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_ch3</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>train_iter</span><span class=p>,</span> <span class=n>test_iter</span><span class=p>,</span> <span class=n>loss</span><span class=p>,</span> <span class=n>num_epochs</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>              <span class=n>params</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>train_l_sum</span><span class=p>,</span> <span class=n>train_acc_sum</span><span class=p>,</span> <span class=n>n</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mf>0.0</span><span class=p>,</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=ow>in</span> <span class=n>train_iter</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>y_hat</span> <span class=o>=</span> <span class=n>net</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>l</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>y_hat</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 梯度清零</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>optimizer</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>params</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>params</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>grad</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=n>param</span> <span class=ow>in</span> <span class=n>params</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>param</span><span class=o>.</span><span class=n>grad</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>zero_</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>l</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>optimizer</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>d2l</span><span class=o>.</span><span class=n>sgd</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>lr</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>  <span class=c1># “softmax回归的简洁实现”一节将用到</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>train_l_sum</span> <span class=o>+=</span> <span class=n>l</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>train_acc_sum</span> <span class=o>+=</span> <span class=p>(</span><span class=n>y_hat</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> <span class=o>==</span> <span class=n>y</span><span class=p>)</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>n</span> <span class=o>+=</span> <span class=n>y</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>test_acc</span> <span class=o>=</span> <span class=n>evaluate_accuracy</span><span class=p>(</span><span class=n>test_iter</span><span class=p>,</span> <span class=n>net</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;epoch </span><span class=si>%d</span><span class=s1>, loss </span><span class=si>%.4f</span><span class=s1>, train acc </span><span class=si>%.3f</span><span class=s1>, test acc </span><span class=si>%.3f</span><span class=s1>&#39;</span>
</span></span><span class=line><span class=cl>              <span class=o>%</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>train_l_sum</span> <span class=o>/</span> <span class=n>n</span><span class=p>,</span> <span class=n>train_acc_sum</span> <span class=o>/</span> <span class=n>n</span><span class=p>,</span> <span class=n>test_acc</span><span class=p>))</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>train_ch3</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>train_iter</span><span class=p>,</span> <span class=n>test_iter</span><span class=p>,</span> <span class=n>loss</span><span class=p>,</span> <span class=n>num_epochs</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>params</span><span class=p>,</span> <span class=n>lr</span><span class=p>)</span>
</span></span></code></pre></div><p>最后得到输出如下</p><pre tabindex=0><code>epoch 1, loss 0.0030, train acc 0.714, test acc 0.753
epoch 2, loss 0.0019, train acc 0.821, test acc 0.777
epoch 3, loss 0.0017, train acc 0.842, test acc 0.834
epoch 4, loss 0.0015, train acc 0.857, test acc 0.839
epoch 5, loss 0.0014, train acc 0.865, test acc 0.845
</code></pre></br><p>全部手工实现多层感知机如上所示，但是这样手动实现其实很容易出现错误，且效率不高。我们可以使用 <code>PyTorch</code> 更简洁地实现多层感知机。</p></br><h3 id=简洁实现>简洁实现</h3><p><strong>模型定义</strong></p><p>模型定义可以直接使用 <code>PyTorch</code> 中的 <code>nn.Sequential</code> 来实现。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>num_inputs</span><span class=p>,</span> <span class=n>num_outputs</span><span class=p>,</span> <span class=n>num_hiddens</span> <span class=o>=</span> <span class=mi>784</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>256</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>net</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>d2l</span><span class=o>.</span><span class=n>FlattenLayer</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_inputs</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>),</span>		<span class=c1># 输入层</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>								<span class=c1># 激活函数</span>
</span></span><span class=line><span class=cl>        <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_outputs</span><span class=p>),</span> 	<span class=c1># 输出层</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>params</span> <span class=ow>in</span> <span class=n>net</span><span class=o>.</span><span class=n>parameters</span><span class=p>():</span>					<span class=c1># 初始化参数</span>
</span></span><span class=line><span class=cl>    <span class=n>init</span><span class=o>.</span><span class=n>normal_</span><span class=p>(</span><span class=n>params</span><span class=p>,</span> <span class=n>mean</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>std</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span>
</span></span></code></pre></div><p>模型从上往下为输入层，激活函数，输出层。</p></br><p><strong>读取数据训练模型</strong></p><p>这里优化直接使用 <code>Pytorch</code> 中的 <code>SGD</code> 梯度下降算法</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>256</span>													<span class=c1># 小批量大小\隐藏单元数量</span>
</span></span><span class=line><span class=cl><span class=n>train_iter</span><span class=p>,</span> <span class=n>test_iter</span> <span class=o>=</span> <span class=n>d2l</span><span class=o>.</span><span class=n>load_data_fashion_mnist</span><span class=p>(</span><span class=n>batch_size</span><span class=p>)</span>		<span class=c1># 读取数据</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>()</span>									<span class=c1># 损失函数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>SGD</span><span class=p>(</span><span class=n>net</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>				<span class=c1># 优化方法</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>num_epochs</span> <span class=o>=</span> <span class=mi>5</span>														<span class=c1># 迭代周期</span>
</span></span><span class=line><span class=cl><span class=n>train_ch3</span><span class=p>(</span><span class=n>net</span><span class=p>,</span> <span class=n>train_iter</span><span class=p>,</span> <span class=n>test_iter</span><span class=p>,</span> <span class=n>loss</span><span class=p>,</span> <span class=n>num_epochs</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>,</span> <span class=n>optimizer</span><span class=p>)</span><span class=c1># 训练模型</span>
</span></span></code></pre></div><p>最后输出</p><pre tabindex=0><code>epoch 1, loss 0.0030, train acc 0.712, test acc 0.744
epoch 2, loss 0.0019, train acc 0.823, test acc 0.821
epoch 3, loss 0.0017, train acc 0.844, test acc 0.842
epoch 4, loss 0.0015, train acc 0.856, test acc 0.842
epoch 5, loss 0.0014, train acc 0.864, test acc 0.818
</code></pre></br><h3 id=参考>参考</h3><p><a href=https://tangshusen.me/Dive-into-DL-PyTorch/#/ target=_blank rel="noopener noreferrer">《动手学深度学习》PyTorch 版</a></p><p><a href=https://nndl.github.io target=_blank rel="noopener noreferrer">《神经网络与深度学习》邱锡鹏教授</a></p></div></article><section class="article labels"><a class=category href=/categories/%E7%83%82%E7%AC%94%E5%A4%B4%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>烂笔头之深度学习</a><a class=tag href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a><a class=tag href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a><a class=tag href=/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a><a class=tag href=/tags/pytorch/>PyTorch</a><a class=tag href=/tags/%E6%84%9F%E7%9F%A5%E6%9C%BA/>感知机</a><a class=tag href=/tags/relu/>ReLU</a></section><section class="article license">版权声明：如果转发请带上本文链接和注明来源。</section><section class="article author"><script src=/js/lottie-player.js></script>
<lottie-player class=avatar src=/json/77907-vault.json background=transparent speed=1 style=max-width:280px; loop autoplay></lottie-player><p class=name>yibocat</p><div class=bio>脑子不够键盘来凑</div><div class=details><a class=item href=https://github.com/yibocat target=_blank rel="noopener noreferrer"><span class="iconfont icon-github"></span>&nbsp;yibocat</a><a class=item href=mailto:yibocat@yeah.net target=_blank rel="noopener noreferrer"><span class="iconfont icon-mail"></span>&nbsp;yibocat@yeah.net</a></div></section></div><div class="article bottom"><div class="article bottom title">更多文章</div><section class="article navigation"><p><a class=link href=/posts/dl/sdxx_3/><span class="iconfont icon-article"></span>[深度学习笔记] 深度学习第 3 篇——网络正则化</a></p><p><a class=link href=/posts/dl/sdxx_1/><span class="iconfont icon-article"></span>[深度学习笔记] 深度学习第 1 篇——简单线性回归</a></p></section><div class=nav-bottom><div class="nav wrap bottom"><nav class=nav><a class="nav item" href=/posts/>文章 </a><a class="nav item" href=/archives/>归档 </a><a class="nav item" href=/categories/>分类 </a><a class="nav item" href=/tags/>标签 </a><a class="nav item" href=/search/>搜索</a></nav></div></div></div></section><section id=footer><div class=tongji style=text-align:center><span>共 13 篇文章 • 总计 22442 字</span></div><div class=footer-wrap><p class=copyright>©2022 Yibocat.</p><p class=powerby><span>Powered&nbsp;by&nbsp;</span><a href=https://gohugo.io target=_blank rel="noopener noreferrer">Hugo</a>
<span>&nbsp;Accelerate with Tencent Cloud CDN.</span>
<a href="https://ipv6-test.com/validate.php?url=referer"><img src=https://ipv6-test.com/button-ipv6-80x15.png alt="ipv6 ready" title="ipv6 ready" border=0></a></p></div></section><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script src=/js/core.min.091f2afc61adf43637b5ab08aa95d7d61c04fe17dcd542b7406dffc53c4a983f3e02d0062197358bf3fda63807f23bb0.js integrity=sha384-CR8q/GGt9DY3tasIqpXX1hwE/hfc1UK3QG3/xTxKmD8+AtAGIZc1i/P9pjgH8juw></script>
<script src=https://cdn.jsdelivr.net/npm/echarts@5.3.0/dist/echarts.min.js></script>
<script type=text/javascript>var myChart=echarts.init(document.getElementById("heatmap")),option,data,rangeArr,listeners;window.onresize=function(){myChart.resize()};function heatmap_width(s){var e=new Date,n,o=e.setMonth(e.getMonth()-s),t=+new Date;return e=+new Date(o),t=echarts.format.formatTime("yyyy-MM-dd",t),e=echarts.format.formatTime("yyyy-MM-dd",e),n=[],n.push([e,t]),n}data=[],data.push(["2022-03-10",1102]),data.push(["2022-02-25",1706]),data.push(["2022-02-19",1142]),data.push(["2022-02-17",1469]),data.push(["2022-02-15",2774]),data.push(["2022-02-14",3494]),data.push(["2022-02-14",2006]),data.push(["2022-02-08",2576]),data.push(["2022-02-02",588]),data.push(["2022-02-01",387]),data.push(["2022-01-30",1028]),data.push(["2022-01-29",3947]),data.push(["2022-01-28",74]),data.push(["2019-05-28",0]),data.push(["2019-02-28",149]),data.push(["0001-01-01",0]),window.matchMedia("(max-width: 320px)").matches?(rangeArr=heatmap_width(3)):window.matchMedia("(max-width: 400px)").matches?(rangeArr=heatmap_width(5)):window.matchMedia("(max-width: 550px)").matches?(rangeArr=heatmap_width(6)):window.matchMedia("(max-width: 1021px)").matches?(rangeArr=heatmap_width(12)):window.matchMedia("(max-width: 1400px)").matches?(rangeArr=heatmap_width(9)):window.matchMedia("(max-width: 1920px)").matches?(rangeArr=heatmap_width(12)):window.matchMedia("(max-width: 2560px)").matches&&(rangeArr=heatmap_width(24));function LightOption(){option={title:{show:!0,top:0,left:20,text:"文章热力图",textStyle:{color:"rgba(29,43,76,.85)",fontSize:10}},tooltip:{padding:10,backgroundColor:"#777",borderColor:"#777",borderWidth:1,formatter:function(t){var e=t.value;return'<div style="font-size: 14px;color: white;">'+e[0]+": "+e[1]+"</div>"}},visualMap:{show:!1,top:0,min:0,max:5e3,textStyle:{color:"#7bc96f",fontWeight:"bold",fontFamily:"monospace"},calculable:!0,inRange:{symbol:"rect",color:["#ccffcc","#82c979","#43923c","#125b15","#002800"]},itemWidth:12,itemHeight:200,orient:"horizontal",right:0},calendar:{top:45,left:30,right:30,cellSize:[12,12],orient:"horizontal",range:rangeArr,splitLine:{show:!1,lineStyle:{width:.5,type:"solid",color:"white"}},itemStyle:{borderWidth:3.4,borderType:"solid",shadowColor:"#f3f6f9",shadowBlur:1.2,borderColor:"#f3f6f9",color:"#dddddd",opacity:1},yearLabel:{show:!0,position:"top",fontWeight:"normal",fontSize:12,color:"#24292e"},monthLabel:{nameMap:["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],fontWeight:"bolder",color:"#24292e",fontSize:8},dayLabel:{formatter:"{start}  1st",fontWeight:"bolder",nameMap:['',"Mon",'',"Wed",'',"Fri",''],color:"#24292e",fontSize:8}},series:{type:"heatmap",coordinateSystem:"calendar",data}},option&&myChart.setOption(option)}function DarkOption(){option={title:{show:!0,top:0,left:20,text:"文章热力图",textStyle:{color:"#c7d5f6",fontSize:10}},tooltip:{padding:10,backgroundColor:"#777",borderColor:"#777",borderWidth:1,formatter:function(t){var e=t.value;return'<div style="font-size: 14px;color: white;">'+e[0]+": "+e[1]+"</div>"}},visualMap:{show:!1,top:0,min:0,max:5e3,textStyle:{color:"#c7d5f6",fontWeight:"bold",fontFamily:"monospace"},calculable:!0,inRange:{symbol:"rect",color:["#ccffcc","#82c979","#43923c","#125b15","#002800"],opacity:.8},itemWidth:12,itemHeight:200,orient:"horizontal",right:0},calendar:{top:45,left:30,right:30,cellSize:[12,12],orient:"horizontal",range:rangeArr,splitLine:{show:!1,lineStyle:{width:.5,type:"solid",color:"white"}},itemStyle:{borderWidth:3.4,borderType:"solid",shadowColor:"#0d1117",shadowBlur:1.2,borderColor:"#0d1117",color:"#24292e",opacity:1},yearLabel:{show:!0,position:"top",fontWeight:"normal",fontSize:12,color:"#c7d5f6"},monthLabel:{nameMap:["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],fontWeight:"bolder",color:"#c7d5f6",fontSize:8},dayLabel:{formatter:"{start}  1st",fontWeight:"bolder",nameMap:['',"Mon",'',"Wed",'',"Fri",''],color:"#c7d5f6",fontSize:8}},series:{type:"heatmap",coordinateSystem:"calendar",data}},option&&myChart.setOption(option)}window.matchMedia("(prefers-color-scheme: dark)").matches?DarkOption():LightOption(),listeners={dark:e=>{e.matches&&DarkOption()},light:e=>{e.matches&&LightOption()}},window.matchMedia("(prefers-color-scheme: dark)").addListener(listeners.dark),window.matchMedia("(prefers-color-scheme: light)").addListener(listeners.light)</script></body></html>