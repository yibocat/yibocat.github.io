<!doctype html><html lang=zh><meta charset=utf-8><meta name=generator content="Hugo 0.102.3"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=color-scheme content="light dark"><meta name=supported-color-schemes content="light dark"><link rel=bookmark type=image/x-icon href=images/yibo_1.jpeg><link rel=icon type=image/x-icon href=images/yibo_1.jpeg><link rel="shortcut icon" type=image/x-icon href=images/yibo_1.jpeg><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?e5030d0d5c8f146cc58d0a4fe660754f",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><title>[深度学习笔记] 深度学习第 5 篇——正向传播与反向传播&nbsp;&ndash;&nbsp;Yibocat</title><link rel=stylesheet href=/css/core.min.0a0307d00613a4a7364263921bc1fbafe7f3eff5f15937907925d4c6a18e9130a5c9d54ad7b4631345308f0335629fed.css integrity=sha384-CgMH0AYTpKc2QmOSG8H7r+fz7/XxWTeQeSXUxqGOkTClydVK17RjE0UwjwM1Yp/t><meta name=twitter:card content="summary"><meta name=twitter:title content="[深度学习笔记] 深度学习第 5 篇——正向传播与反向传播"><body><section id=header><div class="header wrap"><span class="header left-side"><a class="site home" href=/><span class="site name">Yibocat</span></a></span>
<span class="header right-side"><div class="nav wrap"><nav class=nav><a class="nav item" href=/posts/>文章 </a><a class="nav item" href=/archives/>归档 </a><a class="nav item" href=/categories/>分类 </a><a class="nav item" href=/tags/>标签 </a><a class="nav item" href=/search/>搜索 </a><a class="nav item" href></a></nav></div></span></div><div class="site slogan"><span id=type></span></div><script src=https://cdn.jsdelivr.net/npm/typed.js@2.0.12></script>
<script>var typed=new Typed("#type",{strings:["Minecraft","Basketball","Code and code","Grand Theft Auto V","Photography","Back to campus","","<strong>Python<strong>","<strong>C++<strong>","<strong><i>Swift<i><strong>","Java","<strong>Machine Learning<strong>","don't like <strong>algorithm<strong>","","一个不会编程的研究僧"],typeSpeed:60,backSpeed:60,backDelay:1250,smartBackspace:!0,loop:!0})</script></section><section id=content><div class=article-container><section class="article header"><h1 class="article title">[深度学习笔记] 深度学习第 5 篇——正向传播与反向传播</h1><p class="article date">2022-02-17
<span class=wordcount>&nbsp;•&nbsp;共&nbsp;1469&nbsp;个字</span><span class=reading-time> • 预计阅读时间 3 分钟</span></p></section><aside id=toc-container class="toc-container wide"><div class=toc><h3>文章目录</h3><div class=inner><ul><li><a href=#%e6%ad%a3%e5%90%91%e4%bc%a0%e6%92%ad aria-label=正向传播>正向传播</a></li><li><a href=#%e5%8f%8d%e5%90%91%e4%bc%a0%e6%92%ad aria-label=反向传播>反向传播</a></li><li><a href=#%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83 aria-label=深度学习模型训练>深度学习模型训练</a></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></div></aside><article class="article markdown-body"><div><p>什么是正向传播？正向传播是按照神经网络的输入到输出的顺序，依次计算从模型中间变量的过程。而反向传播则是计算神经网络的参数梯度的过程。在训练神经网络时，正向传播与反向传播是相互依赖的。正向传播依次计算模型中间变量，反向传播再根据变量对参数进行梯度求导。所以可以这么理解，在参数进行初始化之后对模型进行交替的正向传播与反向传播，并根据反向传播计算的梯度迭代模型参数。</p></br><h3 id=正向传播>正向传播</h3><p>前几篇文章讲的基本上都是正向传播，这里我们简单地再推导一下。</p><p>假设输入为 $x\in \mathbb{R}^d$ ，不考虑偏差，中间变量为 $z\in\mathbb{R}^h$，$\pmb{W}^{(1)}\in\mathbb{R}^{h\times d}$ 为隐藏层的权重参数。设 $\phi()$ 为激活函数，则有
$$
z=\pmb{W}^{(1)}x,\newline
\pmb{h} = \phi(z)
$$
这里 $\pmb{h}$ 表示得到的隐藏层变量，也就是中间变量。</p><p>假设输出层参数为 $\pmb{W}^{(2)}\in\mathbb{R}^{q\times h}$ ，则得到的向量长度为 $q$ 的输出层变量为
$$
\pmb{o}=\pmb{W}^{(2)}\pmb{h}
$$
我们假设损失函数为 $\ell$ ，样本标签为 $y$ ，则样本损失项为
$$
L=\ell(\pmb{o},y)
$$
引入正则化项 $s$
$$
s=\frac{\lambda}{2}(\vert\vert\pmb{W}^{(1)}\vert\vert^2_F+\vert\vert\pmb{W}^{(2)}\vert\vert^2_F)
$$
得到模型在给定样本上的损失为 $J = L+s$ ，我们称 $J$ 为给定样本的<strong>目标函数</strong>。</p></br><h3 id=反向传播>反向传播</h3><p>我们说反向传播是计算神经网络参数梯度的方法，其基本原理是微积分中的链式法则。</p></br><p><strong>链式法则</strong></p><p>假设 $y=g(x)，z=f(y)$，则 $z=h(x)=f(g(x))$ 。对这两个函数分别求导有
$$
\frac{dy}{dx}=g&rsquo;(x) \newline
\frac{dz}{dy}=f&rsquo;(y)
$$
根据链式法则，有
$$
h&rsquo;(x)=\frac{dz}{dx}=\frac{dz}{dy}\cdot\frac{dy}{dx}
$$
即复合函数求导使用<strong>乘法法则</strong>，或称为<strong>链式法则</strong>。在反向传播中我们可以完美地使用这个法则。</p></br><p><strong>反向传播</strong></p><p>我们将链式法则应用到反向传播中。在上文中，我们的模型参数为 $\pmb{W}^{(1)}$ 和 $\pmb{W}^{(2)}$ ，因此反向传播所要作的计算是 $\frac{\partial J}{\partial\pmb{W}^{(1)}}$ 和 $\frac{\partial J}{\partial\pmb{W}^{(2)}}$，反向传播的计算次序与正向传播计算次序恰恰相反。我们求目标函数关于 $L$ 和 $s$ 的梯度，有
$$
\frac{\partial J}{\partial L}=1,\quad \frac{\partial J}{\partial s}=1
$$
然后根据链式法则计算目标函数对于输出层变量的梯度
$$
\frac{\partial J}{\partial o}=\frac{\partial J}{\partial L}\cdot\frac{\partial L}{\partial o}=\frac{\partial L}{\partial o}
$$
接着计算正则化项对于两个参数的梯度
$$
\frac{\partial s}{\partial\pmb{W}^{(1)}}=\lambda\pmb{W}^{(1)},\quad
\frac{\partial s}{\partial\pmb{W}^{(2)}}=\lambda\pmb{W}^{(2)}
$$
然后先计算靠近输出层参数的梯度 $\frac{\partial J}{\partial\pmb{W}^{(2)}}\in\mathbb{R}^{q\times h}$
$$
\begin{aligned}
\frac{\partial J}{\partial\pmb{W}^{(2)}} & = \frac{\partial J}{\partial o}\centerdot\frac{\partial o}{\partial\pmb{W}^{(2)}}+\frac{\partial J}{\partial s}\centerdot\frac{\partial s}{\partial \pmb{W}^{(2)}} \newline
& = \frac{\partial J}{\partial o}\pmb{h}^{\top}+\lambda\pmb{W}^{(2)}
\end{aligned}
$$
然后继续沿着输出层向隐藏层反向传播，隐藏层变量的梯度为
$$
\frac{\partial J}{\partial \pmb{h}} = \frac{\partial J}{\partial o}\centerdot\frac{\partial o}{\partial\pmb{h}}={\pmb{W}^{(2)}}^{\top}\frac{\partial J}{\partial o}
$$
由于激活函数 $\phi$ 按元素运算，中间变量 $z$ 的梯度 $\frac{\partial J}{\partial z}$ 的计算需要使用<strong>按元素乘法</strong> ，符号位 $\odot$
$$
\frac{\partial J}{\partial z}=\frac{\partial J}{\partial \pmb{h}}\cdot\frac{\partial \pmb{h}}{\partial z}=\frac{\partial J}{\partial\pmb{h}}\odot\phi&rsquo;(z)
$$
最后我们得到靠近输入层的模型参数的梯度 $\frac{\partial J}{\partial\pmb{W}^{(1)}}$
$$
\begin{aligned}
\frac{\partial J}{\partial \pmb{W}^{(1)}} &= \frac{\partial J}{\partial z}\cdot\frac{\partial z}{\partial\pmb{W}^{(1)}}+\frac{\partial J}{\partial s}\cdot\frac{\partial s}{\partial \pmb{W}^{(1)}} \newline
& = \frac{\partial J}{\partial z}x^{\top}+\lambda\pmb{W}^{(1)}
\end{aligned}
$$</br></p><h3 id=深度学习模型训练>深度学习模型训练</h3><p>看了这么些公式可能都忘记我们最初是要干什么的，我们为什么要算这么些公式？我们来回顾一下。</p><p>我们训练模型的目的是为了得到最优的参数以满足我们的模型，那么什么样的模型最优呢？可以认为是损失函数最小的模型就是最优的。那么如何降低损失函数呢，我们知道模型在给定样本上的损失函数为 $J$ ，我们要让损失函数最小化，所以通过梯度下降法迭代最接近的参数，使用梯度下降法进行迭代就要通过反向传播算法计算参数梯度。我们回忆一下带有 $L2$ 参数正则化项的参数迭代公式
$$
\omega\gets(1-\frac{\eta\lambda}{\vert \mathcal{B}\vert})\omega-\frac{\eta}{\vert \mathcal{B}\vert}\sum_{i\in\mathcal{B}}\frac{\partial \ell^{(i)}(\omega,b)}{\partial \omega}
$$
这么一看，后面的一项正是我们所推导的参数迭代公式。这就是反向传播！</p></br><p>一方面，正向传播的计算可能依赖于模型参数的当前值，而这些模型参数是在反向传播的梯度计算后通过优化算法迭代的。另一方面，反向传播的梯度计算可能依赖于各变量的当前值，而这些变量的当前值是通过正向传播计算得到的。</p><p>所以，模型参数初始化之后，我们交替地进行正向传播和反向传播，并根据反向传播计算的梯度迭代模型参数。而正是由于神经网络的特殊性，正向传播结束后不能立即释放变量内存，从而导致神经网络的训练往往占用很大的内存的原因。</p></br><h3 id=参考>参考</h3><p><a href=https://zhuanlan.zhihu.com/p/40378224 target=_blank rel="noopener noreferrer">Back Propagation（梯度反向传播）实例讲解 - HexUp的文章</a></p><p><a href=https://tangshusen.me/Dive-into-DL-PyTorch/#/ target=_blank rel="noopener noreferrer">《动手学深度学习》PyTorch 版，阿斯顿·张、李沐</a></p></div></article><section class="article labels"><a class=category href=/categories/%E7%83%82%E7%AC%94%E5%A4%B4%E4%B9%8B%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>烂笔头之深度学习</a><a class=tag href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a><a class=tag href=/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a><a class=tag href=/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/>反向传播</a></section><section class="article license">版权声明：如果转发请带上本文链接和注明来源。</section><section class="article author"><script src=/js/lottie-player.js></script>
<lottie-player class=avatar src=/json/77907-vault.json background=transparent speed=1 style=max-width:280px; loop autoplay></lottie-player><p class=name>yibocat</p><div class=bio>脑子不够键盘来凑</div><div class=details><a class=item href=https://github.com/yibocat target=_blank rel="noopener noreferrer"><span class="iconfont icon-github"></span>&nbsp;yibocat</a><a class=item href=mailto:yibocat@yeah.net target=_blank rel="noopener noreferrer"><span class="iconfont icon-mail"></span>&nbsp;yibocat@yeah.net</a></div></section></div><div class="article bottom"><div class="article bottom title">更多文章</div><section class="article navigation"><p><a class=link href=/posts/jzrj/jzrj_3/><span class="iconfont icon-article"></span>[建站日记] 动态打字特效</a></p><p><a class=link href=/posts/dl/sdxx_4/><span class="iconfont icon-article"></span>[深度学习笔记] 深度学习第 4 篇——权重衰减与丢弃法</a></p></section><div class=nav-bottom><div class="nav wrap bottom"><nav class=nav><a class="nav item" href=/posts/>文章 </a><a class="nav item" href=/archives/>归档 </a><a class="nav item" href=/categories/>分类 </a><a class="nav item" href=/tags/>标签 </a><a class="nav item" href=/search/>搜索</a></nav></div></div></div></section><section id=footer><div class=tongji style=text-align:center><span>共 14 篇文章 • 总计 22908 字</span></div><div class=footer-wrap><p class=copyright>©2022 Yibocat.</p><p class=powerby><span>Powered&nbsp;by&nbsp;</span><a href=https://gohugo.io target=_blank rel="noopener noreferrer">Hugo</a>
<span>&nbsp;Accelerate with Tencent Cloud CDN.</span>
<a href='https://ipv6-test.com/validate.php?url=referer'><img src=https://ipv6-test.com/button-ipv6-80x15.png alt='ipv6 ready' title='ipv6 ready' border=0></a></p></div></section><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script src=/js/core.min.5c7b67437ff4e36acca5d4f8fd7094276f09573657d699a765d6ad6b40a7c4e00993c7f07cb6637e726ae7c3f77fc968.js integrity=sha384-XHtnQ3/042rMpdT4/XCUJ28JVzZX1pmnZdata0CnxOAJk8fwfLZjfnJq58P3f8lo></script>
<script src=https://cdn.jsdelivr.net/npm/echarts@5.3.0/dist/echarts.min.js></script>
<script type=text/javascript>var myChart=echarts.init(document.getElementById("heatmap")),option,data,rangeArr,listeners;window.onresize=function(){myChart.resize()};function heatmap_width(e){var t=new Date,o=t.setMonth(t.getMonth()-e),n=+new Date,t=+new Date(o),n=echarts.format.formatTime("yyyy-MM-dd",n),t=echarts.format.formatTime("yyyy-MM-dd",t),s=[];return s.push([t,n]),s}data=[],data.push(["2022-03-10",1102]),data.push(["2022-02-25",1706]),data.push(["2022-02-19",1133]),data.push(["2022-02-17",1469]),data.push(["2022-02-15",2774]),data.push(["2022-02-14",3494]),data.push(["2022-02-14",2006]),data.push(["2022-02-08",2576]),data.push(["2022-02-08",456]),data.push(["2022-02-02",588]),data.push(["2022-02-01",387]),data.push(["2022-01-30",1028]),data.push(["2022-01-29",3966]),data.push(["2022-01-28",74]),data.push(["2019-05-28",0]),data.push(["2019-02-28",149]),data.push(["0001-01-01",0]),window.matchMedia("(max-width: 320px)").matches?(rangeArr=heatmap_width(3)):window.matchMedia("(max-width: 400px)").matches?(rangeArr=heatmap_width(5)):window.matchMedia("(max-width: 550px)").matches?(rangeArr=heatmap_width(6)):window.matchMedia("(max-width: 1021px)").matches?(rangeArr=heatmap_width(12)):window.matchMedia("(max-width: 1400px)").matches?(rangeArr=heatmap_width(9)):window.matchMedia("(max-width: 1920px)").matches?(rangeArr=heatmap_width(12)):window.matchMedia("(max-width: 2560px)").matches&&(rangeArr=heatmap_width(24));function LightOption(){option={title:{show:!0,top:0,left:20,text:"文章热力图",textStyle:{color:"rgba(29,43,76,.85)",fontSize:10}},tooltip:{padding:10,backgroundColor:"#777",borderColor:"#777",borderWidth:1,formatter:function(e){var t=e.value;return'<div style="font-size: 14px;color: white;">'+t[0]+": "+t[1]+"</div>"}},visualMap:{show:!1,top:0,min:0,max:5e3,textStyle:{color:"#7bc96f",fontWeight:"bold",fontFamily:"monospace"},calculable:!0,inRange:{symbol:"rect",color:["#ccffcc","#82c979","#43923c","#125b15","#002800"]},itemWidth:12,itemHeight:200,orient:"horizontal",right:0},calendar:{top:45,left:30,right:30,cellSize:[12,12],orient:"horizontal",range:rangeArr,splitLine:{show:!1,lineStyle:{width:.5,type:"solid",color:"white"}},itemStyle:{borderWidth:3.4,borderType:"solid",shadowColor:"#f3f6f9",shadowBlur:1.2,borderColor:"#f3f6f9",color:"#dddddd",opacity:1},yearLabel:{show:!0,position:"top",fontWeight:"normal",fontSize:12,color:"#24292e"},monthLabel:{nameMap:["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],fontWeight:"bolder",color:"#24292e",fontSize:8},dayLabel:{formatter:"{start}  1st",fontWeight:"bolder",nameMap:["","Mon","","Wed","","Fri",""],color:"#24292e",fontSize:8}},series:{type:"heatmap",coordinateSystem:"calendar",data}},option&&myChart.setOption(option)}function DarkOption(){option={title:{show:!0,top:0,left:20,text:"文章热力图",textStyle:{color:"#c7d5f6",fontSize:10}},tooltip:{padding:10,backgroundColor:"#777",borderColor:"#777",borderWidth:1,formatter:function(e){var t=e.value;return'<div style="font-size: 14px;color: white;">'+t[0]+": "+t[1]+"</div>"}},visualMap:{show:!1,top:0,min:0,max:5e3,textStyle:{color:"#c7d5f6",fontWeight:"bold",fontFamily:"monospace"},calculable:!0,inRange:{symbol:"rect",color:["#ccffcc","#82c979","#43923c","#125b15","#002800"],opacity:.8},itemWidth:12,itemHeight:200,orient:"horizontal",right:0},calendar:{top:45,left:30,right:30,cellSize:[12,12],orient:"horizontal",range:rangeArr,splitLine:{show:!1,lineStyle:{width:.5,type:"solid",color:"white"}},itemStyle:{borderWidth:3.4,borderType:"solid",shadowColor:"#0d1117",shadowBlur:1.2,borderColor:"#0d1117",color:"#24292e",opacity:1},yearLabel:{show:!0,position:"top",fontWeight:"normal",fontSize:12,color:"#c7d5f6"},monthLabel:{nameMap:["Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],fontWeight:"bolder",color:"#c7d5f6",fontSize:8},dayLabel:{formatter:"{start}  1st",fontWeight:"bolder",nameMap:["","Mon","","Wed","","Fri",""],color:"#c7d5f6",fontSize:8}},series:{type:"heatmap",coordinateSystem:"calendar",data}},option&&myChart.setOption(option)}window.matchMedia("(prefers-color-scheme: dark)").matches?DarkOption():LightOption(),listeners={dark:e=>{e.matches&&DarkOption()},light:e=>{e.matches&&LightOption()}},window.matchMedia("(prefers-color-scheme: dark)").addListener(listeners.dark),window.matchMedia("(prefers-color-scheme: light)").addListener(listeners.light)</script></body></html>