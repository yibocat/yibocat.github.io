---
title: "[计算学习理论] PAC (Probably Approximately Correct)"
date: 2022-02-11T17:22:46+08:00
author: "yibo"
description: ""
draft: true
math: true
tags: ["PAC","机器学习","计算学习理论","theory"]
categories: ["机器学习"]
---

Probably Approximately Correct 简称 PAC，翻译过来是“概率近似正确”或者“可能近似正确”。没错，看起来很拗口... 。但是 PAC 理论却是计算学习理论中最基本的学习理论。在学习机器学习的时候，我们总是在追求算法与模型，却很少思考什么样的情况下适合机器学习，什么样的学习更有效率，到底需要多少样本才能让机器学得好。

</br>

一般我们通过一定数量的样本进行模型训练，最后得到的模型一定会出现误差。不可否认，误差决定着一个模型的好坏，在样本数量不足的情况下很可能发生过拟合现象。也就是说经验风险最小化原则很容易导致模型在训练集上的错误率很低，但是在未知数据上错误率很高，这就是所谓的过拟合。

因此 PAC 可以这么来理解：我用足够的样本训练了一个模型，是否意味着这个模型在包含这些样本的总体样本下依然很好？因为用来训练模型的样本总是一定数量的，我们无法保证这些包含所有样本的一个未知的分布中，训练的模型是否同样有效。这给人感觉起来很绕，可以举一个例子：假如在一个小岛上有很多木瓜，它们有的很好吃有的不好吃，我们并不知道哪些好吃，但是我们根据对其他水果的判断得出水果的软硬和颜色是对水果好吃与否的很重要的判断。所以我们知道，水果的软硬和颜色是判断水果好不好吃的重要因素并且不再需要更多的判断因素了。

### 经验误差与泛化误差

简单的来说，经验误差（empirical error）是模型在训练集上的误差，泛化误差（generalization error）是指模型在新样本上的误差。

具体来看，一个好的模型 $f(x;\theta)$ 应当有一个小的期望错误，但是我们却不知道真实的**数据分布**和**映射函数**，所以也就无法计算期望风险。注意：我们现在有两个未知，一个是**模型$f$**，还有一个**所有样本服从的隐含的分布$\mathcal{D}$**。而我们可以计算的是经验风险，即在**训练集上的平均损失**。

</br>

给定样本 $D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\},x_i\in\mathcal{X}$ ，$\mathcal{D}$ 是所有样本服从的隐含未知的分布，$D$ 中的所有样本呢都是从 $\mathcal{D}$ 中独立采样，即独立同分布（independent and identically distributed，简称 $i.i.d$） 。我们给定 $\mathcal{Y}$ 表示所有可能的标签， $h:\mathcal{X}\to\mathcal{Y}$ 表示样本集到标签的映射。

则经验误差为
$$
E_{D}^{emp}(h;D)=\frac{1}{m}\sum_{i=1}^m\mathbb{I}(h(x_i)\ne y_i)
$$
选择使得经验风险最小的的一组参数就是经验风险最小化准则（Empirical Risk Minimization，简称 ERM）。

泛化误差为
$$
E_{\mathcal{D}}(h;\mathcal{D})=P_{x\thicksim\mathcal{D}}(h(x)\ne y)
$$



























### 参考

1. [概率近似正确？！（PAC Learning） - Haoran Jiang的文章 - 知乎](https://zhuanlan.zhihu.com/p/44589648)
2. 《机器学习》，周志华
3. 《神经网络与深度学习》，邱锡鹏

